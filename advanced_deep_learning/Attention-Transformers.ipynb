{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6ff4bb",
   "metadata": {},
   "source": "# RNNs, Attention, and Transformers"
  },
  {
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:08.113035Z",
     "start_time": "2025-10-27T19:42:07.648124Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install torch",
   "id": "aa3afe800755f688",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "bd1a2def",
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:08.121756Z",
     "start_time": "2025-10-27T19:42:08.117657Z"
    }
   },
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.__version__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "cc6318a4",
   "metadata": {},
   "source": [
    "## Sequence processing in a nutshell\n",
    "\n",
    "Many tasks are **sequence-to-sequence**: speech $\\to$ text, translation, captioning. We need models that consume inputs over time and optionally emit outputs over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e1a4f",
   "metadata": {},
   "source": [
    "## A simple RNN cell\n",
    "\n",
    "A vanilla RNN updates a hidden state $h_t$ using the previous state and current input:\n",
    "$$\n",
    "h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h),\\quad\n",
    "y_t = W_{hy} h_t + b_y.\n",
    "$$\n",
    "Below is a tiny character-level **toy** RNN that predicts the next character in a short string to illustrate shape flow and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f1803b33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:08.489469Z",
     "start_time": "2025-10-27T19:42:08.131492Z"
    }
   },
   "source": [
    "# Tiny toy dataset: predict next char in a small alphabet\n",
    "alphabet = list(\"abcd \")\n",
    "stoi = {c:i for i,c in enumerate(alphabet)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "\n",
    "def encode(s): return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "def onehot(i, n): \n",
    "    x = torch.zeros(n); x[i] = 1.0; return x\n",
    "\n",
    "seq = \"abca abca abca \"\n",
    "data = encode(seq)\n",
    "\n",
    "nin = len(alphabet); nh = 16; nout = len(alphabet)\n",
    "Wxh = nn.Linear(nin, nh)\n",
    "Whh = nn.Linear(nh, nh)\n",
    "Why = nn.Linear(nh, nout)\n",
    "params = list(Wxh.parameters()) + list(Whh.parameters()) + list(Why.parameters())\n",
    "opt = optim.Adam(params, lr=3e-2)\n",
    "\n",
    "def rnn_step(h, x_idx):\n",
    "    x = onehot(x_idx, nin)\n",
    "    h = torch.tanh(Whh(h) + Wxh(x))\n",
    "    y = Why(h)\n",
    "    return h, y\n",
    "\n",
    "for step in range(400):\n",
    "    h = torch.zeros(nh)\n",
    "    loss = 0.0\n",
    "    for t in range(len(data)-1):\n",
    "        h, y = rnn_step(h, data[t].item())\n",
    "        loss = loss + F.cross_entropy(y.unsqueeze(0), data[t+1].unsqueeze(0))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "# Sample a few steps\n",
    "h = torch.zeros(nh)\n",
    "x = stoi[\"a\"]\n",
    "out = []\n",
    "for t in range(20):\n",
    "    h, y = rnn_step(h, x)\n",
    "    p = F.softmax(y, -1)\n",
    "    x = int(torch.multinomial(p, 1))\n",
    "    out.append(itos[x])\n",
    "\"\".join(out)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bca abca abca abca a'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "cc519d25",
   "metadata": {},
   "source": [
    "## Vanishing and exploding gradients (intuition)\n",
    "\n",
    "Backpropagation through time multiplies by the same weights many times, so gradients can shrink or grow exponentially. For long‑range dependencies, **plain RNNs** struggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfdeee",
   "metadata": {},
   "source": [
    "## LSTM and GRU\n",
    "\n",
    "Gated units regulate information flow with **gates** to help preserve long‑term dependencies.\n",
    "\n",
    "- **LSTM** maintains a cell $c_t$ with forget/input/output gates.\n",
    "- **GRU** merges some gates and omits a separate cell, often working well with fewer parameters.\n",
    "\n",
    "Below we compare a vanilla `nn.RNN` vs `nn.LSTM` on a synthetic long dependency: the label is the **first** token of the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbbbf95a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:08.959121Z",
     "start_time": "2025-10-27T19:42:08.493797Z"
    }
   },
   "source": [
    "# Synthetic dataset: sequences of length T, label = first token (0/1)\n",
    "def make_dataset(N=256, T=50):\n",
    "    X = torch.randint(0, 2, (N, T)).long()\n",
    "    y = X[:, 0]\n",
    "    return X, y\n",
    "\n",
    "def run_model(cell_type=\"rnn\", T=50, epochs=20):\n",
    "    X, y = make_dataset(256, T)\n",
    "    emb = nn.Embedding(2, 8)\n",
    "    if cell_type == \"rnn\":\n",
    "        rnn = nn.RNN(8, 16, batch_first=True, nonlinearity=\"tanh\")\n",
    "    else:\n",
    "        rnn = nn.LSTM(8, 16, batch_first=True)\n",
    "    clf = nn.Linear(16, 2)\n",
    "    opt = optim.Adam(list(emb.parameters()) + list(rnn.parameters()) + list(clf.parameters()), lr=1e-2)\n",
    "    for ep in range(epochs):\n",
    "        x = emb(X)\n",
    "        out, _ = rnn(x)\n",
    "        logits = clf(out[:, -1])  # last step\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad();\n",
    "        loss.backward();\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        x = emb(X);\n",
    "        out, _ = rnn(x)\n",
    "        logits = clf(out[:, -1])\n",
    "        acc = logits.argmax(-1).eq(y).float().mean().item()\n",
    "    return loss.item(), acc\n",
    "\n",
    "loss_rnn, acc_rnn = run_model(\"rnn\", T=50, epochs=30)\n",
    "loss_lstm, acc_lstm = run_model(\"lstm\", T=50, epochs=30)\n",
    "{\"RNN_acc\": round(acc_rnn, 3), \"LSTM_acc\": round(acc_lstm, 3)}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RNN_acc': 0.68, 'LSTM_acc': 0.57}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "7ba55cae",
   "metadata": {},
   "source": [
    "## Attention as content‑based lookup\n",
    "\n",
    "Given **queries** $Q$, **keys** $K$, and **values** $V$, attention produces a weighted sum of values where weights reflect $QK^\\top$ similarity.\n",
    "\n",
    "**Scaled dot‑product attention:**\n",
    "$$\n",
    "\\mathrm{Attn}(Q,K,V) = \\mathrm{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3005ccf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:08.976340Z",
     "start_time": "2025-10-27T19:42:08.972415Z"
    }
   },
   "source": [
    "# Minimal scaled dot-product attention with masking\n",
    "def scaled_dot_attn(Q, K, V, mask=None):\n",
    "    # Q: [B, H, Tq, Dh], K,V: [B, H, Tk, Dh]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "    A = torch.softmax(scores, dim=-1)\n",
    "    return A @ V, A\n",
    "\n",
    "# Quick demo\n",
    "B,H,Tq,Tk,Dh = 2, 1, 3, 4, 8\n",
    "Q = torch.randn(B,H,Tq,Dh)\n",
    "K = torch.randn(B,H,Tk,Dh)\n",
    "V = torch.randn(B,H,Tk,Dh)\n",
    "out, attn = scaled_dot_attn(Q,K,V)\n",
    "out.shape, attn.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 3, 8]), torch.Size([2, 1, 3, 4]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "18f7f292",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "Self‑attention is permutation‑equivariant, so we inject positions. A common choice is **sinusoidal** encoding:\n",
    "$$\n",
    "\\mathrm{PE}(pos,2i) = \\sin\\!\\Big(\\frac{pos}{10000^{2i/d}}\\Big),\\quad\n",
    "\\mathrm{PE}(pos,2i+1) = \\cos\\!\\Big(\\frac{pos}{10000^{2i/d}}\\Big).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5697b770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:08.989985Z",
     "start_time": "2025-10-27T19:42:08.986249Z"
    }
   },
   "source": [
    "def sinusoidal_pe(T, d):\n",
    "    pos = torch.arange(T).unsqueeze(1)\n",
    "    i = torch.arange(d).unsqueeze(0)\n",
    "    angles = pos / (10000 ** (2*(i//2)/d))\n",
    "    pe = torch.zeros(T,d)\n",
    "    pe[:,0::2] = torch.sin(angles[:,0::2])\n",
    "    pe[:,1::2] = torch.cos(angles[:,1::2])\n",
    "    return pe\n",
    "\n",
    "pe = sinusoidal_pe(10, 16)\n",
    "pe[:3, :8]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.3110,  0.9504,  0.0998,  0.9950,  0.0316,  0.9995],\n",
       "        [ 0.9093, -0.4161,  0.5911,  0.8066,  0.1987,  0.9801,  0.0632,  0.9980]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "9fef67b1",
   "metadata": {},
   "source": [
    "## Multi‑head attention\n",
    "\n",
    "Use $h$ heads with separate projections, concatenate head outputs, and project again:\n",
    "$$\n",
    "\\mathrm{MHA}(X) = \\big[ \\mathrm{Attn}(XW_Q^{(1)}, XW_K^{(1)}, XW_V^{(1)});\\ldots \\big] W_O.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "988aa1e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:09.028811Z",
     "start_time": "2025-10-27T19:42:09.023554Z"
    }
   },
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=64, n_heads=4):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.nh = n_heads\n",
    "        self.dh = d_model // n_heads\n",
    "        self.Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.O = nn.Linear(d_model, d_model, bias=False)\n",
    "    def forward(self, x, mask=None):\n",
    "        B,T,D = x.shape\n",
    "        Q = self.Q(x).view(B,T,self.nh,self.dh).transpose(1,2) # [B,H,T,Dh]\n",
    "        K = self.K(x).view(B,T,self.nh,self.dh).transpose(1,2)\n",
    "        V = self.V(x).view(B,T,self.nh,self.dh).transpose(1,2)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # broadcast over heads\n",
    "        y,_ = scaled_dot_attn(Q,K,V,mask=mask)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,D)\n",
    "        return self.O(y)\n",
    "\n",
    "x = torch.randn(2, 5, 64)\n",
    "mha = MultiHeadAttention(64, 4)\n",
    "mha(x).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "0a431f24",
   "metadata": {},
   "source": [
    "## Transformer encoder block\n",
    "\n",
    "Stack **Multi‑Head Attention** and a **positionwise MLP**, with **residual connections** and **layer normalization**:\n",
    "$$\n",
    "x \\leftarrow x + \\mathrm{MHA}(\\mathrm{LN}(x)),\\quad\n",
    "x \\leftarrow x + \\mathrm{MLP}(\\mathrm{LN}(x)).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba0dcf0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:09.078581Z",
     "start_time": "2025-10-27T19:42:09.073340Z"
    }
   },
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=64, n_heads=4, d_ff=128, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff  = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "        self.drop = nn.Dropout(pdrop)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.drop(self.mha(self.ln1(x), mask=mask))\n",
    "        x = x + self.drop(self.ff(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "tok = torch.randn(2, 6, 64)\n",
    "blk = TransformerEncoderBlock()\n",
    "blk(tok).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "076bbff6",
   "metadata": {},
   "source": [
    "## Decoder self‑attention with causal masking\n",
    "\n",
    "In the decoder we **mask future positions** so the model cannot peek ahead during training. The mask is a lower‑triangular matrix $M$:\n",
    "$$\n",
    "M_{ij} = 1 \\ \\text{if}\\ j \\le i,\\ \\ 0\\ \\text{otherwise}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "64926b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:09.097597Z",
     "start_time": "2025-10-27T19:42:09.093620Z"
    }
   },
   "source": [
    "def causal_mask(T):\n",
    "    return torch.tril(torch.ones(T, T, dtype=torch.bool))\n",
    "\n",
    "T = 5\n",
    "mask = causal_mask(T)  # [T,T]\n",
    "# Example: apply in attention (broadcast to [1,1,T,T]) so MHA can broadcast across batch and heads\n",
    "B, H, D_model = 2, 4, 16\n",
    "x = torch.randn(B, T, D_model)\n",
    "mha = MultiHeadAttention(d_model=D_model, n_heads=H)\n",
    "# Broadcast-safe mask: [1,1,T,T] will expand to [B,H,T,T] inside attention\n",
    "attn_mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "y = mha(x, mask=attn_mask)\n",
    "y.shape, mask"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 16]),\n",
       " tensor([[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "de77b73c",
   "metadata": {},
   "source": [
    "## Encoder–decoder cross‑attention\n",
    "\n",
    "Keys/values come from the **encoder**, queries from the **decoder**. This lets the decoder look back at the encoded source sequence:\n",
    "$$\n",
    "\\mathrm{Attn}(Q_\\text{dec}, K_\\text{enc}, V_\\text{enc}).\n",
    "$$\n",
    "Below is a minimal cross‑attention module (API‑compatible with the self‑attention above).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "268adc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:09.114682Z",
     "start_time": "2025-10-27T19:42:09.109185Z"
    }
   },
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model=64, n_heads=4):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.nh = n_heads; self.dh = d_model // n_heads\n",
    "        self.Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.O = nn.Linear(d_model, d_model, bias=False)\n",
    "    def forward(self, qx, kx):\n",
    "        # qx: [B,Tq,D] (decoder states); kx: [B,Tk,D] (encoder states)\n",
    "        B,Tq,D = qx.shape; Tk = kx.size(1)\n",
    "        Q = self.Q(qx).view(B,Tq,self.nh,self.dh).transpose(1,2)\n",
    "        K = self.K(kx).view(B,Tk,self.nh,self.dh).transpose(1,2)\n",
    "        V = self.V(kx).view(B,Tk,self.nh,self.dh).transpose(1,2)\n",
    "        y,_ = scaled_dot_attn(Q,K,V)\n",
    "        y = y.transpose(1,2).contiguous().view(B,Tq,D)\n",
    "        return self.O(y)\n",
    "\n",
    "enc = torch.randn(2,7,64)\n",
    "dec = torch.randn(2,5,64)\n",
    "cross = CrossAttention(64,4)\n",
    "cross(dec, enc).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 64])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "d1453db0",
   "metadata": {},
   "source": [
    "## A tiny end‑to‑end toy: copy task\n",
    "\n",
    "To keep runtime small, we solve a simple **copy** task: given a short token sequence, predict the same sequence (teacher forcing). This demonstrates shapes, masking, and how attention learns alignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ed41e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:42:15.608416Z",
     "start_time": "2025-10-27T19:42:09.120328Z"
    }
   },
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab=16, d_model=64, n_heads=4, n_layers=2, d_ff=128, T=12):\n",
    "        super().__init__()\n",
    "        self.T = T; self.d = d_model; self.vocab = vocab\n",
    "        self.emb = nn.Embedding(vocab, d_model)\n",
    "        self.pos = sinusoidal_pe(T, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerEncoderBlock(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
    "        self.lm_head = nn.Linear(d_model, vocab)\n",
    "    def forward(self, x):\n",
    "        # x: [B,T]\n",
    "        B,T = x.shape\n",
    "        h = self.emb(x) + self.pos[:T].to(x.device)\n",
    "        mask = causal_mask(T).to(x.device).unsqueeze(0).unsqueeze(0)  # [1,1,T,T]\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, mask=mask)\n",
    "        return self.lm_head(h)\n",
    "\n",
    "def make_copy_data(N=256, T=12, vocab=16):\n",
    "    X = torch.randint(1, vocab, (N,T))  # avoid pad=0 for simplicity\n",
    "    Y = X.clone()\n",
    "    return X, Y\n",
    "\n",
    "model = TinyTransformer(vocab=20, T=12)\n",
    "opt = optim.Adam(model.parameters(), lr=3e-3)\n",
    "X, Y = make_copy_data(N=256, T=12, vocab=20)\n",
    "\n",
    "for step in range(300):\n",
    "    logits = model(X)                # [B,T,V]\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    pred = model(X[:4]).argmax(-1)\n",
    "loss.item(), X[:1], pred[:1]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8386692065396346e-05,\n",
       " tensor([[ 6,  3,  6, 15,  5,  1,  9,  8, 17,  9,  1,  6]]),\n",
       " tensor([[ 6,  3,  6, 15,  5,  1,  9,  8, 17,  9,  1,  6]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "56928f5e",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "- Replace the toy tasks with a real dataset (e.g., small translation or language modeling corpus).\n",
    "- Explore **masking strategies** (padding vs. causal) and how they affect attention.\n",
    "- Compare learned embeddings vs. fixed sinusoidal encodings.\n",
    "- Extend to a full **encoder–decoder** by adding cross‑attention and a separate decoder stack.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
