{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "189cc004-14b3-4258-b78b-303f409da889",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "## Tutorial and home work 1: Classification and encoder for MNIST\n",
    "\n",
    "### Installation\n",
    "\n",
    "We will be using torch for the exercises. It is recommended to install a seperate conda environment for the course and install torch, torchvision, python 3.12 and matplotlib there. Check the online manuals for the installation on your computer.\n",
    "\n",
    "We also use two other packages, torchinfo and torcheval that can be installed using pip.\n",
    "\n",
    "The following imports all the packages used, if there are any error messages, check your installation.\n",
    "\n",
    "### Tutorial outline \n",
    "\n",
    "The tutorial recaps how to do design and train a neural network using pytorch. As an exercise, the tutorial should be augmented by designing and training a different network that encodes and decodes the input image and does the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631cd0d-7fd6-4ba9-88b0-9468fa8a91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7fcce1-f6cb-4d8e-a37c-4c5c87632022",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We will use data sets from torchvision. These data sets have to be transformed into a tensor for torch and also must be normalized if they are not already. We will use 1-channel (intensity) images from the (probably too well known :-) ) MNIST data set for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ac2b4-dc0f-47db-a3c6-c0f46f9bd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254eff06-4ee6-48d7-98a7-9ec59321c256",
   "metadata": {},
   "source": [
    "Generate the train and test data sets. Download the data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd52d1-38bd-40f1-8536-1e26e81a4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torchvision.datasets.MNIST(root='data/mnist', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e4c9f-7560-441d-8dd4-80d7e2374e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = torchvision.datasets.MNIST(root='data/mnist', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4099d9-92bc-4763-9aab-cc146d967b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'train: {len(data_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51876199-3114-4662-836c-06bc514bc3e9",
   "metadata": {},
   "source": [
    "### Train and validation data sets\n",
    "\n",
    "The train data set should be further split into train and validation. We can use random split here as the data set is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf868e-72ad-485d-a2ef-373c7f399769",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = (int)(0.8 * len(data_train))\n",
    "len_val = len(data_train) - len_train\n",
    "\n",
    "print(len_train, len_val, len(data_train))\n",
    "\n",
    "data_train_subset, data_val_subset = torch.utils.data.random_split(\n",
    "        data_train, [len_train, len_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3887e-7a21-4afc-bf93-709be638385f",
   "metadata": {},
   "source": [
    "Construct data loaders for the 3 data sets.\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62dda5-0b1e-49b8-9725-585c81d08a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "data_train_loader = torch.utils.data.DataLoader(dataset=data_train_subset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "data_val_loader = torch.utils.data.DataLoader(dataset=data_val_subset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "data_test_loader = torch.utils.data.DataLoader(data_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bcc7d-2e93-4b13-a1b5-e555d302c9a4",
   "metadata": {},
   "source": [
    "### Verify the images in the data set\n",
    "Verify the images and also check that the range is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfce049-d7da-4829-b4f9-3187929fbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(data_train_loader)\n",
    "images, labels = next(train_iter)\n",
    "\n",
    "image = images[0].numpy().squeeze()\n",
    "print(f'max: {np.max(image)}, min: {np.min(image)}')\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55829d-af2c-4cc8-8c0c-1f591178b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(torchvision.utils.make_grid(images), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc6a8e-b0c0-4e2c-8c3f-d5312bdd98c0",
   "metadata": {},
   "source": [
    "## Define a CNN for classification\n",
    "Define a convolutional neural network for classification. \n",
    "\n",
    "We will use a simple network for this example. This should be refined in the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ba401-f209-4702-b813-30b2801752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, out_channels=4, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(4, 8, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(8, 8, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(72, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 10)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x)\n",
    "\n",
    "\n",
    "my_cnn = MyCNN()\n",
    "print(my_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1c211-abf1-414c-973e-c316b12b290a",
   "metadata": {},
   "source": [
    "### Displaying the network\n",
    "The print function does not display the resulting sizes and number of parameters. The summary function from torchinfo provides similar output as the function in keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2f189-2d38-4081-9a1c-78c2caec56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(my_cnn, input_size=(64, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee0e0e-c12c-4b44-b6dd-1325069f5ebc",
   "metadata": {},
   "source": [
    "### Loss function and optimizer\n",
    "We next define the loss function and the optimizer to use. We will use a simple optimizer for the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130f8fc-30e9-4261-bf46-96b4686bd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_cnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2266d-c30a-4d00-8eb9-7670630693de",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "In pytorch, we must specify which device to use and move the input and the model to the device. Lets first have a simple function to get the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ec064-1ad0-46dc-a5eb-8c24dd6e6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # test if it worked\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using CUDA device')\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using MPS device')\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e198f6-0f3d-41c3-a342-99fe7fe542f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0e32f-82ce-44e6-bb33-f41d29419be3",
   "metadata": {},
   "source": [
    "### Simple train function\n",
    "Lets define a simple train function for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5ed5f-1b2c-4803-8650-c4ef298880ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, model, loss_function, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train(True)\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(data_train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss:.5f}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2ec51-9e68-4070-9742-ff74022b4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch + 1}')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    my_cnn.train(True)\n",
    "    avg_loss = train_one_epoch(epoch, my_cnn, criterion, optimizer, device)\n",
    "    print(f'EPOCH {epoch + 1} Loss: {avg_loss:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c0ce2-93e9-4f45-bdb9-6f5a6f185bd8",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "The code above works, however there are a couple of things missing:\n",
    "- first it would be more interesting to calculate some metrics in addition to the loss function\n",
    "- then, we should evaluate the loss and the metrics on the evaluation set\n",
    "- and thirdly, we would like to monitor the loss and metrics using graphics representation specially for longer training time\n",
    "\n",
    "We will use weights and biases (wandb) for the display. Another option is to use tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d5b0d-16a6-4673-8cef-b79b33acd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04896827-f8fc-47b3-9d4c-f131d1c35edf",
   "metadata": {},
   "source": [
    "### Training and evalution loop\n",
    "\n",
    "We define our a training loop that receives the model, loss function, optimizer, metrics and the device as parameters.\n",
    "\n",
    "There are different possibilities to do this, with wandb I find it easier to do training and evaluation in the same loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1aff5-b600-4d37-bfc4-74db25bb4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, model, loss_function, optimizer, metrics, device):\n",
    "    # define the project and store some setting for the projects to compare results later.\n",
    "    run = wandb.init(project=\"mnist-example\", config={'epochs': epochs, \n",
    "                                                       'batch_size': data_train_loader.batch_size}\n",
    "                                                       )\n",
    "    input_count = 0\n",
    "    step_count = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        metrics.reset()\n",
    "        for step, (inputs, labels) in enumerate(data_train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "            # calculate results\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            train_loss = loss_function(outputs, labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metrics.update(predicted, labels)\n",
    "            train_acc = metrics.compute()\n",
    "\n",
    "            # wandb will store the matrics with the step across x, so we also store the epoch\n",
    "            train_metrics = {'train/train_loss:': train_loss,\n",
    "                       'train/train_acc': train_acc,\n",
    "                       'train/epoch': epoch}\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "            wandb.log(train_metrics, step=step_count)\n",
    "\n",
    "        model.eval()\n",
    "        metrics.reset()\n",
    "        val_loss = []\n",
    "        val_steps = 0\n",
    "        for step, (inputs, labels) in enumerate(data_val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                val_loss.append(loss_function(outputs, labels).item())\n",
    "                metrics.update(predicted, labels)\n",
    "\n",
    "        val_acc = metrics.compute()\n",
    "        val_loss_mean = np.mean(val_loss)\n",
    "        val_metrics = {'val/val_loss': val_loss_mean,\n",
    "                       'val/val_acc' : val_acc}\n",
    "        # log both metrics\n",
    "        wandb.log(val_metrics, step=step_count)\n",
    "\n",
    "        print(f\"Epoch {epoch:02} Train Loss: {train_loss:.3f}, Valid Loss: {val_loss_mean:.3f}, Train Accuracy: {train_acc:.2f} Valid Acc: {val_acc:.2f}\")\n",
    "    wandb.finish()\n",
    "                       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2812110-504d-4731-8c0f-7dfa20641d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_cnn = MyCNN()\n",
    "my_metrics = MulticlassAccuracy(num_classes=10)\n",
    "my_optimizer = optim.Adam(my_cnn.parameters(), lr=0.001)\n",
    "my_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(10, my_cnn, criterion, my_optimizer, my_metrics, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d6cd2-e2bf-418c-9506-45bb9617b38a",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Autoencoder\n",
    "\n",
    "In exercise 1.1, you should implement an autoencoder. In an autoencoder, we would like to compress the input image into a small representation, this is the encoder part and then decode this representation again into an image.\n",
    "\n",
    "The resulting image should be similar to the input image.\n",
    "\n",
    "The size of the encoded representation, the so called latent variables should be a parameter to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb306a22-6bd2-490e-b612-c60ca184e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 28 x 28\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # 14 x 14\n",
    "            nn.Conv2d(32, 16, 3, 2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # 7 x 7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 16, latent_dim),\n",
    "            )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 2\n",
    "            nn.Linear(latent_dim, 7 * 7 * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (16, 7, 7)),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            # 28 x 28 \n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        dec = self.decoder(enc)\n",
    "        return dec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2d449-05ba-49a0-8607-fee8e406e625",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "- Adapt the training loop above for the autoencoder\n",
    "- Train the autoencoder, adjusting hyper parameters if necessary\n",
    "- Visualize the results, i.e. the decoded image\n",
    "- Try different number of latent variables, for example 2 and 10\n",
    "- Visualize the latent variables (for 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b20149-bf41-435d-ab31-dbe109cc0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(epochs: int, model, loss_function, optimizer, device):\n",
    "    # define the project and store some setting for the projects to compare results later.\n",
    "    run = wandb.init(project=\"mnist-ae\", config={'epochs': epochs, \n",
    "                                                 'batch_size': data_train_loader.batch_size}\n",
    "                    )\n",
    "    input_count = 0\n",
    "    step_count = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for step, (inputs, labels) in enumerate(data_train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            # we dont need the labels here\n",
    "            \n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "            # calculate results\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # the loss function is here between the inputs and the outputs\n",
    "            train_loss = loss_function(inputs, outputs)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # wandb will store the metrics with the step across x, so we also store the epoch\n",
    "            train_metrics = {'train/train_loss:': train_loss,\n",
    "                       'train/epoch': epoch}\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "            wandb.log(train_metrics, step=step_count)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        for step, (inputs, labels) in enumerate(data_val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                val_loss.append(loss_function(outputs, inputs).item())\n",
    "\n",
    "        val_loss_mean = np.mean(val_loss)\n",
    "        val_metrics = {'val/val_loss': val_loss_mean}\n",
    "\n",
    "        wandb.log(val_metrics, step=step_count)\n",
    "\n",
    "        print(f\"Epoch {epoch:02} Train Loss: {train_loss:.3f}, Valid Loss: {val_loss_mean:.3f}\")\n",
    "    wandb.finish()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ea21f-a574-4960-92ae-48370f804c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Autoencoder(latent_dim=10)\n",
    "optimizer = torch.optim.RMSprop(auto_encoder.parameters(), lr=0.001)\n",
    "loss_mse = nn.MSELoss()\n",
    "summary(auto_encoder, input_size=(64, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76129bd1-5cb9-422c-9bd3-b5f92f15d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder.to(device)\n",
    "train_ae(10, auto_encoder, loss_mse, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517e756-02d2-4e9c-a731-07f15b97a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(data_train_loader)\n",
    "images, labels = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b65ff8-456d-49e1-89db-b683db3bc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(images), (1, 2, 0)))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(auto_encoder.forward(images.to(device)).cpu()), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc47993-ceef-4117-b09c-bfbd2c0c209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Autoencoder(latent_dim=2)\n",
    "optimizer = torch.optim.RMSprop(auto_encoder.parameters(), lr=0.001)\n",
    "loss_mse = nn.MSELoss()\n",
    "summary(auto_encoder, input_size=(64, 1, 28, 28))\n",
    "auto_encoder.to(device)\n",
    "train_ae(10, auto_encoder, loss_mse, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97bc00-55bd-4a6a-9621-086417188ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(data_train_loader)\n",
    "images, labels = next(train_iter)\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(images), (1, 2, 0)))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(auto_encoder.forward(images.to(device)).cpu()), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff78db-2ce4-47b5-beec-fc15ef817ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latent space\n",
    "def plot_latent_space(ae, n, figsize, device):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 2.5\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                z_sample = torch.tensor([[xi, yi]], dtype=torch.float32, device=device)\n",
    "                x_decoded = ae.decoder(z_sample)\n",
    "                digit = x_decoded[0].reshape(digit_size, digit_size).cpu()\n",
    "                figure[\n",
    "                    i * digit_size : (i + 1) * digit_size,\n",
    "                    j * digit_size : (j + 1) * digit_size,\n",
    "                ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(auto_encoder, n=20, figsize=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064bd79-f92a-4f4d-bbed-6525e4334995",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Two headed network with autoencoder and classificator\n",
    "\n",
    "Next we want to train a model that can do both the autoencoder and the classification. One idea of this approach is to force the model to learn a representation that works well for reconstruction and classification. The classification head should branch off after the encoding part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8d957-fba8-4f2c-b923-0d21f823a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderAndClassificator(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(EncoderAndClassificator,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 28 x 28\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # 14 x 14\n",
    "            nn.Conv2d(32, 16, 3, 2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # 7 x 7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 16, latent_dim),\n",
    "            )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 7 * 7 * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (16, 7, 7)),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            # 28 x 28 \n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "        self.classifier = nn.Sequential (\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        dec = self.decoder(enc)\n",
    "        prob = self.classifier(enc)\n",
    "        return dec, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74fa1c-13f0-4cd9-be8b-e449564e751e",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Adapt the training loop, how is the loss calculated?\n",
    "- Train the model and compare it to the two seperate models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0343d3-26c9-4aca-857c-5c79e557d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enc_class(epochs: int, model, cls_loss, rec_loss, loss_coeff, optimizer, device):\n",
    "    run = wandb.init(project=\"mnist-encode-cls\", config={'epochs': epochs, \n",
    "                                                       'batch_size': data_train_loader.batch_size,\n",
    "                                                        'latent_dim': model.latent_dim}\n",
    "                                                       )\n",
    "    input_count = 0\n",
    "    step_count = 0\n",
    "\n",
    "    metrics = MulticlassAccuracy(num_classes=10)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #\n",
    "        # Training Loop\n",
    "        #\n",
    "        model.train()\n",
    "        metrics.reset()\n",
    "        for step, (inputs, labels) in enumerate(data_train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # calculate results\n",
    "            reconstruction, class_prob = model(inputs)\n",
    "            _, predicted = torch.max(class_prob, 1)\n",
    "\n",
    "            # calculate metrics\n",
    "            metrics.update(predicted, labels)\n",
    "            train_acc = metrics.compute()\n",
    "\n",
    "            # calculate and add losses\n",
    "            loss_1 = loss_coeff * rec_loss(inputs, reconstruction)\n",
    "            loss_2 = (1.0-loss_coeff) * cls_loss(class_prob, labels)\n",
    "            train_total_loss = loss_1 + loss_2\n",
    "            \n",
    "            train_total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_metrics = \\\n",
    "                {'train/train_total_loss:': train_total_loss,\n",
    "                 'train/train_rec_loss:': loss_1,\n",
    "                 'train/train_cls_loss:': loss_2,\n",
    "                 'train/train_acc': train_acc,\n",
    "                 'train/epoch': epoch}\n",
    "\n",
    "            wandb.log(train_metrics)\n",
    "        #\n",
    "        # Evaluation Loop\n",
    "        #\n",
    "        model.eval()\n",
    "        metrics.reset()\n",
    "        val_loss = []\n",
    "        val_steps = 0\n",
    "        for step, (inputs, labels) in enumerate(data_val_loader):\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                 # calculate results\n",
    "                reconstruction, class_prob = model(inputs)\n",
    "                _, predicted = torch.max(class_prob, 1)\n",
    "\n",
    "                # calculate metrics\n",
    "                metrics.update(predicted, labels)\n",
    "\n",
    "                loss_1 = loss_coeff * rec_loss(inputs, reconstruction)\n",
    "                loss_2 = (1.0-loss_coeff) * cls_loss(class_prob, labels)\n",
    "                val_total_loss = loss_1 + loss_2\n",
    "                val_loss.append(val_total_loss.cpu().numpy())\n",
    "            \n",
    "                val_acc = metrics.compute()\n",
    "\n",
    "        val_loss_mean = np.mean(val_loss)\n",
    "        val_metrics = {'val/val_total_loss': val_loss_mean,\n",
    "                       'val/val_acc' : val_acc}\n",
    "        # log both metrics\n",
    "        wandb.log({**train_metrics, **val_metrics})\n",
    "\n",
    "        print(f\"Epoch {epoch:02} Train Loss: {train_total_loss:.3f}, Train Acc: {train_acc:.3f}, Val Loss: {val_loss_mean:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "    wandb.finish()\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622da772-5700-43f8-bbd7-08c8fba8fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderAndClassificator(10)\n",
    "classifier_loss = nn.CrossEntropyLoss()\n",
    "reconstruction_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_enc_class(20, model.to(device), classifier_loss, reconstruction_loss, 0.95, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404fc4b-ff6d-47f2-81dd-e6738072f3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
