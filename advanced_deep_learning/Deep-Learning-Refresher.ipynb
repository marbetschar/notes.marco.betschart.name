{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c674788b",
   "metadata": {},
   "source": "# Deep Learning Refresher"
  },
  {
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.041035Z",
     "start_time": "2025-10-27T19:28:51.602219Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install torch",
   "id": "badfaf489457953a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3032dd1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.051429Z",
     "start_time": "2025-10-27T19:28:52.048415Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "torch.__version__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "9a86b13c",
   "metadata": {},
   "source": [
    "## Neural network types at a glance\n",
    "\n",
    "- **MLP (Dense/Feed‑Forward):** Good when inputs are tabular features $x\\in\\mathbb{R}^n$.\n",
    "- **CNN:** Local connectivity + weight sharing for data on grids (images, audio, videos).\n",
    "- **RNN/Seq models:** For sequences; often superseded by attention/Transformers for long‑range deps.\n",
    "- **GNN:** Generalizes convolutions to graphs; a CNN is a GNN on a pixel‑grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35856c",
   "metadata": {},
   "source": [
    "## Multi‑Layer Perceptron (MLP)\n",
    "\n",
    "**Key idea:** Stack affine transforms + nonlinearities to approximate functions.  \n",
    "The Universal Approximation Theorem (UAT) states a 1‑hidden‑layer MLP with enough units and a suitable nonlinearity can approximate continuous functions on compact subsets:\n",
    "$$\n",
    "f(x)\\approx \\sum_{i=1}^{N} a_i\\,\\sigma(w_i^\\top x + b_i).\n",
    "$$\n",
    "\n",
    "### Example: Tabular regression with an MLP\n",
    "Below we learn a toy map $x\\mapsto y=\\sin(x)$ with noise to illustrate capacity and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "81c56896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.809205Z",
     "start_time": "2025-10-27T19:28:52.061659Z"
    }
   },
   "source": [
    "import math, random\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Toy dataset: y = sin(x) + noise\n",
    "N = 256\n",
    "x = torch.linspace(-3*math.pi, 3*math.pi, N).unsqueeze(1)\n",
    "y = torch.sin(x) + 0.1*torch.randn_like(x)\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(1, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "opt = optim.Adam(mlp.parameters(), lr=1e-2)\n",
    "losses = []\n",
    "for step in range(1500):\n",
    "    opt.zero_grad()\n",
    "    pred = mlp(x)\n",
    "    loss = F.mse_loss(pred, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if step % 100 == 0:\n",
    "        losses.append(loss.item())\n",
    "\n",
    "losses[-5:], float(F.mse_loss(mlp(x), y))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mg/f7gk10vj1_153rz1zkvd13_00000gn/T/ipykernel_3836/3242498083.py:28: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
      "  losses[-5:], float(F.mse_loss(mlp(x), y))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.017481729388237,\n",
       "  0.008053405210375786,\n",
       "  0.00952376052737236,\n",
       "  0.008279295638203621,\n",
       "  0.01027391105890274],\n",
       " 0.05733589082956314)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "b955bb22",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Convolution (continuous):** $(f*g)(t)=\\int f(\\tau)\\,g(t-\\tau)\\,d\\tau$  \n",
    "**Convolution (discrete 2D):**\n",
    "$$\n",
    "(\\omega * f)(x,y)=\\sum_{i=-a}^{a}\\sum_{j=-b}^{b}\\omega(i,j)\\,f(x-i,\\,y-j).\n",
    "$$\n",
    "Deep‑learning libraries often implement **cross‑correlation** (no kernel flip) but with learned weights it’s functionally equivalent.\n",
    "\n",
    "**Properties:** weight sharing, sparse connectivity, receptive fields, multi‑channel kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "636f556c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.820260Z",
     "start_time": "2025-10-27T19:28:52.814330Z"
    }
   },
   "source": [
    "# A tiny ConvNet that shows spatial shape changes via stride/pooling\n",
    "class TinyConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1),  # keep H,W\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                            # halves H,W\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1, stride=2),  # halves H,W again\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),               # global average pooling\n",
    "        )\n",
    "        self.head = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)          # [B, 32, 1, 1]\n",
    "        x = x.flatten(1)         # [B, 32]\n",
    "        return self.head(x)      # [B, 10]\n",
    "\n",
    "x = torch.randn(4, 3, 64, 64)\n",
    "model = TinyConv()\n",
    "out = model(x)\n",
    "x.shape, out.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 64, 64]), torch.Size([4, 10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "8f4803e6",
   "metadata": {},
   "source": [
    "### Downsampling: pooling vs. strided convolutions\n",
    "\n",
    "- **Pooling:** Max/Average over windows to reduce spatial size (translation tolerance).\n",
    "- **Strided conv:** Learnable downsampling by moving the filter with stride $s>1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ccbfe7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.857811Z",
     "start_time": "2025-10-27T19:28:52.852969Z"
    }
   },
   "source": [
    "X = torch.arange(0, 4*4.).view(1,1,4,4)  # simple toy image\n",
    "pool = nn.MaxPool2d(2)\n",
    "conv_s2 = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "with torch.no_grad():\n",
    "    conv_s2.weight.fill_(1/9)  # average-like\n",
    "\n",
    "X, pool(X), conv_s2(X)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "           [ 4.,  5.,  6.,  7.],\n",
       "           [ 8.,  9., 10., 11.],\n",
       "           [12., 13., 14., 15.]]]]),\n",
       " tensor([[[[ 5.,  7.],\n",
       "           [13., 15.]]]]),\n",
       " tensor([[[[ 1.1111,  2.6667],\n",
       "           [ 5.6667, 10.0000]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "871edbc8",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Learns to normalize intermediate activations channel‑wise:\n",
    "$$\n",
    "\\hat{I}_{b,c,x,y} = \\frac{I_{b,c,x,y}-\\mu_c}{\\sqrt{\\sigma_c^2+\\varepsilon}},\\quad\n",
    "O_{b,c,x,y} = \\gamma_c \\hat{I}_{b,c,x,y} + \\beta_c.\n",
    "$$\n",
    "**Effects:** faster training, more stable gradients, allows larger learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f63e874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.868954Z",
     "start_time": "2025-10-27T19:28:52.864070Z"
    }
   },
   "source": [
    "bn = nn.BatchNorm2d(8)\n",
    "T = torch.randn(16, 8, 32, 32)\n",
    "out = bn(T)\n",
    "out.mean(dim=(0,2,3)).abs().max().item(), out.std(dim=(0,2,3)).sub(1).abs().max().item()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7462298274040222e-08, 2.562999725341797e-05)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "eac7f7c8",
   "metadata": {},
   "source": [
    "## Residual connections (ResNets)\n",
    "\n",
    "Skip connections add the input back to the output of a few layers to ease optimization:\n",
    "$$\n",
    "\\mathrm{y} = F(\\mathrm{x}) + \\mathrm{x}.\n",
    "$$\n",
    "This helps gradient flow and enables **very deep** networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b73a6124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.878914Z",
     "start_time": "2025-10-27T19:28:52.873299Z"
    }
   },
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(C, C, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(C)\n",
    "        self.conv2 = nn.Conv2d(C, C, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = F.relu(out + identity)   # residual add\n",
    "        return out\n",
    "\n",
    "blk = BasicBlock(16)\n",
    "y = blk(torch.randn(2,16,32,32))\n",
    "y.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "2e8bceae",
   "metadata": {},
   "source": [
    "## Increasing spatial size: upsampling, unpooling, transposed conv\n",
    "\n",
    "- **Nearest/bilinear upsampling:** Non‑learned resize.\n",
    "- **Max‑unpooling:** Uses saved pooling indices to place values back.\n",
    "- **Transposed convolution:** Learnable upsampling; careful with checkerboard artifacts.\n",
    "\n",
    "Below: simple transposed convolution doubling the size.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "32d8ba8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.885957Z",
     "start_time": "2025-10-27T19:28:52.883269Z"
    }
   },
   "source": [
    "tconv = nn.ConvTranspose2d(4, 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "Z = torch.randn(1,4,8,8)\n",
    "tconv(Z).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 16, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "efff1b85",
   "metadata": {},
   "source": [
    "## Bottleneck layers and $1\\times 1$ convolutions\n",
    "\n",
    "A $1\\times 1$ convolution can **reduce channels** before expensive $3\\times 3$ ops, cutting parameters while preserving depth:\n",
    "- Direct $3\\times 3$ with $C=256$ in/out: $(3\\cdot 3 \\cdot 256 + 1)\\cdot 256$ params.\n",
    "- Bottleneck $256\\to 64 \\to 256$ using $1\\times 1 \\to 3\\times 3 \\to 1\\times 1$ drastically reduces params.\n",
    "\n",
    "This idea appears in **ResNet** and **Inception** families.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b1c073c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.898207Z",
     "start_time": "2025-10-27T19:28:52.893994Z"
    }
   },
   "source": [
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "direct = nn.Conv2d(256, 256, 3, padding=1, bias=True)\n",
    "\n",
    "bottleneck = nn.Sequential(\n",
    "    nn.Conv2d(256, 64, 1, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 256, 1, bias=True),\n",
    ")\n",
    "\n",
    "count_params(direct), count_params(bottleneck)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590080, 70016)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "73070083",
   "metadata": {},
   "source": [
    "## Separable and depthwise‑separable convolutions\n",
    "\n",
    "A 2D kernel may factorize, e.g. smoothing:\n",
    "$$\n",
    "\\frac{1}{3}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\ *\\ \\frac{1}{3}\\begin{bmatrix}1&1&1\\end{bmatrix}\n",
    "= \\frac{1}{9}\\begin{bmatrix}1&1&1\\\\1&1&1\\\\1&1&1\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "**Depthwise‑separable** conv splits **spatial** and **cross‑channel** correlations:\n",
    "1) depthwise: per‑channel $k\\times k$, 2) pointwise: $1\\times 1$ across channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7d494a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.908980Z",
     "start_time": "2025-10-27T19:28:52.905524Z"
    }
   },
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, nin, nout, k=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=k, padding=padding, groups=nin, bias=False)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))\n",
    "\n",
    "dwsc = DepthwiseSeparableConv(32, 64)\n",
    "plain = nn.Conv2d(32, 64, 3, padding=1, bias=False)\n",
    "\n",
    "count_params(plain), count_params(dwsc)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18432, 2336)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "622a94ff",
   "metadata": {},
   "source": [
    "## Multi‑headed networks\n",
    "\n",
    "One shared backbone with **multiple heads** lets you learn related tasks jointly (e.g., policy + value in games, or class + bounding box in detection).\n",
    "\n",
    "Below: a tiny example with two heads and combined losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b829dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.924882Z",
     "start_time": "2025-10-27T19:28:52.915213Z"
    }
   },
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(16,32,3,padding=1),   nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.feat(x).flatten(1)\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.class_head = nn.Linear(32, 5)   # classification logits\n",
    "        self.reg_head   = nn.Linear(32, 2)   # simple 2D regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        return self.class_head(h), self.reg_head(h)\n",
    "\n",
    "net = MultiHead()\n",
    "x = torch.randn(8,3,64,64)\n",
    "cls, reg = net(x)\n",
    "\n",
    "# Example combined loss: cross-entropy + L1\n",
    "target_cls = torch.randint(0,5,(8,))\n",
    "target_reg = torch.randn(8,2)\n",
    "loss = F.cross_entropy(cls, target_cls) + F.l1_loss(reg, target_reg)\n",
    "cls.shape, reg.shape, float(loss)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 5]), torch.Size([8, 2]), 2.2864792346954346)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "6332a654",
   "metadata": {},
   "source": [
    "## Sparsity and pruning (brief)\n",
    "\n",
    "Encourage sparse weights (e.g., with $\\ell_1$ regularization) and prune small‑magnitude connections to reduce compute. Many structured/unstructured pruning algorithms exist. Below: a quick, illustrative unstructured magnitude prune.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "79f8e4c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:52.947484Z",
     "start_time": "2025-10-27T19:28:52.936001Z"
    }
   },
   "source": [
    "# Simple magnitude pruning demonstration\n",
    "lin = nn.Linear(64, 64, bias=False)\n",
    "with torch.no_grad():\n",
    "    lin.weight.normal_(0, 0.1)\n",
    "\n",
    "threshold = lin.weight.abs().quantile(0.8).item()  # keep top-20% magnitude\n",
    "mask = (lin.weight.abs() >= threshold).float()\n",
    "pruned_params = mask.numel() - int(mask.sum())\n",
    "\n",
    "lin.weight.mul_(mask)  # zero-out small weights\n",
    "pruned_params, mask.mean().item()"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m mask \u001B[38;5;241m=\u001B[39m (lin\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mabs() \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m threshold)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m      8\u001B[0m pruned_params \u001B[38;5;241m=\u001B[39m mask\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mint\u001B[39m(mask\u001B[38;5;241m.\u001B[39msum())\n\u001B[0;32m---> 10\u001B[0m \u001B[43mlin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# zero-out small weights\u001B[39;00m\n\u001B[1;32m     11\u001B[0m pruned_params, mask\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem()\n",
      "\u001B[0;31mRuntimeError\u001B[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "172ce1ef",
   "metadata": {},
   "source": [
    "## Classic CNN architectures (very short recap)\n",
    "\n",
    "- **AlexNet:** Early deep CNN; large initial kernels/strides + pooling.\n",
    "- **VGG:** Only $3\\times3$ convs; depth via stacking; heavy parameter counts.\n",
    "- **GoogLeNet/Inception:** Multi‑resolution branches + $1\\times 1$ bottlenecks.\n",
    "- **ResNet:** Residual connections enable very deep nets; bottleneck blocks in deeper variants.\n",
    "\n",
    "> Try swapping blocks in the toy models above to feel differences in parameter counts and shapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af78b3",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "- Replace synthetic data with a small real dataset (e.g., CIFAR‑10) for end‑to‑end demos.\n",
    "- Explore **depthwise separable convs** vs. standard convs on speed and accuracy.\n",
    "- Extend the multi‑head example to include an **auxiliary loss** (e.g., self‑supervised feature loss).\n",
    "\n",
    "This notebook is designed as a compact **practice companion** to the slides.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
