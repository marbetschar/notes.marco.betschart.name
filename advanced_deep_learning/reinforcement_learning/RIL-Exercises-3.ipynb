{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d78c58-adcf-4645-9916-1a3316121f12",
   "metadata": {},
   "source": [
    "# Exercises 3\n",
    "\n",
    "## Gradient Based Method\n",
    "\n",
    "In this exercise we want to explore policy-based methods and first look at a simple hill-climbing approach to solve a reinforcement learning problem and then implement policy gradient as an exercise\n",
    "\n",
    "### Hill climbing\n",
    "In order to develop the algorithm, we need:\n",
    "* a function approximation (neural network) to calculate the _policy_ from the observation,\n",
    "* to sample episodes and calculate the returns (or a similar measure), and\n",
    "* change the network weights using noise\n",
    "\n",
    "We will use the cart pole environment as in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98b7836",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (0.0.9)\n",
      "Requirement already satisfied: ipywidgets in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: pyglet in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (2.0.18)\n",
      "Requirement already satisfied: pygame in /Users/toko/miniforge3/envs/adl/lib/python3.12/site-packages (2.6.1)\n",
      "Matplotlib version: 3.9.1\n",
      "Pyglet version: 2.0.18\n",
      "Ipywidgets version: 8.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc\n",
    "!pip install ipywidgets\n",
    "!pip install pyglet\n",
    "!pip install pygame\n",
    "import jdc\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "print(f'Matplotlib version: {matplotlib.__version__}')\n",
    "print(f'Pyglet version: {pyglet.__version__}')\n",
    "print(f'Ipywidgets version: {ipywidgets.__version__}')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6fccfa-3da9-4939-a71e-f1ae82ceb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72df12-f213-4217-84fe-4b18b9bbdb94",
   "metadata": {},
   "source": [
    "### Build the model for the policy\n",
    "\n",
    "We will define a class for the neural network (the model) that will be used for the agent. We will pass the number of observation values and the number of actions values as parameters.\n",
    "\n",
    "This time, the model must calculate the policy function for each state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc9c401-f28d-4ed5-bfde-5990b8f3de02",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f57f2a0435bd064",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # define a function self.fc that contains the network using nn.Sequential\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(observation_space.shape[0], 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, action_space.n),\n",
    "            torch.nn.Softmax(dim=-1),\n",
    "        )\n",
    "        # self.fc.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def act(self, obs):\n",
    "        # calculate the action and return it\n",
    "        obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "        probs = self.forward(obs)\n",
    "        m = torch.distributions.Categorical(probs=probs)\n",
    "        action = m.sample()\n",
    "        return action.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9787f0e3-966e-4d85-94f8-e3950d45f90a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-02f08792391930df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0.4556, 0.5444]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "policy = PolicyNetwork(env.observation_space, env.action_space)\n",
    "obs_sample = env.observation_space.sample()\n",
    "action = policy.act(obs_sample)\n",
    "print(action)\n",
    "action_prob = policy.forward(torch.from_numpy(obs_sample).float().unsqueeze(0))\n",
    "print(action_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cd294-1491-4191-a25b-de8aafcd7762",
   "metadata": {},
   "source": [
    "### Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. Check the parameters and the descriptions as they will be used in the implementation.\n",
    "\n",
    "We have two attributes in the class that save the best weights and the corresponding best return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fc0213-5dc9-4f23-ab2d-1d44d983a144",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c19c66d131c3fbd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class HillClimbingAgent:\n",
    "    \"\"\"\n",
    "    Implementation of a hill climbing reinforcement learning agent.\n",
    "\n",
    "    The weights of the neural networks are perturbed randomly using noise, if the returns are larger, the new weights\n",
    "    are kept if not the old weights are restored.\n",
    "\n",
    "    The noise of the random changes are diminished if the agent got better and increased if not\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, action_space,\n",
    "                 gamma: float = 0.99):\n",
    "        \"\"\"\n",
    "        Initialize agent\n",
    "        Args:\n",
    "            observation_space: the observation space of the environment\n",
    "            action_space: the action space of the environment\n",
    "            gamma: the discount factor\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # additional hyperparameters\n",
    "        self.min_noise_scale = 0.001\n",
    "        self.max_noise_scale = 2.0\n",
    "\n",
    "        # noise scaling\n",
    "        self.noise_scale = 1.0\n",
    "\n",
    "        # generate the model\n",
    "        self.policy = PolicyNetwork(env.observation_space, env.action_space)\n",
    "\n",
    "        # array to store the rewards for calculating the return\n",
    "        self.rewards = []\n",
    "\n",
    "        # array to store the weights of the model\n",
    "        self.best_weights = [param.data for param in self.policy.parameters()]\n",
    "        self.best_return = -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b09c6-c2b2-45dc-81e2-c87179dab829",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "Next we implement the calculation of the action. In torch we just have to call the appropriate method from the policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808c4b3c-82ec-456a-beb4-f94a3bc1e7b3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-941a10c9caf2e5a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to HillClimbingAgent\n",
    "\n",
    "def calculate_action(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action to take\n",
    "    Args:\n",
    "        obs: the observation\n",
    "    Returns:\n",
    "        the action to take\n",
    "    \"\"\"\n",
    "    return self.policy.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d490aa0b-73f6-4001-8ee5-0387ff91a8dc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b52d56ccd08d24e1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "obs_sample = env.observation_space.sample()\n",
    "agent = HillClimbingAgent(observation_space=env.observation_space,\n",
    "                          action_space=env.action_space)\n",
    "\n",
    "action = agent.calculate_action(obs_sample)\n",
    "assert action == 0 or action == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cbec0-6b65-40b4-9bbf-c0c136afd366",
   "metadata": {},
   "source": [
    "### Step functions and training\n",
    "\n",
    "Next we will add the step function as in the previous implementations of an agent last week. If the reward is equal to None, then this is the first step in the environment.\n",
    "\n",
    "Simular to MC methods, updates only occur at the end of episodes. For the update, you have to check if the return is better than the best return so far. If yes the new weights are stored as best weights, if not, the previous best weights are restored.\n",
    "\n",
    "The current set of weights is then changed by adding random, normally distributed noise which is multiplied by the current noise scale value.\n",
    "\n",
    "This value should be divided by a factor of 2 if the return was better, or multiplied by a factor of 2 if the return was not. The noise should not exceed `self.max_noise_scale` or become smaller than `self.min_noise_scale`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922fa8c8-bc5d-4552-8709-91c6a8c9c421",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f11d5c8957197292",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to HillClimbingAgent\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    if reward == None:\n",
    "        return self.calculate_action(obs)\n",
    "\n",
    "    # append the reward from the last time step\n",
    "    self.rewards.append(reward)\n",
    "\n",
    "    if not done:\n",
    "        # just return an action\n",
    "        return self.calculate_action(obs)\n",
    "    else:\n",
    "        # an episode is finished, so we can calculate the return and update the weights\n",
    "\n",
    "        # calculate total return of the episode (could be done more efficiently)\n",
    "        g = 0.0\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            g = self.gamma * g + self.rewards[step]\n",
    "\n",
    "        if g > self.best_return:\n",
    "            # we have a new best return, so we store the weights\n",
    "            self.best_return = g\n",
    "            self.best_weights = [param.data for param in self.policy.parameters()]\n",
    "            self.noise_scale = max(self.min_noise_scale, self.noise_scale / 2)\n",
    "        else:\n",
    "            # we have a worse return, so we dont save the weights\n",
    "            self.noise_scale = min(self.max_noise_scale, self.noise_scale * 2)\n",
    "\n",
    "        # calculate the new weights based on the best weights\n",
    "        for param, best_weight in zip(self.policy.parameters(), self.best_weights):\n",
    "            noise = torch.randn_like(param.data)\n",
    "            param.data = best_weight + self.noise_scale * noise\n",
    "\n",
    "        # reset the rewards\n",
    "        del self.rewards[:]\n",
    "\n",
    "        # return None, as there is no action from a terminal state\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5ef602",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "hill_climbing_agent = HillClimbingAgent(env.observation_space,\n",
    "                                        env.action_space,\n",
    "                                        gamma=0.99)\n",
    "\n",
    "# Check if one complete episode runs through\n",
    "obs, _ = env.reset()\n",
    "action = hill_climbing_agent.step(obs, None, False)\n",
    "done = False\n",
    "truncated = False\n",
    "while not done and not truncated:\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    action = hill_climbing_agent.step(obs, reward, done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e1f9b-2b90-4d3f-94b7-adf575b31e33",
   "metadata": {},
   "source": [
    "### Training and evaluation\n",
    "\n",
    "We add the train and evaluate methods in the agents, similar to the last exercise so that it is easier to run some tests. Note that the number of steps for training are episodes here, as we only change the weights at the end of episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a08464",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to HillClimbingAgent\n",
    "def train(self, env: gym.Env,\n",
    "          nr_episodes_train: int,\n",
    "          eval_env: gym.Env,\n",
    "          eval_frequency: int,\n",
    "          eval_nr_episodes: int,\n",
    "          eval_gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_train: the number of episodes to train\n",
    "        eval_env: the environment to use for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent (in episodes)\n",
    "        eval_nr_episodes: The number of episodes to evaluate\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step(obs, None, False)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.step(obs, reward, done)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, eval_nr_episodes, eval_gamma)\n",
    "            print(f'Evaluation: Episode trained {nr_episodes}, mean reward: {np.mean(rewards)}')\n",
    "\n",
    "        if nr_episodes > nr_episodes_train:\n",
    "            return\n",
    "\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int, gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "\n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        a = self.calculate_action(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        gamma_current = gamma\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.calculate_action(obs)\n",
    "            episode_reward += gamma_current * reward\n",
    "            gamma_current *= gamma\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e52b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We train the agent for a number of steps to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc24d50",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb7ac82-ea78-4f37-bb3d-86cc843a1d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function with a few steps\n",
    "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
    "obs, _ = env.reset()\n",
    "for _ in range(200):\n",
    "    action = hill_climbing_agent.calculate_action(obs)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c50de3-bccf-42ad-9a05-55694dffc277",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise: Implement Policy Gradient Agent\n",
    "\n",
    "Next we want to implement a policy gradient agent instead of the hill climbing.\n",
    "\n",
    "As exercise, you should implement the training of the agent, all else will be given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d562b26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build the model\n",
    "\n",
    "We can build a similar model as before.\n",
    "\n",
    "For the loss calculation later, we will need not only the selected action, but also the (log) probability of the selected action and the gradient on it. We should make it possible to save this from the policy network. So we implement the `act` method so that it return both the selected action and the log of the probability of choosing this action. We can use the method `log_prob` from `Categorical`. For the log_prob, return the tensor from torch instead of the value (i.e. do not use `item`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e16085b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the policy network\n",
    "class PolicyNetwork2(torch.nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork2, self).__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(observation_space.shape[0], 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, action_space.n),\n",
    "            torch.nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def act(self, obs):\n",
    "        # calculate the action and return it\n",
    "        obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "        probs = self.forward(obs)\n",
    "        m = torch.distributions.Categorical(probs=probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26115a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. We will start with the class definition and the `__init__` method. Check the parameters and the descriptions as they will be used in the implementation. There is one additional array `log_prob` to save the log probabilities of the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3948b2ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VPGAgent:\n",
    "    \"\"\"\n",
    "    Implementation of (vanilla) policy gradient agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space,\n",
    "                 gamma: float = 0.99,\n",
    "                 learning_rate: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize agent\n",
    "        Args:\n",
    "            observation_space: the observation space of the environment\n",
    "            action_space: the action space of the environment\n",
    "            gamma: the discount factor\n",
    "            learning_rate: the learning rate\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # generate the model\n",
    "        self.policy_network = PolicyNetwork2(observation_space, action_space)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # arrays to store an episode for training\n",
    "        self.obs = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9c4b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Action\n",
    "\n",
    "The model directly calculates the policy, so we just have to draw an action from the resulting probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8af7b61f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def calculate_action(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action to take\n",
    "    Args:\n",
    "        obs: the observation\n",
    "    Returns:\n",
    "        the action to take, the log probability of the action\n",
    "    \"\"\"\n",
    "    return self.policy_network.act(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7244eed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step functions and training\n",
    "\n",
    "Next we will add the step functions and the _training_ inside them.\n",
    "\n",
    "### Step and training\n",
    "\n",
    "\n",
    "Simular to MC methods, updates only occur at the end of episodes and we only use the calculated return once for the gradient, however we do this for each of the actions during this episode. Therefor the update batch has the length of an episode.\n",
    "\n",
    "The update of the gradients is according to\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\alpha \\nabla_\\theta J(\\pi_\\theta)\n",
    "$$\n",
    "\n",
    "where $J(\\pi_\\theta)$ is the loss function. In general the gradient of the loss functionhas the form\n",
    "\n",
    "$$\n",
    "\\nabla\\theta J(\\pi\\theta) = \\mathbb{E}\\left[\\sum_{t=0}^T \\nabla_\\theta\\Phi_t\\log\\pi_\\theta(a_t| s_t) \\right]\n",
    "$$\n",
    "\n",
    "where there are different choices for $\\Phi_t$. We can for example use\n",
    "\n",
    "$$\n",
    "\\Phi_t = G\n",
    "$$\n",
    "where $G$ is the (total) return of the episode, or use the obtained return from each state, sometimes also called the sum of the discounted future rewards.\n",
    "$$\n",
    "\\Phi_t = \\sum_{t'=t}^T R_t\n",
    "$$\n",
    "It can be proven, that all these choices actually lead to the same expectation of the gradient. I would suggest to use the sum of discounted future rewards.\n",
    "\n",
    "It can also help to normalize the values to zero mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3daa4f68",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    if reward == None:\n",
    "        # first step\n",
    "        self.obs.append(obs)\n",
    "        action, log_prob = self.calculate_action(obs)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "\n",
    "    # udpate the reward from the last time step (so that all arrays should now have the same length)\n",
    "    self.rewards.append(reward)\n",
    "\n",
    "    if not done:\n",
    "        self.obs.append(obs)\n",
    "        action, log_prob = self.calculate_action(obs)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    else:\n",
    "        # an episode is finished, so we calculate the gradient and update the weights\n",
    "        assert len(self.obs) == len(self.actions)\n",
    "        assert len(self.obs) == len(self.rewards)\n",
    "        assert len(self.obs) == len(self.log_probs)\n",
    "\n",
    "        # Here you have to do the implementation:\n",
    "        # - calculate the returns from each step, \n",
    "        # - calculate the performance (loss) function\n",
    "        # - calculate the gradient\n",
    "        # - change the weights bu doing a step in the obtimizer\n",
    "       \n",
    "\n",
    "\n",
    "        del self.rewards[:]\n",
    "        del self.obs[:]\n",
    "        del self.actions[:]\n",
    "        del self.log_probs[:]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b23bd37c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test implementation\n",
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "agent = VPGAgent(env.observation_space,\n",
    "                 env.action_space,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.001)\n",
    "\n",
    "# Check if one complete episode runs through\n",
    "obs, _ = env.reset()\n",
    "action = agent.step(obs, None, False)\n",
    "done = False\n",
    "truncated = False\n",
    "while not done and not truncated:\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    action = agent.step(obs, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687177d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training and evaluation\n",
    "\n",
    "We add the train and evaluate methods in the agents, similar to the last exercise so that it is easier to run some tests. Nothing to code here. Note that the number of steps for training are episodes here, as we only change the weights at the end of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f917d63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "def train(self, env: gym.Env,\n",
    "          nr_episodes_train: int,\n",
    "          eval_env: gym.Env,\n",
    "          eval_frequency: int,\n",
    "          eval_nr_episodes: int,\n",
    "          eval_gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_train: the number of episodes to train\n",
    "        eval_env: the environment to use for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent (in episodes)\n",
    "        eval_nr_episodes: The number of episodes to evaluate\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step(obs, None, False)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.step(obs, reward, done or truncated)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, eval_nr_episodes, eval_gamma)\n",
    "            print(f'Evaluation: Episode trained {nr_episodes}, mean reward: {np.mean(rewards)}')\n",
    "\n",
    "        if nr_episodes > nr_episodes_train:\n",
    "            return\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int, gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "\n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        a,_ = self.calculate_action(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        gamma_current = gamma\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a, _ = self.calculate_action(obs)\n",
    "            episode_reward += gamma_current * reward\n",
    "            gamma_current *= gamma\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effca67d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We train the agent for a number of steps to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcdb1fd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: Episode trained 25, mean reward: 20.0\n",
      "Evaluation: Episode trained 50, mean reward: 42.0\n",
      "Evaluation: Episode trained 75, mean reward: 27.0\n",
      "Evaluation: Episode trained 100, mean reward: 22.0\n",
      "Evaluation: Episode trained 125, mean reward: 43.0\n",
      "Evaluation: Episode trained 150, mean reward: 25.0\n",
      "Evaluation: Episode trained 175, mean reward: 15.0\n",
      "Evaluation: Episode trained 200, mean reward: 71.0\n",
      "Evaluation: Episode trained 225, mean reward: 13.0\n",
      "Evaluation: Episode trained 250, mean reward: 16.0\n",
      "Evaluation: Episode trained 275, mean reward: 30.0\n",
      "Evaluation: Episode trained 300, mean reward: 24.0\n",
      "Evaluation: Episode trained 325, mean reward: 63.0\n",
      "Evaluation: Episode trained 350, mean reward: 66.0\n",
      "Evaluation: Episode trained 375, mean reward: 156.0\n",
      "Evaluation: Episode trained 400, mean reward: 65.0\n",
      "Evaluation: Episode trained 425, mean reward: 45.0\n",
      "Evaluation: Episode trained 450, mean reward: 97.0\n",
      "Evaluation: Episode trained 475, mean reward: 39.0\n",
      "Evaluation: Episode trained 500, mean reward: 71.0\n",
      "Evaluation: Episode trained 525, mean reward: 182.0\n",
      "Evaluation: Episode trained 550, mean reward: 134.0\n",
      "Evaluation: Episode trained 575, mean reward: 135.0\n",
      "Evaluation: Episode trained 600, mean reward: 176.0\n",
      "Evaluation: Episode trained 625, mean reward: 122.0\n",
      "Evaluation: Episode trained 650, mean reward: 85.0\n",
      "Evaluation: Episode trained 675, mean reward: 226.0\n",
      "Evaluation: Episode trained 700, mean reward: 301.0\n",
      "Evaluation: Episode trained 725, mean reward: 187.0\n",
      "Evaluation: Episode trained 750, mean reward: 500.0\n",
      "Evaluation: Episode trained 775, mean reward: 188.0\n",
      "Evaluation: Episode trained 800, mean reward: 500.0\n",
      "Evaluation: Episode trained 825, mean reward: 451.0\n",
      "Evaluation: Episode trained 850, mean reward: 158.0\n",
      "Evaluation: Episode trained 875, mean reward: 500.0\n",
      "Evaluation: Episode trained 900, mean reward: 500.0\n",
      "Evaluation: Episode trained 925, mean reward: 500.0\n",
      "Evaluation: Episode trained 950, mean reward: 380.0\n",
      "Evaluation: Episode trained 975, mean reward: 500.0\n",
      "Evaluation: Episode trained 1000, mean reward: 500.0\n",
      "Evaluation: [500.0]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "agent = VPGAgent(env.observation_space,\n",
    "                 env.action_space,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.001)\n",
    "\n",
    "agent.train(env, nr_episodes_train=1000, eval_env=eval_env, eval_frequency=25, eval_nr_episodes=1, eval_gamma=1.0)\n",
    "\n",
    "# calculate return at end using evaluation\n",
    "return_eval = agent.evaluate(env=eval_env, nr_episodes=1, gamma=1.0)\n",
    "\n",
    "print(f'Evaluation: {return_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87b96188",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "obs, _  = env.reset()\n",
    "for _ in range(200):\n",
    "    action, _ = agent.calculate_action(obs)\n",
    "    obs, _, _, _,_ = env.step(action)  # Take a random action\n",
    "    display_environment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099146cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
