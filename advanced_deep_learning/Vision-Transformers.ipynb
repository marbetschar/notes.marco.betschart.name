{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f28de0",
   "metadata": {},
   "source": "# Vision Transformers"
  },
  {
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:17.682149Z",
     "start_time": "2025-10-27T19:49:17.246270Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install matplotlib torch",
   "id": "e684815beb91a374",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (3.9.4)\r\n",
      "Requirement already satisfied: torch in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (25.0)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (3.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\r\n",
      "Requirement already satisfied: filelock in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marbetschar/Development/marbetschar/notes/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1dba4ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:17.869284Z",
     "start_time": "2025-10-27T19:49:17.685812Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "import math, random\n",
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# Optional: plotting for attention maps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.__version__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "63726490",
   "metadata": {},
   "source": [
    "## From images to sequences: patchifying\n",
    "\n",
    "A Vision Transformer (ViT) treats an image as a **sequence of patches**. For an image of size $H\\times W$ and a square patch size $P$, the number of patches is\n",
    "$$\n",
    "N = \\frac{H}{P}\\cdot\\frac{W}{P}.\n",
    "$$\n",
    "\n",
    "Each $P\\times P$ patch is flattened and linearly projected to a $D$-dimensional embedding. We then add a learnable **class token** and **position embeddings** (1D over patches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1975e2",
   "metadata": {},
   "source": [
    "## Patch embedding layer\n",
    "\n",
    "A compact way to implement patchifying + linear projection is a strided convolution with kernel size $P$ and stride $P$ producing $D$ channels:\n",
    "$$\n",
    "\\text{PatchEmbed}(x) = \\text{Conv2d}(C\\rightarrow D,\\ \\text{kernel}=P,\\ \\text{stride}=P).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9630828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:17.888751Z",
     "start_time": "2025-10-27T19:49:17.884121Z"
    }
   },
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid = img_size // patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W] -> [B, N, D]\n",
    "        x = self.proj(x)                     # [B, D, H/P, W/P]\n",
    "        x = x.flatten(2).transpose(1, 2)     # [B, N, D]\n",
    "        return x\n",
    "\n",
    "# Shape demo\n",
    "x = torch.randn(2, 3, 32, 32)\n",
    "pe = PatchEmbed(32, 4, 3, 64)\n",
    "pe(x).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "d494c83f",
   "metadata": {},
   "source": [
    "## Class token and positional encodings\n",
    "\n",
    "We prepend a learned **[CLS]** embedding to the patch sequence. The transformer's output at this position is used for classification via an MLP **head**. We also add **learned** position embeddings $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$:\n",
    "$$\n",
    "Z_0 = \\big[ x_{\\text{cls}};\\ X E \\big] + E_{pos}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eff21521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:17.901813Z",
     "start_time": "2025-10-27T19:49:17.897668Z"
    }
   },
   "source": [
    "class AddClassTokenAndPos(nn.Module):\n",
    "    def __init__(self, seq_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos = nn.Parameter(torch.zeros(1, seq_len + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos, std=0.02)\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)         # [B, N+1, D]\n",
    "        return x + self.pos[:, :N+1, :]\n",
    "\n",
    "x = torch.randn(2, 64, 128)  # [B,N,D]\n",
    "wrapper = AddClassTokenAndPos(seq_len=64, embed_dim=128)\n",
    "wrapper(x).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 65, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "c93af5f1",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention (MHSA)\n",
    "\n",
    "Given $X\\in\\mathbb{R}^{B\\times N\\times D}$, we compute queries, keys and values, split into $h$ heads of size $d_h=D/h$:\n",
    "\n",
    "$$\n",
    "Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V,\\qquad\n",
    "\\text{Attn}(Q,K,V)=\\mathrm{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_h}}\\right)V.\n",
    "$$\n",
    "\n",
    "Concatenate head outputs and project with $W_O$.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "211e2f3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:17.915680Z",
     "start_time": "2025-10-27T19:49:17.909427Z"
    }
   },
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim=128, num_heads=4, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.h = num_heads\n",
    "        self.dh = dim // num_heads\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        # for visualization\n",
    "        self.last_attn = None\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B,N,D = x.shape\n",
    "        qkv = self.qkv(x).view(B, N, 3, self.h, self.dh).permute(2,0,3,1,4)  # 3, B, H, N, Dh\n",
    "        Q,K,V = qkv[0], qkv[1], qkv[2]                                       # [B,H,N,Dh]\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.dh)              # [B,H,N,N]\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "        A = scores.softmax(dim=-1)\n",
    "        self.last_attn = A.detach()  # store for later inspection\n",
    "        A = self.attn_drop(A)\n",
    "        y = A @ V                                                         # [B,H,N,Dh]\n",
    "        y = y.transpose(1,2).contiguous().view(B, N, D)                   # [B,N,D]\n",
    "        y = self.proj_drop(self.proj(y))                                  # [B,N,D]\n",
    "        return y\n",
    "\n",
    "# Quick shape check\n",
    "mha = MultiHeadSelfAttention(dim=128, num_heads=8)\n",
    "tok = torch.randn(2, 65, 128)  # include [CLS]\n",
    "mha(tok).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 65, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e5497e5a",
   "metadata": {},
   "source": [
    "## Transformer encoder block (Pre-LN)\n",
    "\n",
    "Each block uses residual connections and layer normalization:\n",
    "$$\n",
    "X \\leftarrow X + \\mathrm{MHSA}(\\mathrm{LN}(X)),\\qquad\n",
    "X \\leftarrow X + \\mathrm{MLP}(\\mathrm{LN}(X)),\n",
    "$$\n",
    "where the MLP is a position-wise 2-layer network with expansion ratio $r$ (e.g., $r=4$).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b95a56a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:17.929673Z",
     "start_time": "2025-10-27T19:49:17.924054Z"
    }
   },
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim=128, num_heads=4, mlp_ratio=4, pdrop=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadSelfAttention(dim, num_heads, proj_drop=pdrop)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim*mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim*mlp_ratio, dim),\n",
    "        )\n",
    "        self.drop = nn.Dropout(pdrop)\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop(self.attn(self.ln1(x)))\n",
    "        x = x + self.drop(self.mlp(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "blk = TransformerEncoderBlock(dim=128, num_heads=8, mlp_ratio=4, pdrop=0.1)\n",
    "blk(torch.randn(2, 65, 128)).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 65, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "484149a9",
   "metadata": {},
   "source": [
    "## Minimal Vision Transformer (ViT)\n",
    "\n",
    "We combine **PatchEmbed**, **[CLS]+Position**, a stack of encoder blocks, and a **classification head** using the [CLS] output:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f88ace8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:17.947297Z",
     "start_time": "2025-10-27T19:49:17.937465Z"
    }
   },
   "source": [
    "class MiniViT(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n",
    "                 embed_dim=128, depth=4, num_heads=4, mlp_ratio=4, pdrop=0.0):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        seq_len = (img_size // patch_size) ** 2\n",
    "        self.add_tokens = AddClassTokenAndPos(seq_len, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, pdrop)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch(x)             # [B,N,D]\n",
    "        x = self.add_tokens(x)        # [B,N+1,D]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        cls = x[:, 0]                 # [B,D]\n",
    "        return self.head(cls)         # [B,num_classes]\n",
    "\n",
    "# Shape demo\n",
    "model = MiniViT(img_size=32, patch_size=4, embed_dim=128, depth=3, num_heads=4, num_classes=5)\n",
    "demo = torch.randn(4,3,32,32)\n",
    "model(demo).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "1ee17ae4",
   "metadata": {},
   "source": [
    "## Toy example: learn to classify simple synthetic patterns\n",
    "\n",
    "We create tiny $32\\times 32$ images with two classes:\n",
    "- Class 0: a bright square in the **top-left** quadrant.\n",
    "- Class 1: a bright square in the **bottom-right** quadrant.\n",
    "\n",
    "This shows the ViT can learn spatial patterns from patches.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "397c77fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:36.418304Z",
     "start_time": "2025-10-27T19:49:17.955234Z"
    }
   },
   "source": [
    "def make_synthetic_dataset(N=512, img_size=32, square=6):\n",
    "    X = torch.zeros(N, 3, img_size, img_size)\n",
    "    y = torch.zeros(N, dtype=torch.long)\n",
    "    for i in range(N):\n",
    "        cls = random.randint(0,1)\n",
    "        y[i] = cls\n",
    "        if cls == 0:\n",
    "            r0, c0 = 3, 3\n",
    "        else:\n",
    "            r0, c0 = img_size - square - 3, img_size - square - 3\n",
    "        X[i,:, r0:r0+square, c0:c0+square] = 1.0\n",
    "        # add tiny noise\n",
    "        X[i] += 0.05*torch.randn_like(X[i])\n",
    "        X[i].clamp_(0,1)\n",
    "    return X, y\n",
    "\n",
    "Xtr, ytr = make_synthetic_dataset(512, 32, square=6)\n",
    "Xte, yte = make_synthetic_dataset(128, 32, square=6)\n",
    "\n",
    "model = MiniViT(img_size=32, patch_size=4, embed_dim=128, depth=4, num_heads=4, num_classes=2, pdrop=0.1)\n",
    "opt = optim.Adam(model.parameters(), lr=3e-3)\n",
    "for step in range(200):\n",
    "    idx = torch.randint(0, Xtr.size(0), (64,))\n",
    "    xb, yb = Xtr[idx], ytr[idx]\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(Xte).argmax(-1)\n",
    "    acc = (pred == yte).float().mean().item()\n",
    "acc"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "f876b74a",
   "metadata": {},
   "source": [
    "## Visualizing an attention map\n",
    "\n",
    "We grab the last attention matrix from the first block to see how the **[CLS]** token attends to patches.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "70935220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:36.580145Z",
     "start_time": "2025-10-27T19:49:36.438222Z"
    }
   },
   "source": [
    "# Forward a small batch and capture attention from the first block\n",
    "_ = model(Xte[:1])\n",
    "attn = model.blocks[0].attn.last_attn  # [B,H,N+1,N+1]\n",
    "attn_cls = attn[0,0,0]                 # head 0, from CLS: [N+1]\n",
    "L = int(math.sqrt(attn_cls.numel()))   # includes CLS; rough grid viz\n",
    "# Plot as a square, ignoring the CLS element for nicer layout\n",
    "fig = plt.figure()\n",
    "grid = int(math.sqrt(attn_cls.numel()-1))\n",
    "plt.imshow(attn_cls[1:].view(grid, grid).cpu().numpy())\n",
    "plt.title(\"Head 0 attention from [CLS] to patches\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "34ea108b",
   "metadata": {},
   "source": [
    "## Effect of patch size on compute\n",
    "\n",
    "Self-attention cost scales as $\\mathcal{O}(N^2)$ with sequence length $N$ (number of patches). For fixed $H,W$:\n",
    "$$\n",
    "N = \\left(\\frac{H}{P}\\right)\\left(\\frac{W}{P}\\right)\\ \\Rightarrow\\ \\text{cost}\\propto N^2 \\propto \\left(\\frac{HW}{P^2}\\right)^2.\n",
    "$$\n",
    "\n",
    "Below we compare sequence lengths and a proxy for attention cost across patch sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ca8776c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:36.618533Z",
     "start_time": "2025-10-27T19:49:36.603556Z"
    }
   },
   "source": [
    "def seq_len_and_cost(img_size=224, patch_sizes=(32,16,14,8)):\n",
    "    rows = []\n",
    "    HW = img_size*img_size\n",
    "    for P in patch_sizes:\n",
    "        N = (img_size//P)*(img_size//P)\n",
    "        proxy = (HW/(P*P))**2\n",
    "        rows.append((P, int(N), int(proxy)))\n",
    "    return rows\n",
    "\n",
    "rows = seq_len_and_cost(224, (32,16,14,8))\n",
    "rows"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, 49, 2401), (16, 196, 38416), (14, 256, 65536), (8, 784, 614656)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "7ed5ba4f",
   "metadata": {},
   "source": [
    "## Parameter scaling and common variants\n",
    "\n",
    "A rough parameter estimate for a ViT-like model with depth $L$, hidden size $D$, MLP ratio $r$, and heads $h$ is dominated by **QKV/O** and **MLP** weights per block:\n",
    "$$\n",
    "\\text{per-block} \\approx 4D^2 \\ (\\text{Q,K,V,O}) \\ + \\ 2DrD \\ (\\text{MLP}).\n",
    "$$\n",
    "\n",
    "We add patch projection and classifier head for a coarse total. This helps relate **ViT-Base/Large/Huge** configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "326c4692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:36.647317Z",
     "start_time": "2025-10-27T19:49:36.632045Z"
    }
   },
   "source": [
    "def vit_param_estimate(img_size=224, patch=16, in_chans=3, D=768, L=12, r=4, heads=12, num_classes=1000):\n",
    "    # Patch projection\n",
    "    patch_params = (in_chans * patch * patch) * D + D  # conv weights + bias\n",
    "    # Per block: QKV/O ~ 4*D^2, MLP ~ 2*D*r*D\n",
    "    per_block = 4*D*D + 2*D*r*D\n",
    "    enc_params = L * per_block\n",
    "    head_params = D * num_classes + num_classes\n",
    "    total = patch_params + enc_params + head_params\n",
    "    return {\"patch\": patch_params, \"encoder_blocks\": enc_params, \"head\": head_params, \"total_approx\": total}\n",
    "\n",
    "est_B16 = vit_param_estimate(D=768, L=12, heads=12, patch=16)\n",
    "est_L16 = vit_param_estimate(D=1024, L=24, heads=16, patch=16)\n",
    "est_H14 = vit_param_estimate(D=1280, L=32, heads=16, patch=14)\n",
    "est_B16, est_L16, est_H14"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'patch': 590592,\n",
       "  'encoder_blocks': 84934656,\n",
       "  'head': 769000,\n",
       "  'total_approx': 86294248},\n",
       " {'patch': 787456,\n",
       "  'encoder_blocks': 301989888,\n",
       "  'head': 1025000,\n",
       "  'total_approx': 303802344},\n",
       " {'patch': 753920,\n",
       "  'encoder_blocks': 629145600,\n",
       "  'head': 1281000,\n",
       "  'total_approx': 631180520})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "8c8add65",
   "metadata": {},
   "source": [
    "## Mixing CNNs and Attention (CoAt-style sketch)\n",
    "\n",
    "Depthwise convolutions excel at local, translation-invariant patterns; attention is **input-adaptive** with global receptive fields. A simple hybrid stacks a few **conv stages** (downsample + depthwise separable conv) followed by **transformer stages**.\n",
    "\n",
    "Below is a tiny hybrid block showing how you might combine them.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4a1d173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:49:36.683416Z",
     "start_time": "2025-10-27T19:49:36.660116Z"
    }
   },
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, C, expand=4):\n",
    "        super().__init__()\n",
    "        self.pw1 = nn.Conv2d(C, C*expand, 1)\n",
    "        self.dw  = nn.Conv2d(C*expand, C*expand, 3, padding=1, groups=C*expand)\n",
    "        self.pw2 = nn.Conv2d(C*expand, C, 1)\n",
    "        self.bn = nn.BatchNorm2d(C)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        x = self.pw1(x); x = self.act(self.dw(x)); x = self.pw2(x)\n",
    "        return self.bn(x)\n",
    "\n",
    "class TinyHybridStage(nn.Module):\n",
    "    def __init__(self, C=32, embed_dim=128, num_heads=4, grid=8):\n",
    "        super().__init__()\n",
    "        self.down = nn.Conv2d(C, C, 3, stride=2, padding=1)  # downsample\n",
    "        self.conv = DepthwiseSeparableConv(C)\n",
    "        self.to_seq = nn.Conv2d(C, embed_dim, 1)\n",
    "        self.blk = TransformerEncoderBlock(embed_dim, num_heads)\n",
    "        self.grid = grid//2\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.to_seq(x)          # [B, D, H', W']\n",
    "        B,D,H,W = x.shape\n",
    "        x = x.flatten(2).transpose(1,2)           # [B, N, D]\n",
    "        x = torch.cat([torch.zeros(B,1,D, device=x.device), x], dim=1)  # add a fake CLS (zero) for demo\n",
    "        x = self.blk(x)\n",
    "        return x\n",
    "\n",
    "hyb = TinyHybridStage(C=16, embed_dim=64, num_heads=4, grid=8)\n",
    "hyb(torch.randn(2,16,16,16)).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 65, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "a3a076e5",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "- Swap the synthetic task for a small real dataset and compare patch sizes ($P{=}$8 vs 16).\n",
    "- Visualize multiple heads/layers to see **attention distance** patterns.\n",
    "- Try a hybrid: 3 convolutional stages followed by 2 transformer stages.\n",
    "- Experiment with **relative position bias** in attention (replace absolute positions).\n",
    "\n",
    "This notebook is designed as a compact **practice companion** to the slides.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
