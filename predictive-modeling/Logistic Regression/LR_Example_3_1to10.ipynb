{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.1\n",
    "The estimated coefficients are\n",
    "$\\hat{\\beta}_{0} = -10.6513$ \n",
    "\n",
    "$\\hat{\\beta}_{1} = 0.0055$\n",
    "\n",
    "Thus, if an individual has **balance = 1000**, then the model yields\n",
    "\\begin{equation}\n",
    "\\hat{p}(1000)\n",
    "=\\dfrac{e^{-10.65+ 0.0055\\cdot 1000}}{1+e^{-10.65+ 0.0055\\cdot 1000}}\n",
    "\\approx 0.00577\n",
    "\\end{equation}"
   ],
   "id": "4b42aa144ce204a6"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:34.368821Z",
     "start_time": "2025-10-20T12:22:34.357017Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/Default.csv', sep=';')\n",
    "\n",
    "# Add a numerical column for default\n",
    "df = df.join(pd.get_dummies(df['default'], \n",
    "                            prefix='default', \n",
    "                            drop_first=True))\n",
    "\n",
    "# Fit logistic model\n",
    "x = df['balance']\n",
    "y = df['default_Yes']\n",
    "\n",
    "x_sm = sm.add_constant(x)\n",
    "\n",
    "model = sm.GLM(y, x_sm, family=sm.families.Binomial())\n",
    "model = model.fit()\n",
    "\n",
    "# Predict for balance = 1000\n",
    "x_pred = [1, 1000]\n",
    "y_pred = model.predict(x_pred)\n",
    "\n",
    "print(y_pred)"
   ],
   "id": "c206e481dfccd9f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00575215]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probability of default is well below $1\\%$, which is very low. However, a different individual with **balance = 2000** has a default probability\n",
    "of approximately $59\\%$."
   ],
   "id": "ad0aab0f8cf8161f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:34.378819Z",
     "start_time": "2025-10-20T12:22:34.376991Z"
    }
   },
   "source": [
    "# Predict for balance = 2000\n",
    "x_pred = [1, 2000]\n",
    "y_pred = model.predict(x_pred)\n",
    "\n",
    "print(y_pred)"
   ],
   "id": "49e0d2d1ef9affe8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58576937]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.2\n",
    "For the **Default** data the following **Python**-code computes the training classification error. "
   ],
   "id": "2c2a863a2fb6cec3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:34.394679Z",
     "start_time": "2025-10-20T12:22:34.391898Z"
    }
   },
   "source": [
    "\"\"\" Follows Example 3.1 \"\"\"\n",
    "# Predict for training data\n",
    "x_pred = x_sm\n",
    "y_pred = model.predict(x_pred)\n",
    "print(y_pred[10])\n",
    "\n",
    "# Round to 0 or 1\n",
    "y_pred = y_pred.round()\n",
    "print(y_pred[10])\n",
    "\n",
    "# Compute training error\n",
    "e_train = abs(y - y_pred)\n",
    "e_train = e_train.mean()\n",
    "\n",
    "print(e_train)"
   ],
   "id": "e9920c98677a8c6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3668765516963094e-05\n",
      "0.0\n",
      "0.0275\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the training error in this example is 0.0275, which is to say that approximately 97.25%  of the cases in the training set are classified correctly. "
   ],
   "id": "777e2764b7e7e9c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.3\n",
    "The following **Python**-code produces the confusion matrix for the **Default** data set and the logistic regression model."
   ],
   "id": "3defc94f1e60033c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:34.410855Z",
     "start_time": "2025-10-20T12:22:34.403019Z"
    }
   },
   "source": [
    "\"\"\" Follows Example 3.2 \"\"\"\n",
    "# Create confusion matrix\n",
    "confusion = pd.DataFrame({'predicted': y_pred,\n",
    "                          'true': y})\n",
    "confusion = pd.crosstab(confusion.predicted, confusion.true, \n",
    "                        margins=True, margins_name=\"Sum\")\n",
    "\n",
    "print(confusion)"
   ],
   "id": "2fb208f14e697ade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true       False  True    Sum\n",
      "predicted                    \n",
      "0.0         9625   233   9858\n",
      "1.0           42   100    142\n",
      "Sum         9667   333  10000\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that out of $9667$ cases with **default=No**, the vast majority of $9625$ are classified correctly. On the other hand, only approximately $1/3$\n",
    "of the **default=Yes** cases are classified correctly. The confusion matrix shows that the present classification scheme is by no means useful, in particular,\n",
    "if you want to predict the case of **default=Yes**.\n",
    "\n",
    "The reason for this bad result is the *imbalance* of the two classes. The training data only contains $333$ out of $ 10000$ cases with **default=Yes**.\n",
    "Therefore, the likelihood function is dominated by the factors corresponding to **default=No**, so the parameters are chosen as to match mainly those \n",
    "cases. Note also that the trivial classifier predicting all observations $x$ to $\\hat{f}(x)=0$ has a classification error of $333/10000=0.0333$ which \n",
    "is not much worse than that of our logistic model.\n",
    "\n",
    "The situation can also be visualized by the histograms of the estimated probabilities of **default=Yes** separated by true class.\n",
    "\n",
    "It is striking that the **default=No** group has a high concentration of probabilities near $0$ which is reasonable for this group. On the other hand, though,\n",
    "the estimated probabilities for the **default=Yes** cases do not exhibit high mass at $1$. Instead, the maximal probability is attained close to $0$ as well!"
   ],
   "id": "8868cb4d44dbc6c3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.8\n",
    "We can compute the F1 score by means of the"
   ],
   "id": "77ba7e5f7dfdfa04"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:37.796587Z",
     "start_time": "2025-10-20T12:22:34.418784Z"
    }
   },
   "source": [
    "\"\"\" Follows Example 3.3 \"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Find F1-score\n",
    "f1 = f1_score(y, y_pred, pos_label=1, average='binary')\n",
    "print(f1)\n"
   ],
   "id": "57f273337dd8cd2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42105263157894735\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pos\\_label**  is an optional character string for the factor level that corresponds to a *positive* result."
   ],
   "id": "7d3aac02feb1bfb6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.9\n",
    "If we consider the case **default=No** as positive, then the F1 score changes to"
   ],
   "id": "8514fd6f99d4de35"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:37.824462Z",
     "start_time": "2025-10-20T12:22:37.819933Z"
    }
   },
   "source": [
    "\"\"\" Follows Example 3.8 \"\"\"\n",
    "# Find F1-score\n",
    "f1 = f1_score(y, y_pred, pos_label=0, average='binary')\n",
    "print(f1)\n"
   ],
   "id": "e5d2dfcd33aa39e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9859154929577465\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example 3.10\n",
    "We analyze the **Default** data set and fit a logistic regression model by downsampling the **default=No** class to the same size as the **default=yes** case. "
   ],
   "id": "7f41623684898c95"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:37.835509Z",
     "start_time": "2025-10-20T12:22:37.829046Z"
    }
   },
   "source": [
    "\"\"\" Follows Example 3.9 \"\"\"\n",
    "# Set ramdom seed\n",
    "np.random.seed(1)\n",
    "# Index of Yes:\n",
    "i_yes = df.loc[df['default_Yes'] == 1, :].index\n",
    "\n",
    "# Random set of No:\n",
    "i_no = df.loc[df['default_Yes'] == 0, :].index\n",
    "i_no = np.random.choice(i_no, replace=False, size=333)\n",
    "\n",
    "# Fit Linear Model on downsampled data\n",
    "i_ds = np.concatenate((i_no, i_yes))\n",
    "x_ds = df.iloc[i_ds]['balance']\n",
    "y_ds = df.iloc[i_ds]['default_Yes']\n",
    "\n",
    "x_sm = sm.add_constant(x_ds)\n",
    "\n",
    "model_ds = sm.GLM(y_ds, x_sm, family=sm.families.Binomial())\n",
    "model_ds = model_ds.fit()\n",
    "\n",
    "# Predict for downsampled data\n",
    "x_pred_ds = x_sm\n",
    "y_pred_ds = model_ds.predict(x_pred_ds)\n",
    "\n",
    "# Round to 0 or 1\n",
    "y_pred_ds = y_pred_ds.round()\n",
    "\n",
    "# Classification error on training data:\n",
    "e_train = abs(y_ds- y_pred_ds)\n",
    "e_train = e_train.mean()\n",
    "\n",
    "print(np.round(e_train, 4))"
   ],
   "id": "ea0c0a9f22150158",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1171\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:37.856970Z",
     "start_time": "2025-10-20T12:22:37.847823Z"
    }
   },
   "source": [
    "# Create confusion matrix\n",
    "confusion = pd.DataFrame({'predicted': y_pred_ds,\n",
    "                          'true': y_ds})\n",
    "confusion = pd.crosstab(confusion.predicted, confusion.true, \n",
    "                        margins=True, margins_name=\"Sum\")\n",
    "\n",
    "print(confusion)"
   ],
   "id": "8e145d8b58a7d164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true       False  True  Sum\n",
      "predicted                  \n",
      "0.0          293    38  331\n",
      "1.0           40   295  335\n",
      "Sum          333   333  666\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T12:22:37.867076Z",
     "start_time": "2025-10-20T12:22:37.862545Z"
    }
   },
   "source": [
    "# Print F1-scores\n",
    "f1_pos = f1_score(y_ds, y_pred_ds, pos_label=1, average='binary')\n",
    "f1_neg = f1_score(y_ds, y_pred_ds, pos_label=0, average='binary')\n",
    "\n",
    "print('\\nF1-Score (positive = default) = \\n', f1_pos,\n",
    "      '\\nF1-Score (positive = not-default) = \\n', f1_neg)"
   ],
   "id": "52df8d75541a1166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1-Score (positive = default) = \n",
      " 0.8832335329341318 \n",
      "F1-Score (positive = not-default) = \n",
      " 0.8825301204819277\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the downsampled training set, the confusion matrix is balanced, and the classification error is 0.1171, which amounts to 88.29% correctly classified samples. As we observe now, the F1 score for **default=Yes** as positive case has now considerably improved.\n",
    "\n",
    "Furthermore, the histograms of the predicted probabilities have a complete different shape than before. The separation of the two classes becomes clearly visible."
   ],
   "id": "cdfd8ddcce5891ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
