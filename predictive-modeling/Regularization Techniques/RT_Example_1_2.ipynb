{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Methods Example 1.2:\n",
    "In the following, we will discuss the function **sklearn.linear\\_model.Ridge()** in depth. Firstly, note that the qualitative predictors in x have to be transformed into dummy variables. The flag **normalize = True** makes sure that the predictors are mean centred and scaled to unit variance. When comparing to **R**, note that the implementation is slightly different, which makes it hard to compare coefficients as a function of lambda. The optimal solution however, will generally be the same. \n",
    "\n",
    "We will now perform ridge regression in order to predict **Balance** in the **Credit** data set."
   ],
   "id": "d15c4f48395d3eb7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T07:31:57.979909Z",
     "start_time": "2025-10-23T07:31:57.200092Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/Credit.csv', index_col=\"Unnamed: 0\")\n",
    "\n",
    "# Convert Categorical variables\n",
    "df = pd.get_dummies(data=df, drop_first=True, \n",
    "                    prefix=('Gender_', 'Student_', \n",
    "                            'Married_', 'Ethnicity_'))\n",
    "\n",
    "# Define target and predictors\n",
    "x = df.drop(columns='Balance') \n",
    "y = df['Balance']\n",
    "\n",
    "# Fit model:\n",
    "lambda_ = 100\n",
    "reg = Ridge(alpha=lambda_) #, normalize=True) normalize was removed in recent versions of scikit-learn\n",
    "reg = reg.fit(x, y)\n",
    "\n",
    "# Coefficient and corresponding predictors\n",
    "coef = np.round(reg.coef_, 3)\n",
    "# coef = scaler.inverse_transform(coef)\n",
    "x_cols = x.columns.values"
   ],
   "id": "bd52ec6efeb05555",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller, in terms of $\\ell_2$ norm, when a large value of $\\lambda$ is used, as compared to when a small value of $\\lambda$ is used. These are the coefficients when $\\lambda = 100$, along with their $\\ell_2$ norm:\n"
   ],
   "id": "8c534cfe57144218"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T07:32:01.090086Z",
     "start_time": "2025-10-23T07:32:01.086318Z"
    }
   },
   "source": [
    "print(pd.DataFrame(data={'Feature': x_cols,\n",
    "                         'Coefficient':coef}),\n",
    "      '\\n\\nl2-norm:', np.sqrt(np.sum(coef**2)))"
   ],
   "id": "a59965260416fd5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Coefficient\n",
      "0                 Income       -7.624\n",
      "1                  Limit        0.131\n",
      "2                 Rating        2.007\n",
      "3                  Cards       10.820\n",
      "4                    Age       -0.831\n",
      "5              Education        1.210\n",
      "6           Gender__Male        0.053\n",
      "7           Student__Yes      110.837\n",
      "8           Married__Yes      -12.679\n",
      "9       Ethnicity__Asian        9.062\n",
      "10  Ethnicity__Caucasian       -1.347 \n",
      "\n",
      "l2-norm: 112.74277058419312\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, here are the coefficients when $\\lambda = 50$, along with their $\\ell_2$ norm. Note the much larger $\\ell_2$ norm of the coefficients associated with this smaller value of $\\lambda$."
   ],
   "id": "f174e65339c32ba3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T07:32:09.680330Z",
     "start_time": "2025-10-23T07:32:09.674640Z"
    }
   },
   "source": [
    "# Fit model:\n",
    "lambda_ = 50\n",
    "reg = Ridge(alpha=lambda_) #, normalize=True)\n",
    "reg = reg.fit(x, y)\n",
    "\n",
    "# Coefficient and corresponding predictors\n",
    "coef = np.round(reg.coef_, 3)\n",
    "x_cols = x.columns.values\n",
    "\n",
    "print(pd.DataFrame(data={'Feature': x_cols,\n",
    "                         'Coefficient':coef}),\n",
    "      '\\n\\nl2-norm:', np.sqrt(np.sum(coef**2)))"
   ],
   "id": "d66367f3b95ee8ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Coefficient\n",
      "0                 Income       -7.659\n",
      "1                  Limit        0.143\n",
      "2                 Rating        1.821\n",
      "3                  Cards       12.583\n",
      "4                    Age       -0.793\n",
      "5              Education        0.769\n",
      "6           Gender__Male        1.502\n",
      "7           Student__Yes      175.642\n",
      "8           Married__Yes      -14.776\n",
      "9       Ethnicity__Asian       12.068\n",
      "10  Ethnicity__Caucasian        0.478 \n",
      "\n",
      "l2-norm: 177.30795899225728\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard least squares coefficient estimates are scale equivariant: multiplying a predictor variable $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. In other words, regardless of how the $j$th predictor is scaled, $\\hat{\\beta}_j X_j $ will remain the\n",
    "same. In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. For instance, consider the **income** variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result in a reduction in the observed values of income by a factor of $1000$. Now due to the sum of squared coefficients in the ridge regression formulation equation, such a change in scale will not simply cause the ridge regression coefficient estimate for **income** to change by a factor of $1000$. In other words,\n",
    "$\\hat{\\beta}_j X_{j,\\lambda}^{R} $ will depend not only on the value of $\\lambda$, but also on the scaling of the $j$th predictor. In fact, the value of $\\hat{\\beta}_j X_{j,\\lambda}^{R} $ may even depend on the scaling of the other predictors. Therefore, it is best to apply ridge regression after standardizing the predictors.\n",
    "\n",
    "Note that by default, the **Ridge()** function does not standardizes the variables. To turn on scaling, use the argument **normalize = True**."
   ],
   "id": "e6f34489c14cd92f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
