{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Multiple Linear Regression 4.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following **Python**-output displays the correlation matrix for the **Credit** data set. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T07:52:52.908533Z",
     "start_time": "2025-10-09T07:52:52.691459Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/Credit.csv')\n",
    "\n",
    "# Drop all quantitative columns\n",
    "df = df.drop(['Unnamed: 0','Gender','Student','Married','Ethnicity'], \n",
    "             axis=1)\n",
    "\n",
    "# Find the correlation Matrix using DataFrame.corr()\n",
    "print(round(df.corr(), 4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Income   Limit  Rating   Cards     Age  Education  Balance\n",
      "Income     1.0000  0.7921  0.7914 -0.0183  0.1753    -0.0277   0.4637\n",
      "Limit      0.7921  1.0000  0.9969  0.0102  0.1009    -0.0235   0.8617\n",
      "Rating     0.7914  0.9969  1.0000  0.0532  0.1032    -0.0301   0.8636\n",
      "Cards     -0.0183  0.0102  0.0532  1.0000  0.0429    -0.0511   0.0865\n",
      "Age        0.1753  0.1009  0.1032  0.0429  1.0000     0.0036   0.0018\n",
      "Education -0.0277 -0.0235 -0.0301 -0.0511  0.0036     1.0000  -0.0081\n",
      "Balance    0.4637  0.8617  0.8636  0.0865  0.0018    -0.0081   1.0000\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **Python**-output we read off that the  correlation coefficient between **limit** and **age** is $ 0.101 $ which corresponds to a rather weak correlation. On the other hand, we find for the correlation between **limit** and **rating** a value of $ 0.997 $ which is very large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Multiple Linear Regression 4.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Credit** data, a regression of **balance** on **age**, **rating**, and **limit** indicates that the predictors have VIF values of 1.01, 160.67, and 160.59. As we suspected, there is considerable collinearity in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T07:52:53.378854Z",
     "start_time": "2025-10-09T07:52:52.923777Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "# Define the linear model:\n",
    "x = pd.DataFrame({\n",
    "    'Age' : df['Age'], \n",
    "    'Rating' : df['Rating'],\n",
    "    'Limit' : df['Limit']})\n",
    "y = df['Balance']\n",
    "\n",
    "# VIF Analysis\n",
    "x_c = sm.add_constant(x)\n",
    "VIF  = []\n",
    "for i in range(1,4):\n",
    "    VIF.append(variance_inflation_factor(x_c.to_numpy(), i))\n",
    "    \n",
    "print(list(x.columns), '\\n', np.round(VIF, 3))\n",
    "\n",
    "# R Squared for 'complete' system\n",
    "# Fit models\n",
    "x_sm = sm.add_constant(x)\n",
    "model = sm.OLS(y, x_sm).fit()\n",
    "\n",
    "print('\\nRsquared for the complete model is given by:\\n', \n",
    "      np.round(model.rsquared, 4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Rating', 'Limit'] \n",
      " [  1.011 160.668 160.593]\n",
      "\n",
      "Rsquared for the complete model is given by:\n",
      " 0.7536\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Multiple Linear Regression 4.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. \n",
    "\n",
    "For instance, if we regress **balance** onto **age** and **limit**, without the **rating** predictor, then the resulting VIF values are close to the minimum possible value of 1, and the $ R^2 $ drops from 0.754 to 0.75. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T07:52:53.398159Z",
     "start_time": "2025-10-09T07:52:53.394949Z"
    }
   },
   "source": [
    "# Define the linear model:\n",
    "x = x.drop('Rating', axis=1, errors='ignore')\n",
    "\n",
    "# Fit models\n",
    "x_sm = sm.add_constant(x)\n",
    "model = sm.OLS(y, x_sm).fit()\n",
    "\n",
    "# Print result\n",
    "print('\\n Rsquared without \\'Rating\\' is given by:\\n', \n",
    "      np.round(model.rsquared, 4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Rsquared without 'Rating' is given by:\n",
      " 0.7498\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So dropping **rating** from the set of predictors has effectively solved the collinearity problem without compromosing  the fit. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
