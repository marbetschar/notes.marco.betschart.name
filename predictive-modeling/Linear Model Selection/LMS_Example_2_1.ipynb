{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model Selection Example 2.1\n",
    "For the  **Credit** example, we begin with the so-called *null model* $ \\mathcal{M}_{0} $, which contains no predictors:\n",
    "\\begin{equation*}\n",
    "balance\n",
    "=\\beta_{0}+\\epsilon\n",
    "\\end{equation*}\n",
    "Then, we add a predictor variable to the null model. \n",
    "For this example, we will write two **Python**-functions, one to fit a linear model and return important scoring metrics and one to add one predictor to a model, based on the returned scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Predictor          AIC        R2           RSS\n",
      "0             Unnamed: 0  6042.696603  0.000037  8.433681e+07\n",
      "1                 Income  5945.894250  0.214977  6.620874e+07\n",
      "2                  Limit  5499.982633  0.742522  2.171566e+07\n",
      "3                 Rating  5494.781548  0.745848  2.143512e+07\n",
      "4                  Cards  6039.710202  0.007475  8.370950e+07\n",
      "5                    Age  6042.709965  0.000003  8.433963e+07\n",
      "6              Education  6042.685316  0.000065  8.433443e+07\n",
      "7         Gender__Female  6042.526817  0.000461  8.430102e+07\n",
      "8           Student__Yes  6014.932656  0.067090  7.868154e+07\n",
      "9           Married__Yes  6042.698437  0.000032  8.433720e+07\n",
      "10      Ethnicity__Asian  6042.672799  0.000096  8.433179e+07\n",
      "11  Ethnicity__Caucasian  6042.706987  0.000011  8.433900e+07 \n",
      "\n",
      "Best predictor is: Rating\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/Credit.csv')\n",
    "\n",
    "# Convert Categorical variables\n",
    "df = pd.get_dummies(data=df, drop_first=True, \n",
    "                    prefix=('Gender_', 'Student_', \n",
    "                            'Married_', 'Ethnicity_'))\n",
    "\n",
    "x_full = df.drop(columns='Balance')\n",
    "y = df['Balance']\n",
    "\n",
    "\n",
    "def fit_linear_reg(x, y):\n",
    "    '''Fit Linear model with predictors x on y \n",
    "    return AIC, BIC, R2 and R2 adjusted '''\n",
    "    x = sm.add_constant(x)\n",
    "    # Create and fit model\n",
    "    model_k = sm.OLS(y, x).fit()\n",
    "    \n",
    "    # Find scores\n",
    "    BIC = model_k.bic\n",
    "    AIC = model_k.aic\n",
    "    R2 = model_k.rsquared\n",
    "    R2_adj = model_k.rsquared_adj\n",
    "    RSS = model_k.ssr\n",
    "    \n",
    "    # Return result in Series\n",
    "    results = pd.Series(data={'BIC': BIC, 'AIC': AIC, 'R2': R2,\n",
    "                              'R2_adj': R2_adj, 'RSS': RSS})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def add_one(x_full, x, y, scoreby='RSS'):\n",
    "    ''' Add possible predictors from x_full to x, \n",
    "    Fit a linear model on y using fit_linear_reg\n",
    "    Returns Dataframe showing scores as well as best model '''\n",
    "    # Predefine DataFrame\n",
    "    x_labels = x_full.columns\n",
    "    zeros = np.zeros(len(x_labels))\n",
    "    results = pd.DataFrame(\n",
    "        data={'Predictor': x_labels.values, 'BIC': zeros, \n",
    "               'AIC': zeros, 'R2': zeros, \n",
    "               'R2_adj': zeros, 'RSS': zeros})\n",
    "\n",
    "    # For every predictor find R^2, RSS, and AIC\n",
    "    for i in range(len(x_labels)):\n",
    "        x_i = np.concatenate((x, [np.array(x_full[x_labels[i]])]))\n",
    "        results.iloc[i, 1:] = fit_linear_reg(x_i.T, y)\n",
    "        \n",
    "    # Depending on where we scoreby, we select the highest or lowest\n",
    "    if scoreby in ['RSS', 'AIC', 'BIC']:\n",
    "        best = x_labels[results[scoreby].argmin()]\n",
    "    elif scoreby in ['R2', 'R2_adj']:\n",
    "        best = x_labels[results[scoreby].argmax()]\n",
    "        \n",
    "    return results, best \n",
    "\n",
    "\n",
    "# Define the empty predictor\n",
    "x_empty = [np.zeros(len(y))]\n",
    "\n",
    "results, best1 = add_one(x_full, x_empty, y)\n",
    "\n",
    "print(results[['Predictor', 'AIC', 'R2', 'RSS']], \n",
    "      '\\n\\nBest predictor is:',  best1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the definition used in a helper file, **LMS_def.py**.\n",
    "\n",
    "We now choose the *best* variable in the sense that adding this variable leads to the regression model with the lowest RSS or the highest $ R^2 $. The variable that results in the model with the lowest RSS is in this case **rating**. \n",
    "\n",
    "Thus, we have found the model $ \\mathcal{M}_{1} $\n",
    "\\begin{equation*}\n",
    "balance\n",
    "=\\beta_{0}+\\beta_{1}\\cdot rating +\\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "We now add a further predictor variable to this model by first updating the reference model and removing the chosen predictor from the set of possible predictors. Subsequently, we can run the same procedure and decide which predictor variable to add next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Predictor          AIC        R2           RSS\n",
      "0             Unnamed: 0  5496.518489  0.746016  2.142103e+07\n",
      "1                 Income  5212.557085  0.875118  1.053254e+07\n",
      "2                  Limit  5496.632982  0.745943  2.142716e+07\n",
      "3                  Cards  5494.187124  0.747492  2.129654e+07\n",
      "4                    Age  5484.481339  0.753545  2.078601e+07\n",
      "5              Education  5496.272851  0.746171  2.140788e+07\n",
      "6         Gender__Female  5496.481640  0.746039  2.141906e+07\n",
      "7           Student__Yes  5372.232473  0.813849  1.569996e+07\n",
      "8           Married__Yes  5494.569548  0.747250  2.131691e+07\n",
      "9       Ethnicity__Asian  5496.067431  0.746302  2.139689e+07\n",
      "10  Ethnicity__Caucasian  5496.772749  0.745854  2.143465e+07 \n",
      "\n",
      "Best predictor is: Income\n"
     ]
    }
   ],
   "source": [
    "# Update the empty predictor with the best predictor\n",
    "x_1 = [df[best1]]\n",
    "# Remove the chosen predictor from the list of options\n",
    "x_red = x_full.drop(columns=best1, errors='ignore')\n",
    "\n",
    "results, best2 = add_one(x_red, x_1, y)\n",
    "\n",
    "print(results[['Predictor', 'AIC', 'R2', 'RSS']], \n",
    "      '\\n\\nBest predictor is:',  best2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select again the variable that leads when added to the reference model to the lowest RSS. In this case, we select the predictor variable **income** which gives us the model $ \\mathcal{M}_2 $:\n",
    "\\begin{equation*}\n",
    "balance\n",
    "=\\beta_{0}+\\beta_{1}\\cdot rating+\\beta_{2}\\cdot income +\\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "This procedure will be repeated. In particular, we will add one variable among the \n",
    "remaining $ p - 2 $ variables to the model $ \\mathcal{M}_{2} $. The resulting model with the lowest RSS will become model $ \\mathcal{M}_{3} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Predictor          AIC        R2           RSS\n",
      "0            Unnamed: 0  5214.551863  0.875120  1.053240e+07\n",
      "1                 Limit  5210.950291  0.876239  1.043800e+07\n",
      "2                 Cards  5214.477534  0.875143  1.053045e+07\n",
      "3                   Age  5211.113461  0.876188  1.044226e+07\n",
      "4             Education  5213.765645  0.875365  1.051172e+07\n",
      "5        Gender__Female  5214.521087  0.875129  1.053159e+07\n",
      "6          Student__Yes  4849.386992  0.949879  4.227219e+06\n",
      "7          Married__Yes  5210.930247  0.876245  1.043747e+07\n",
      "8      Ethnicity__Asian  5212.042074  0.875901  1.046653e+07\n",
      "9  Ethnicity__Caucasian  5213.976405  0.875299  1.051726e+07 \n",
      "\n",
      "Best predictor is: Student__Yes\n"
     ]
    }
   ],
   "source": [
    "# Update the empty predictor with the best predictor\n",
    "x_2 = np.concatenate((x_1, [df[best2]]))\n",
    "# Remove the chosen predictor from the list of options\n",
    "x_red = x_red.drop(columns=best2, errors='ignore')\n",
    "\n",
    "results, best3 = add_one(x_red, x_2, y)\n",
    "\n",
    "print(results[['Predictor', 'AIC', 'R2', 'RSS']], \n",
    "      '\\n\\nBest predictor is:',  best3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automatically select the n-best predictors using SequentialFeatureSelector from **sklearn.feature_selection**. Therfore, we need to define the Linear model with **sklearn.linear_model.LinearRegression**. \n",
    "The chosen predictors are returned in the support_ attribute.\n",
    "**Note**: If *None* features are selected, the algorithm automatically choses half number of features given. \n",
    "\n",
    "In this case, the predictor **student** turns out to be the variable that we add to model $ \\mathcal{M}_{2} $ to obtain model $ \\mathcal{M}_{3} $:\n",
    "\\begin{equation*}\n",
    "balance\n",
    "=\\beta_{0}+\\beta_{1}\\cdot rating+\\beta_{2}\\cdot income +\\beta_{3}\\cdot student+\\epsilon\n",
    "\\end{equation*}\n",
    "We will end up with the following models: \n",
    " $\\mathcal{M}_{0},\\mathcal{M}_{1},\\ldots,\\mathcal{M}_{10}$. \n",
    "\n",
    "But how are we going to identify the *best* model among these 11 models? We may base our decision on the value of the AIC which is listed in the **Python** output. This statistic allows us to compare different models with each other. We will later discuss the AIC in greater detail. \n",
    "\n",
    "The procedure we have followed can also be automated using **sklear.feature\\_selection**. However, this is a relatively new package, which is not as flexible or extensive yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Income' 'Rating' 'Student__Yes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# define Linear Regression Model in sklearn\n",
    "linearmodel = LinearRegression()\n",
    "# Sequential Feature Selection using sklearn\n",
    "sfs = SequentialFeatureSelector(linearmodel, n_features_to_select=3, \n",
    "                                direction='forward')\n",
    "sfs.fit(x_full, y)\n",
    "\n",
    "# Print Chosen variables\n",
    "print(x_full.columns[sfs.support_].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with three predictor variables includes the variables **income**, **rating** and **student**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
