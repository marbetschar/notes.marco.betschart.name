{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559b9261",
   "metadata": {},
   "source": [
    "# Bayesian Networks - Causal Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de0070",
   "metadata": {},
   "source": [
    "## Causal AI and the limits of pure prediction\n",
    "\n",
    "Many impressive machine learning systems (including large language models) excel at **prediction**:\n",
    "\n",
    "- Given $X$, they can accurately predict $Y$.\n",
    "\n",
    "However, prediction alone is not sufficient when we care about:\n",
    "\n",
    "- **Interventions**: *What happens if we change $X$?*\n",
    "- **Policy decisions**: *Should we give a drug? Launch a campaign? Change a workflow?*\n",
    "- **Fairness and bias**: *Is a model discriminating against a particular group?*\n",
    "- **Explanation and trust**: *Why did this happen? What are the causes?*\n",
    "\n",
    "A central message of the slides:\n",
    "\n",
    "> **Causality** is the missing link between pattern recognition and reasoning.\n",
    "\n",
    "Causal questions require knowledge about **how variables influence each other**, not just how they correlate in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176774ad",
   "metadata": {},
   "source": [
    "## Causality vs. correlation and Simpson’s paradox\n",
    "\n",
    "The slides use examples like a drug study and exercise–cholesterol relationships to show:\n",
    "\n",
    "- Purely observational statistics can lead to **paradoxes**.\n",
    "- Aggregated data can suggest one conclusion, while **stratified data** suggest the opposite.\n",
    "\n",
    "A classic example is **Simpson’s paradox**:\n",
    "\n",
    "- A treatment seems worse overall, but better for each subgroup (e.g. men and women separately).\n",
    "- The paradox is resolved when we realize that a **confounder** (e.g. gender) influences both:\n",
    "  - treatment assignment (who gets the drug), and\n",
    "  - outcome (recovery).\n",
    "\n",
    "Key idea:\n",
    "\n",
    "- A **confounder** is a variable that is a **common cause** of both treatment and outcome.\n",
    "- To correctly assess the causal effect of a treatment, we must **control for confounders**.\n",
    "\n",
    "Informally:\n",
    "\n",
    "> Causality is about understanding which variables **produce** changes in others, not just which ones **move together**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b218d",
   "metadata": {},
   "source": [
    "## Informal definition of causation\n",
    "\n",
    "The slides give an intuitive definition:\n",
    "\n",
    "- A variable $Y$ is a **direct cause** of a variable $X$ if $Y$ appears in the function that assigns $X$’s value.\n",
    "- A variable $Y$ is a **cause** of $X$ if it is a direct cause of $X$, or a cause of some other variable that causes $X$.\n",
    "\n",
    "In other words:\n",
    "\n",
    "- If changing $Y$ (while holding all other relevant inputs fixed) would change $X$, then $Y$ is a cause of $X$.\n",
    "- Causal relations are fundamentally about **what would happen under interventions**, not just observed co-occurrence.\n",
    "\n",
    "This motivates a more formal framework: **structural causal models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd51d1",
   "metadata": {},
   "source": [
    "## Structural Causal Models (SCMs)\n",
    "\n",
    "A **structural causal model** (SCM) describes how variables in a system are generated from exogenous noise and other\n",
    "variables via **structural equations**.\n",
    "\n",
    "Formal definition:\n",
    "\n",
    "A structural causal model\n",
    "$$\n",
    "M = \\langle V, U, F, P(u) \\rangle\n",
    "$$\n",
    "consists of:\n",
    "\n",
    "- $V = \\{V_1,\\dots,V_n\\}$: **endogenous variables**\n",
    "  - Determined *within* the model.\n",
    "  - Each $V_i$ has at least one cause (either exogenous or endogenous).\n",
    "\n",
    "- $U = \\{U_1,\\dots,U_m\\}$: **exogenous variables**\n",
    "  - External to the model; we do not model their causes.\n",
    "  - Represent background factors and noise.\n",
    "\n",
    "- $F = \\{f_1,\\dots,f_n\\}$: **structural functions**\n",
    "  - Each $V_i$ is determined by a function\n",
    "    $$\n",
    "    v_i = f_i(\\text{pa}_i, u_i),\n",
    "    $$\n",
    "    where:\n",
    "    - $\\text{pa}_i \\subseteq V$ are the **endogenous parents** (causes) of $V_i$,\n",
    "    - $u_i \\subseteq U$ are the exogenous parents of $V_i$.\n",
    "\n",
    "- $P(u)$: joint probability distribution over the exogenous variables $U$.\n",
    "\n",
    "Assumption in the slides:\n",
    "\n",
    "- The model is **recursive** (or acyclic): there are no feedback loops or cycles such that a variable is ultimately a\n",
    "  cause of itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f1c9f",
   "metadata": {},
   "source": [
    "### Example of structural equations\n",
    "\n",
    "Example from the slides: exercise, diet, and fitness level.\n",
    "\n",
    "Variables:\n",
    "\n",
    "- $X$: amount of exercise,\n",
    "- $Y$: diet quality,\n",
    "- $Z$: fitness level.\n",
    "\n",
    "Exogenous variables:\n",
    "\n",
    "- $U_X$: external motivation affecting exercise,\n",
    "- $U_Y$: education / culture affecting diet,\n",
    "- $U_Z$: genetic predisposition and other unobserved factors affecting fitness.\n",
    "\n",
    "Structural equations:\n",
    "\n",
    "$$\n",
    "X = f_X(U_X),\n",
    "$$\n",
    "$$\n",
    "Y = f_Y(U_Y),\n",
    "$$\n",
    "$$\n",
    "Z = f_Z(X, Y, U_Z).\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- $X$ and $Y$ are **causes** of $Z$ because they appear in the structural function $f_Z$.\n",
    "- $U_X$, $U_Y$, and $U_Z$ are exogenous and are not caused by any variable in $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33701a",
   "metadata": {},
   "source": [
    "## Structural Equation Models (SEMs)\n",
    "\n",
    "A **structural equation model** (SEM) is a special case of an SCM with:\n",
    "\n",
    "- A fixed causal ordering of variables.\n",
    "- **Linear** structural functions.\n",
    "- Typically Gaussian exogenous noise.\n",
    "\n",
    "For example, with variables $X, Y, Z$ and exogenous errors $\\varepsilon_X, \\varepsilon_Y, \\varepsilon_Z$:\n",
    "\n",
    "$$\n",
    "Z = \\beta_{Z0} + \\varepsilon_Z,\n",
    "$$\n",
    "$$\n",
    "X = \\beta_{X0} + \\beta_{XZ} Z + \\varepsilon_X,\n",
    "$$\n",
    "$$\n",
    "Y = \\beta_{Y0} + \\beta_{YZ} Z + \\beta_{YX} X + \\varepsilon_Y.\n",
    "$$\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- Exogenous errors are often taken to be independent Gaussians:\n",
    "  $$\n",
    "  \\varepsilon_X, \\varepsilon_Y, \\varepsilon_Z \\sim \\mathcal{N}(0,\\Sigma),\n",
    "  $$\n",
    "  where $\\Sigma$ is typically diagonal.\n",
    "\n",
    "SEMs are widely used in fields like psychology and economics, where linear relationships and Gaussian noise are\n",
    "reasonable approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208d1f0",
   "metadata": {},
   "source": [
    "## Graphical causal models\n",
    "\n",
    "Every SCM can be associated with a **graphical causal model** (a directed graph):\n",
    "\n",
    "- **Nodes**: variables in $U$ and $V$.\n",
    "- **Directed edges**: from each parent to its child, according to the structural equations.\n",
    "\n",
    "Graph construction:\n",
    "\n",
    "- For each structural equation\n",
    "  $$\n",
    "  V_i = f_i(\\text{pa}_i, u_i),\n",
    "  $$\n",
    "  draw directed edges from each variable in $\\text{pa}_i$ and $u_i$ to $V_i$.\n",
    "\n",
    "Properties:\n",
    "\n",
    "- Exogenous variables ($U$) appear as **root nodes** (no parents).\n",
    "- Endogenous variables ($V$) are **descendants** of at least one exogenous variable.\n",
    "- In models considered here, the graph is a **directed acyclic graph (DAG)**:\n",
    "  - No cycles or self-loops.\n",
    "  - You cannot follow directed edges and return to the same node.\n",
    "\n",
    "Graphical definition of causation:\n",
    "\n",
    "- If there is a directed edge $Y \\to X$, then $Y$ is a **direct cause** of $X$.\n",
    "- If there is a directed path from $Y$ to $X$, then $Y$ is a **potential cause** of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ca420",
   "metadata": {},
   "source": [
    "## Basics of graph terminology\n",
    "\n",
    "Some basic concepts from graph theory used in the slides:\n",
    "\n",
    "- A **graph** consists of nodes (vertices) and edges (links).\n",
    "- Edges can be:\n",
    "  - **Undirected**: no arrowheads, just a line between nodes.\n",
    "  - **Directed**: arrows indicating direction.\n",
    "\n",
    "In a **directed graph**:\n",
    "\n",
    "- If there is a directed edge $Y \\to X$:\n",
    "  - $Y$ is a **parent** of $X$,\n",
    "  - $X$ is a **child** of $Y$.\n",
    "- If there is a directed path $Y \\to \\dots \\to X$:\n",
    "  - $Y$ is an **ancestor** of $X$,\n",
    "  - $X$ is a **descendant** of $Y$.\n",
    "\n",
    "A **Directed Acyclic Graph (DAG)** is a directed graph with **no cycles**:\n",
    "\n",
    "- You cannot start at a node, follow directed edges, and come back to the same node.\n",
    "- DAGs are the graph type used in **Bayesian networks** and many causal models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d72108",
   "metadata": {},
   "source": [
    "## Why use graphical models instead of raw SCM equations?\n",
    "\n",
    "SCMs specify **exact functional relationships** $V_i = f_i(\\text{pa}_i, u_i)$, which can be difficult to elicit and\n",
    "manipulate directly.\n",
    "\n",
    "Graphical models offer:\n",
    "\n",
    "- A **qualitative** description of causal structure (who causes whom),\n",
    "- A compact way to encode **conditional independencies** between variables,\n",
    "- A way to **factorize** joint probability distributions into simpler pieces,\n",
    "- An intuitive visual language for communicating assumptions.\n",
    "\n",
    "Typically:\n",
    "\n",
    "- We first think in terms of **graphs / DAGs**,\n",
    "- Then attach **local probability distributions** (Bayesian networks),\n",
    "- And sometimes further refine to **structural equations** when functional forms are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf6587",
   "metadata": {},
   "source": [
    "## Bayesian networks: definition\n",
    "\n",
    "Let $X = (X_1,\\dots,X_n)$ be random variables.\n",
    "\n",
    "A **Bayesian network** is:\n",
    "\n",
    "- A **DAG** whose nodes are the variables $X_1,\\dots,X_n$.\n",
    "- For each node $X_i$, a **local conditional distribution**\n",
    "  $$\n",
    "  p(x_i \\mid x_{\\text{Parents}(i)}),\n",
    "  $$\n",
    "  where $\\text{Parents}(i)$ are the parents of $X_i$ in the DAG.\n",
    "\n",
    "The Bayesian network defines a **joint distribution** over all variables by the product\n",
    "\n",
    "$$\n",
    "P(X_1 = x_1,\\dots,X_n = x_n)\n",
    "= \\prod_{i=1}^n p(x_i \\mid x_{\\text{Parents}(i)}).\n",
    "$$\n",
    "\n",
    "Key points:\n",
    "\n",
    "- For nodes with **no parents**, $p(x_i \\mid x_{\\text{Parents}(i)})$ is just a **marginal** $p(x_i)$.\n",
    "- For nodes with parents, we specify **conditional probabilities** given the parent configuration.\n",
    "- The arrows in a **probabilistic** Bayesian network encode **conditional dependence structure** (not automatically causality).\n",
    "\n",
    "The slides distinguish later between **probabilistic** Bayesian networks and **causal** Bayesian networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95235e",
   "metadata": {},
   "source": [
    "## Explaining away with Bayesian networks\n",
    "\n",
    "A classic structure from the slides:\n",
    "\n",
    "- $B$: burglary (0/1),\n",
    "- $E$: earthquake (0/1),\n",
    "- $A$: alarm (0/1).\n",
    "\n",
    "Graph:\n",
    "\n",
    "$$\n",
    "B \\to A \\leftarrow E.\n",
    "$$\n",
    "\n",
    "Local distributions:\n",
    "\n",
    "- $p(b)$ and $p(e)$ for burglary and earthquake.\n",
    "- $p(a \\mid b,e)$ for the alarm, which depends on both $B$ and $E$.\n",
    "\n",
    "Joint factorization:\n",
    "\n",
    "$$\n",
    "P(B=b, E=e, A=a)\n",
    "= p(b)\\, p(e)\\, p(a \\mid b,e).\n",
    "$$\n",
    "\n",
    "**Explaining away**:\n",
    "\n",
    "- Suppose we know $A=1$ (alarm goes off).\n",
    "- Burglary and earthquake are both possible causes.\n",
    "- If we now learn that $E=1$ (there is an earthquake), the probability of burglary **decreases**:\n",
    "  $$\n",
    "  P(B=1 \\mid A=1, E=1) < P(B=1 \\mid A=1).\n",
    "  $$\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- Before knowing $E$, a burglary was a plausible explanation for the alarm.\n",
    "- Once we know there *was* an earthquake, the alarm is largely “explained” by $E$, so a burglary becomes less likely.\n",
    "\n",
    "This **explaining away** effect is typical for converging connections (called **colliders**) $B \\to A \\leftarrow E$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fa1d4",
   "metadata": {},
   "source": [
    "### Remarks on explaining away\n",
    "\n",
    "Important details emphasized in the slides:\n",
    "\n",
    "- Without conditioning on the effect $A$, the causes $B$ and $E$ can be **independent**:\n",
    "  $$\n",
    "  P(B \\mid E) = P(B).\n",
    "  $$\n",
    "- Once we **condition on the effect** $A=1$, $B$ and $E$ become **dependent**:\n",
    "  $$\n",
    "  P(B \\mid A=1,E=1) \\ne P(B \\mid A=1).\n",
    "  $$\n",
    "- This induced dependence through a common effect is central to understanding many probabilistic puzzles.\n",
    "\n",
    "Takeaway:\n",
    "\n",
    "> In Bayesian networks, **conditioning on a collider** can create dependencies between its parents (explaining away)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32dc67",
   "metadata": {},
   "source": [
    "## Medical diagnosis example: cold or allergies?\n",
    "\n",
    "The slides present a small Bayesian network for a toy medical diagnosis problem.\n",
    "\n",
    "Variables:\n",
    "\n",
    "- $C$: cold,\n",
    "- $A$: allergies,\n",
    "- $H$: cough,\n",
    "- $I$: itchy eyes.\n",
    "\n",
    "Graph structure:\n",
    "\n",
    "- $C \\to H$,\n",
    "- $A \\to H$,\n",
    "- $A \\to I$.\n",
    "\n",
    "Factorization of the joint distribution:\n",
    "\n",
    "$$\n",
    "P(C=c, A=a, H=h, I=i)\n",
    "= p(c)\\, p(a)\\, p(h \\mid c,a)\\, p(i \\mid a).\n",
    "$$\n",
    "\n",
    "Given this model, we can ask questions like:\n",
    "\n",
    "- $P(C=1 \\mid H=1)$: probability of a cold given a cough.\n",
    "- $P(C=1 \\mid H=1, I=1)$: probability of a cold given both cough and itchy eyes.\n",
    "\n",
    "Typically, we observe an **explaining away** effect:\n",
    "\n",
    "- Observing itchy eyes makes allergies more likely,\n",
    "- Which in turn reduces the probability that the cough is due to a cold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d1433",
   "metadata": {},
   "source": [
    "## Flu virus example and conditional independence\n",
    "\n",
    "Another example in the slides:\n",
    "\n",
    "Variables:\n",
    "\n",
    "- $T \\in \\{\\text{cold},\\text{hot}\\}$: ambient temperature,\n",
    "- $V \\in \\{\\text{yes},\\text{no}\\}$: presence of flu virus,\n",
    "- $F \\in \\{\\text{sick},\\neg\\text{sick}\\}$: having flu.\n",
    "\n",
    "Graph:\n",
    "\n",
    "$$\n",
    "T \\to V \\to F.\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Cold temperature $T$ influences the prevalence of the flu virus $V$,\n",
    "- The presence of the virus $V$ influences whether a person gets sick $F$.\n",
    "\n",
    "Key conditional independence:\n",
    "\n",
    "- When the virus status $V$ is known, temperature and sickness are **conditionally independent**:\n",
    "  $$\n",
    "  T \\perp F \\mid V.\n",
    "  $$\n",
    "\n",
    "The chain rule factorization (using the graph) is:\n",
    "\n",
    "$$\n",
    "P(F=f,V=v,T=t)\n",
    "= p(f \\mid v)\\, p(v \\mid t)\\, p(t).\n",
    "$$\n",
    "\n",
    "From this factorization we can compute:\n",
    "\n",
    "- Joint distributions over $(F,V,T)$,\n",
    "- Marginals like $P(F=f,T=t)$,\n",
    "- Conditionals like $p(t \\mid f)$ using **marginalization** and **Bayes’ rule**.\n",
    "\n",
    "For example,\n",
    "\n",
    "$$\n",
    "p(t \\mid f)\n",
    "= \\frac{P(f,t)}{P(f)}\n",
    "= \\frac{\\sum_v P(f,v,t)}{\\sum_{t'} \\sum_v P(f,v,t')}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9c392",
   "metadata": {},
   "source": [
    "## Compactness of Bayesian networks\n",
    "\n",
    "A full joint distribution over $n$ discrete variables typically requires specifying probabilities for **all combinations** of values.\n",
    "\n",
    "- If each variable has $k$ states, we need about $k^n$ numbers.\n",
    "\n",
    "Bayesian networks exploit the graph structure to express the joint as\n",
    "\n",
    "$$\n",
    "P(X_1,\\dots,X_n) = \\prod_{i=1}^n p(X_i \\mid \\text{Parents}(i)).\n",
    "$$\n",
    "\n",
    "Each local distribution involves only a **small subset** of variables, greatly reducing the number of parameters needed.\n",
    "\n",
    "Flu example:\n",
    "\n",
    "$$\n",
    "P(F,V,T)\n",
    "= p(F \\mid V)\\, p(V \\mid T)\\, p(T),\n",
    "$$\n",
    "\n",
    "so we only need:\n",
    "\n",
    "- A table for $p(T)$,\n",
    "- A table for $p(V \\mid T)$,\n",
    "- A table for $p(F \\mid V)$,\n",
    "\n",
    "instead of a full table for $P(F,V,T)$.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- More **compact** representation,\n",
    "- Easier **elicitation** from domain experts,\n",
    "- More efficient **computation** and **inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597ee70",
   "metadata": {},
   "source": [
    "## Probabilistic inference in Bayesian networks\n",
    "\n",
    "General inference problem:\n",
    "\n",
    "- Given:\n",
    "  - A Bayesian network defining $P(X_1,\\dots,X_n)$,\n",
    "  - **Evidence** $E=e$ (observed values for a subset of variables $E \\subset X$),\n",
    "  - A query set $Q \\subset X$,\n",
    "\n",
    "- Compute:\n",
    "  $$\n",
    "  P(Q \\mid E=e),\n",
    "  $$\n",
    "  often represented as a probability table over all values of $Q$.\n",
    "\n",
    "Example query:\n",
    "\n",
    "- In the medical diagnosis network:\n",
    "  $$\n",
    "  P(C \\mid H=1, I=1),\n",
    "  $$\n",
    "  the probability of a cold given a cough and itchy eyes.\n",
    "\n",
    "In principle, inference can be done by:\n",
    "\n",
    "1. Forming the joint distribution via the product rule.\n",
    "2. Summing out (marginalizing) **non-query, non-evidence** variables.\n",
    "3. Normalizing to get conditional probabilities.\n",
    "\n",
    "However:\n",
    "\n",
    "- Exact inference can be computationally expensive in large networks.\n",
    "- Many specialized algorithms (variable elimination, message passing, approximate methods) have been developed.\n",
    "\n",
    "The slides focus more on the **conceptual structure** rather than algorithms, but the key idea is:\n",
    "\n",
    "> Once the graph and local conditional distributions are specified, any probability over variables can, in principle, be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ed208",
   "metadata": {},
   "source": [
    "## Probabilistic vs causal Bayesian networks vs SCMs\n",
    "\n",
    "The slides conclude by contrasting three related but distinct notions:\n",
    "\n",
    "1. **Probabilistic Bayesian network**\n",
    "   - Arrows into a node $Y$ indicate that the probability of $Y$ is described by\n",
    "     $$\n",
    "     p(y \\mid \\text{Parents}(Y)).\n",
    "     $$\n",
    "   - The network defines a **joint distribution** purely as a factorizable probability model.\n",
    "   - Arrows encode conditional dependencies, but not necessarily causal relations.\n",
    "\n",
    "2. **Causal Bayesian network**\n",
    "   - Same graphical structure, but arrows are interpreted **causally**.\n",
    "   - Conditional probabilities represent the distribution of $Y$ under **interventions** on its parents:\n",
    "     $$\n",
    "     p(y \\mid \\text{do}(\\text{Parents}(Y))).\n",
    "     $$\n",
    "   - Still specifies probabilities, but with an explicit causal interpretation.\n",
    "\n",
    "3. **Structural causal model (SCM)**\n",
    "   - Consists of structural functions $V_i = f_i(\\text{pa}_i, u_i)$ and a distribution over $U$.\n",
    "   - No explicit conditional probability tables at the level of the structural functions.\n",
    "   - Causal semantics arise directly from the **structural equations**, and interventions correspond to modifying them.\n",
    "\n",
    "In short:\n",
    "\n",
    "- SCMs provide the most detailed **causal mechanism**.\n",
    "- Causal Bayesian networks provide a **graphical abstraction** of an SCM.\n",
    "- Probabilistic Bayesian networks may have the same graphical structure but do **not** automatically encode causality.\n",
    "\n",
    "Interventions (using Pearl’s **do-calculus**) and causal effects are treated in more detail in later lectures (Bayesian Networks II/III)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec764dee",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "This notebook contains the text of **Series 1 – Bayesian Networks** exercises, converted from the PDF into\n",
    "Markdown with LaTeX formulas using the `$...$` / `$$...$$` notation.\n",
    "\n",
    "You can use this as a starting point to write your own solutions in additional cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2e21d",
   "metadata": {},
   "source": [
    "## Problem 1.1 — Simpson's paradox in batting averages\n",
    "\n",
    "For baseball fans: the table below gives hits and at-bats for two players (David Justice and Derek Jeter) in three seasons.\n",
    "\n",
    "| Player        | 1995                    | 1996                    | 1997                    | All three years            |\n",
    "|--------------|-------------------------|-------------------------|-------------------------|----------------------------|\n",
    "| David Justice | $104/411 \\approx 0.253$ | $45/140 \\approx 0.321$  | $163/495 \\approx 0.329$ | $312/1046 \\approx 0.298$   |\n",
    "| Derek Jeter  | $12/48 \\approx 0.250$   | $183/582 \\approx 0.314$ | $190/654 \\approx 0.291$ | $385/1284 \\approx 0.300$   |\n",
    "\n",
    "Note the paradoxical pattern: Justice has the higher batting average in each of the three seasons (1995, 1996, 1997),\n",
    "yet Jeter has the higher batting average when the three seasons are combined. In this exercise you will explain\n",
    "qualitatively how this **Simpson** phenomenon can arise and (optionally) analyze the data quantitatively with Bayesian models.\n",
    "\n",
    "**(a)** How can one player be a worse hitter than the other in 1995, 1996, and 1997 but better over the three-year\n",
    "period? Dissolve the paradox qualitatively.\n",
    "\n",
    "**(b) (Optional)** Propose a hierarchical (exchangeable) Beta–Binomial model that\n",
    "\n",
    "1. models season-level batting probabilities $\\theta_{p,s}$,\n",
    "2. allows pooling across seasons (and possibly across players), and\n",
    "3. has weakly informative hyperpriors.\n",
    "\n",
    "Compare the conclusions (probability statements, intervals) from the hierarchical model to the independent per-season\n",
    "and aggregated analyses. Which analysis do you find most persuasive for estimating future batting performance and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fee59",
   "metadata": {},
   "source": [
    "## Problem 1.2 — Aggregated vs segregated data\n",
    "\n",
    "In each of the following scenarios, you are asked to decide whether the **aggregate data** (pooled across all subgroups)\n",
    "or the **segregated data** (analyzed separately by relevant subgroups) should be used to infer the true **causal effect**.\n",
    "\n",
    "Provide a short justification for each answer, explaining which type of data better represents the causal relationship\n",
    "of interest.\n",
    "\n",
    "**(a) Kidney stone treatments I.**  \n",
    "In an observational study published in 1996, open surgery to remove kidney stones had a better success rate than\n",
    "endoscopic surgery for small kidney stones. It also had a better success rate for large kidney stones. However, it had\n",
    "a lower success rate overall. Dissolve the paradox.\n",
    "\n",
    "**(b) Kidney stone treatments II.**  \n",
    "There are two treatments used on kidney stones: Treatment A and Treatment B. Doctors are more likely to use Treatment A\n",
    "on large (and therefore more severe) stones and more likely to use Treatment B on small stones. Should a patient who\n",
    "doesn’t know the size of his or her stone examine the **general population data** or the **stone size-specific data**\n",
    "when deciding which treatment they would like to request?\n",
    "\n",
    "**(c) Smoking and thyroid disease survival.**  \n",
    "A 1995 study on thyroid disease reported that smokers had a higher twenty-year survival rate (76%) than nonsmokers (69%).\n",
    "However, when survival rates were analyzed within seven age groups, nonsmokers had higher survival in six of the seven\n",
    "groups, with only a minimal difference in the remaining one. To assess the causal effect of smoking on survival, should\n",
    "the analysis be based on the overall (aggregate) survival rates or on the age-specific (segregated) data?\n",
    "\n",
    "**(d) Surgical performance of two doctors.**  \n",
    "In a small town, two doctors have each performed 100 surgeries, divided into two categories: one very difficult surgery\n",
    "and one very easy surgery. Doctor 1 performs mostly easy surgeries, while Doctor 2 performs mostly difficult ones.\n",
    "You need surgery but do not know whether your case will be easy or difficult. To maximize your chances of a successful\n",
    "operation, should you compare the doctors’ overall (aggregate) success rates, or their success rates within each type\n",
    "of surgery (segregated data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fbba6c",
   "metadata": {},
   "source": [
    "## Problem 1.3 — Simpson’s reversal with a lollipop\n",
    "\n",
    "In an attempt to estimate the effectiveness of a new drug, a randomized experiment is conducted. Half of the patients\n",
    "(50%) are assigned to receive the new drug, and the remaining half (50%) are assigned to receive a placebo.\n",
    "\n",
    "A day before the actual experiment, a nurse distributes lollipops to some patients who show signs of depression. By\n",
    "coincidence, most of these patients happen to be in the treatment-bound ward—that is, among those who will receive the\n",
    "new drug the next day.\n",
    "\n",
    "When the experiment is analyzed, an unexpected pattern emerges: a **Simpson’s reversal**. Although the drug appears\n",
    "beneficial to the population as a whole, within both subgroups (lollipop receivers and nonreceivers) drug takers are\n",
    "less likely to recover than nontakers.\n",
    "\n",
    "Assume that receiving and sucking a lollipop has **no direct effect** whatsoever on recovery.\n",
    "\n",
    "Using this setup, answer the following questions:\n",
    "\n",
    "**(a)** Is the drug beneficial to the population as a whole or harmful?\n",
    "\n",
    "**(b)** Does your answer contradict the gender example discussed in class, where sex-specific data were deemed more\n",
    "appropriate for determining the causal effect?\n",
    "\n",
    "**(c)** Draw (informally) a causal graph that captures the essential structure of the story.\n",
    "\n",
    "**(d)** Explain how Simpson’s reversal arises in this scenario. What roles do the variables lollipop, treatment\n",
    "assignment, and recovery play?\n",
    "\n",
    "**(e)** Would your explanation change if the lollipops were handed out (according to the same criterion) *after* the\n",
    "study rather than before?\n",
    "\n",
    "**Hint.** Receiving a lollipop is an indicator of two things:\n",
    "\n",
    "- a higher likelihood of being assigned to the drug treatment group, and\n",
    "- a higher likelihood of depression, which in turn is associated with a lower probability of recovery.\n",
    "\n",
    "Use this information to reason about the direction of confounding and the emergence of the Simpson’s reversal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82651ffe",
   "metadata": {},
   "source": [
    "## Problem 1.4 — The Monty Hall problem\n",
    "\n",
    "In the late 1980s, a writer named Marilyn vos Savant started a regular column in *Parade* magazine, a weekly supplement\n",
    "to the Sunday newspaper in many U.S. cities. Her column, **Ask Marilyn**, continues to this day and features her answers\n",
    "to various puzzles, brainteasers, and scientific questions submitted by readers. The magazine billed her as “the world’s\n",
    "smartest woman,” which undoubtedly motivated readers to come up with a question that would stump her.\n",
    "\n",
    "Of all the questions she ever answered, none created a greater furor than this one, which appeared in a column in\n",
    "September 1990:\n",
    "\n",
    "> “Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the\n",
    "> others, goats. You pick a door, say #1, and the host, who knows what’s behind the doors, opens another door, say\n",
    "> #3, which has a goat. He says to you, ‘Do you want to pick door #2?’ Is it to your advantage to switch your choice\n",
    "> of doors?”\n",
    "\n",
    "For American readers, the question was obviously based on a popular televised game show called *Let’s Make a Deal*,\n",
    "whose host, Monty Hall, used to play precisely this sort of mind game with the contestants. In her answer, vos Savant\n",
    "argued that contestants should switch doors. By not switching, they would have only a one-in-three probability of\n",
    "winning; by switching, they would double their chances to two in three.\n",
    "\n",
    "Even the smartest woman in the world could never have anticipated what happened next. Over the next few months, she\n",
    "received more than 10,000 letters from readers, most of them disagreeing with her, and many of them from people who\n",
    "claimed to have PhDs in mathematics or statistics. A small sample of the comments from academics includes:\n",
    "\n",
    "- “You blew it, and you blew it big!” (Scott Smith, PhD)\n",
    "- “May I suggest that you obtain and refer to a standard textbook on probability before you try to answer a question of\n",
    "  this type again?” (Charles Reid, PhD)\n",
    "- “You blew it!” (Robert Sachs, PhD)\n",
    "- “You are utterly incorrect.” (Ray Bobo, PhD)\n",
    "\n",
    "In general, the critics argued that it shouldn’t matter whether you switch doors or not—there are only two doors left\n",
    "in the game, and you have chosen your door completely at random, so the probability that the car is behind your door\n",
    "must be one-half either way.\n",
    "\n",
    "**(a)** Who was right? Who was wrong? And why does the problem incite such passion? Provide a qualitative explanation\n",
    "by considering the following table:\n",
    "\n",
    "| Door 1 | Door 2 | Door 3 | Outcome if you **switch** | Outcome if you **stay** |\n",
    "|--------|--------|--------|----------------------------|-------------------------|\n",
    "| Auto   | Goat   | Goat   | Lose                       | Win                     |\n",
    "| Goat   | Auto   | Goat   | Win                        | Lose                    |\n",
    "| Goat   | Goat   | Auto   | Win                        | Lose                    |\n",
    "\n",
    "The table shows that switching doors is **twice as attractive** as not switching.\n",
    "\n",
    "**(b)** Prove, using Bayes’ theorem, that switching doors improves your chances of winning the car in the Monty Hall\n",
    "problem.\n",
    "\n",
    "**(c)** Define the structural model that corresponds to the Monty Hall problem, and use it to describe the joint\n",
    "distribution of all variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b276e5",
   "metadata": {},
   "source": [
    "## Problem 1.5 — Small Bayesian network calculations\n",
    "\n",
    "Consider the following Bayesian network containing four Boolean random variables $A,B,C,D$:\n",
    "\n",
    "- $A$ and $B$ have no parents,\n",
    "- $C$ has parent $A$,\n",
    "- $D$ has parents $A$ and $B$.\n",
    "\n",
    "The conditional probability tables are:\n",
    "\n",
    "- $P(A) = 0.1$\n",
    "- $P(B) = 0.5$\n",
    "- $P(C \\mid A) = 0.7$\n",
    "- $P(C \\mid \\neg A) = 0.2$\n",
    "\n",
    "and for $D$:\n",
    "\n",
    "- $P(D \\mid A,B) = 0.9$\n",
    "- $P(D \\mid A,\\neg B) = 0.7$\n",
    "- $P(D \\mid \\neg A,B) = 0.6$\n",
    "- $P(D \\mid \\neg A,\\neg B) = 0.3$\n",
    "\n",
    "**(a)** Compute $P(\\neg A, B, \\neg C, D)$.\n",
    "\n",
    "Choose one:\n",
    "\n",
    "A. $0.216$  \n",
    "B. $0.054$  \n",
    "C. $0.024$  \n",
    "D. $0.006$  \n",
    "E. None of the above\n",
    "\n",
    "---\n",
    "\n",
    "**(b)** Compute $P(A \\mid B, C, D)$.\n",
    "\n",
    "Choose one:\n",
    "\n",
    "A. $0.0315$  \n",
    "B. $0.0855$  \n",
    "C. $0.368$  \n",
    "D. $0.583$  \n",
    "E. None of the above\n",
    "\n",
    "---\n",
    "\n",
    "**(c)** True or False: The Bayesian network associated with the computation\n",
    "\n",
    "$$\n",
    "P(A)\\,P(B)\\,P(C \\mid A,B)\\,P(D \\mid C)\\,P(E \\mid B,C)\n",
    "$$\n",
    "\n",
    "has edges $A \\to C$, $B \\to C$, $B \\to E$, $C \\to D$, $C \\to E$ and no other edges.\n",
    "\n",
    "---\n",
    "\n",
    "**(d)** True or False: The product\n",
    "\n",
    "$$\n",
    "P(A \\mid B)\\,P(B \\mid C)\\,P(C \\mid D)\\,P(D \\mid A)\n",
    "$$\n",
    "\n",
    "corresponds to a valid Bayesian network over $A,B,C,D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88877b16",
   "metadata": {},
   "source": [
    "## Problem 1.6 — Constructing and reading a Bayesian network\n",
    "\n",
    "**(a)** Given the tables below, draw a **minimal** representative Bayesian network for this model. Be sure to label all\n",
    "nodes and the directionality of the edges.\n",
    "\n",
    "Marginal for $D$:\n",
    "\n",
    "| $D$ | $P(D)$ |\n",
    "|-----|--------|\n",
    "| $+$ | $0.1$  |\n",
    "| $-$ | $0.9$  |\n",
    "\n",
    "Conditional for $B$ given $D$:\n",
    "\n",
    "| $D$ | $B$ | $P(B \\mid D)$ |\n",
    "|-----|-----|---------------|\n",
    "| $+$ | $+$ | $0.7$         |\n",
    "| $+$ | $-$ | $0.3$         |\n",
    "| $-$ | $+$ | $0.5$         |\n",
    "| $-$ | $-$ | $0.5$         |\n",
    "\n",
    "Conditional for $X$ given $D$:\n",
    "\n",
    "| $D$ | $X$ | $P(X \\mid D)$ |\n",
    "|-----|-----|---------------|\n",
    "| $+$ | $+$ | $0.7$         |\n",
    "| $+$ | $-$ | $0.3$         |\n",
    "| $-$ | $+$ | $0.8$         |\n",
    "| $-$ | $-$ | $0.2$         |\n",
    "\n",
    "Conditional for $A$ given $D$ and $X$:\n",
    "\n",
    "| $D$ | $X$ | $A$ | $P(A \\mid D,X)$ |\n",
    "|-----|-----|-----|-----------------|\n",
    "| $+$ | $+$ | $+$ | $0.9$           |\n",
    "| $+$ | $+$ | $-$ | $0.1$           |\n",
    "| $+$ | $-$ | $+$ | $0.8$           |\n",
    "| $+$ | $-$ | $-$ | $0.2$           |\n",
    "| $-$ | $+$ | $+$ | $0.6$           |\n",
    "| $-$ | $+$ | $-$ | $0.4$           |\n",
    "| $-$ | $-$ | $+$ | $0.1$           |\n",
    "| $-$ | $-$ | $-$ | $0.9$           |\n",
    "\n",
    "---\n",
    "\n",
    "**(b)** Compute the following probabilities:\n",
    "\n",
    "1. $P(+d \\mid +b)$  \n",
    "2. $P(+d, +a)$  \n",
    "3. $P(+d \\mid +a)$\n",
    "\n",
    "Here “$+d$” means $D = +$, “$+b$” means $B = +$, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**(c)** Which of the following conditional independencies are guaranteed by the above network?\n",
    "\n",
    "- $X \\perp\\!\\!\\!\\perp B \\mid D$ (i.e. $X$ and $B$ are conditionally independent given $D$)  \n",
    "- $D \\perp\\!\\!\\!\\perp A \\mid X$  \n",
    "- $D \\perp\\!\\!\\!\\perp A \\mid B$  \n",
    "- $D \\perp\\!\\!\\!\\perp X \\mid A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cadced",
   "metadata": {},
   "source": [
    "## Problem 1.7 — Simpson’s reversal with a fatal syndrome\n",
    "\n",
    "Assume that a population of patients contains a fraction $r$ of individuals who suffer from a certain fatal syndrome\n",
    "$Z$, which simultaneously makes it uncomfortable for them to take a life-prolonging drug $X$ (see Figure 1 in the\n",
    "exercise sheet).\n",
    "\n",
    "Let\n",
    "\n",
    "- $Z = z_1$ and $Z = z_0$ represent, respectively, the presence and absence of the syndrome,\n",
    "- $Y = y_1$ and $Y = y_0$ represent death and survival, respectively,\n",
    "- $X = x_1$ and $X = x_0$ represent taking and not taking the drug.\n",
    "\n",
    "Assume that patients *not* carrying the syndrome ($Z = z_0$) die with probability $p_2$ if they take the drug and with\n",
    "probability $p_1$ if they do not. Patients carrying the syndrome ($Z = z_1$), on the other hand, die with probability\n",
    "$p_3$ if they do not take the drug and with probability $p_4$ if they do take the drug.\n",
    "\n",
    "Further, patients having the syndrome are more likely to avoid the drug, with probabilities\n",
    "$$\n",
    "q_1 = P(x_1 \\mid z_0),\n",
    "\\qquad\n",
    "q_2 = P(x_1 \\mid z_1).\n",
    "$$\n",
    "\n",
    "**(a)** Based on this model, compute the joint distributions $P(x,y,z)$, $P(x,y)$, $P(x,z)$, and $P(y,z)$ for all\n",
    "values of $x,y,z$, in terms of the parameters $(r, p_1, p_2, p_3, p_4, q_1, q_2)$.\n",
    "\n",
    "*Hint.* Decompose the product using the graph structure.\n",
    "\n",
    "---\n",
    "\n",
    "**(b)** Calculate the difference\n",
    "$$\n",
    "P(y_1 \\mid x_1) - P(y_1 \\mid x_0)\n",
    "$$\n",
    "for three populations:\n",
    "\n",
    "1. those carrying the syndrome,  \n",
    "2. those not carrying the syndrome,  \n",
    "3. the population as a whole.\n",
    "\n",
    "---\n",
    "\n",
    "**(c)** Using your results from part (b), find a combination of parameters that exhibits **Simpson’s reversal** (i.e.\n",
    "the drug appears beneficial in each subgroup but harmful, or vice versa, in the population as a whole)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
