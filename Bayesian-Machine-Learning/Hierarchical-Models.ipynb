{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ede1ac1",
   "metadata": {},
   "source": "# Hierarchical Models"
  },
  {
   "cell_type": "markdown",
   "id": "f2608052",
   "metadata": {},
   "source": [
    "## Probabilistic graphical model notation\n",
    "\n",
    "Probabilistic graphical models (PGMs) are graphs that encode **conditional dependence** structure between variables.\n",
    "\n",
    "Basic notation from the slides:\n",
    "\n",
    "- Observed variable: usually drawn as a **shaded** node.\n",
    "- Latent (unobserved) variable: drawn as an **unshaded** node.\n",
    "- Deterministic variable: drawn as a node with a different border (function of parents).\n",
    "- Repeated structure (“**plate**”): a box with an index indicating repetition, e.g. over observations or groups.\n",
    "\n",
    "Rule of thumb:\n",
    "\n",
    "> Every unobserved variable that has no incoming arrows needs a **prior**.\n",
    "\n",
    "Bayesian inference then provides **posterior distributions** for all unobserved variables.\n",
    "\n",
    "Canonical examples:\n",
    "\n",
    "- **Beta–binomial model**:\n",
    "\n",
    "  - Prior:\n",
    "    $$\n",
    "    \\pi \\sim \\operatorname{Beta}(\\alpha,\\beta)\n",
    "    $$\n",
    "  - Likelihood:\n",
    "    $$\n",
    "    y \\mid \\pi \\sim \\operatorname{Bin}(n,\\pi)\n",
    "    $$\n",
    "\n",
    "- **Gamma–Poisson model**:\n",
    "\n",
    "  - Prior:\n",
    "    $$\n",
    "    \\lambda \\sim \\operatorname{Gamma}(s,r)\n",
    "    $$\n",
    "  - Likelihood:\n",
    "    $$\n",
    "    y \\mid \\lambda \\sim \\operatorname{Pois}(\\lambda)\n",
    "    $$\n",
    "\n",
    "- **Normal likelihood with unknown mean and variance**:\n",
    "\n",
    "  - Priors:\n",
    "    $$\n",
    "    \\mu \\sim \\text{some prior}, \\qquad \\sigma \\sim \\text{some positive prior},\n",
    "    $$\n",
    "  - Likelihood (for data points $y_i$):\n",
    "    $$\n",
    "    y_i \\mid \\mu,\\sigma \\sim \\mathcal{N}(\\mu,\\sigma^2).\n",
    "    $$\n",
    "\n",
    "- **Simple linear regression**:\n",
    "\n",
    "  - Parameters: intercept $\\beta_0$, slope $\\beta_1$, noise $\\sigma$.\n",
    "  - Mean function:\n",
    "    $$\n",
    "    \\mu_i = \\beta_0 + \\beta_1 x_i.\n",
    "    $$\n",
    "  - Likelihood:\n",
    "    $$\n",
    "    y_i \\mid \\beta_0,\\beta_1,\\sigma,x_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2).\n",
    "    $$\n",
    "\n",
    "In all cases, the PGM makes explicit **which variables depend on which**, and where priors must be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c8831",
   "metadata": {},
   "source": [
    "## Grouped data and hierarchical structure\n",
    "\n",
    "Many real datasets have a **grouped** or **multilevel** structure.\n",
    "\n",
    "Examples from the slides:\n",
    "\n",
    "- Cancer rates:\n",
    "  - Counties nested within **states**, states nested within the **USA**.\n",
    "- Medical data:\n",
    "  - Repeated measurements nested within **patients**, patients nested within a **population**.\n",
    "\n",
    "Such data naturally form **hierarchies**:\n",
    "\n",
    "- Level 1: individual observations (e.g. county, weekly measurement),\n",
    "- Level 2: groups (e.g. state, patient),\n",
    "- Level 3: higher-level population (e.g. country, disease level).\n",
    "\n",
    "We would like models that:\n",
    "\n",
    "- Respect the fact that **observations within the same group are related**,\n",
    "- Allow **information sharing** across groups,\n",
    "- Give reasonable predictions for **groups with few data** and even for **new groups**.\n",
    "\n",
    "This motivates **hierarchical models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbca1b9",
   "metadata": {},
   "source": [
    "## Modelling strategies for grouped data\n",
    "\n",
    "For grouped data (e.g. cancer rates per county, grouped by state) the slides discuss three approaches:\n",
    "\n",
    "- **Complete pooling**\n",
    "- **No pooling**\n",
    "- **Partial pooling (hierarchical modelling)**\n",
    "\n",
    "Each approach corresponds to a different assumption about how **group means** are related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a67b0",
   "metadata": {},
   "source": [
    "### Complete pooling\n",
    "\n",
    "Complete pooling **ignores group structure** and models all observations as if they came from the same distribution.\n",
    "\n",
    "Example: cancer rates $y_i$ for all counties in the US (ignoring states):\n",
    "\n",
    "$$\n",
    "y_i \\mid \\mu,\\sigma_y \\sim \\mathcal{N}(\\mu,\\sigma_y^2),\n",
    "\\quad i = 1,\\dots,n.\n",
    "$$\n",
    "\n",
    "We put priors on $\\mu$ and $\\sigma_y$ and infer their posterior distributions.\n",
    "\n",
    "Properties:\n",
    "\n",
    "- Very simple model (**few parameters**, here just $\\mu$ and $\\sigma_y$).\n",
    "- Can estimate the **overall mean** (e.g. average cancer rate in the US).\n",
    "- Cannot say anything about **differences between groups** (states) because it ignores them.\n",
    "- Predictions for individual states are essentially the **same** (up to noise).\n",
    "\n",
    "This is sometimes called a **complete pooling** model because it pools all groups into one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08087124",
   "metadata": {},
   "source": [
    "### No pooling\n",
    "\n",
    "No pooling fits an **independent model per group**, treating each group as if it had nothing to do with other groups.\n",
    "\n",
    "Example: cancer rates for counties in state $j$:\n",
    "\n",
    "$$\n",
    "y_{ij} \\mid \\mu_j,\\sigma_j \\sim \\mathcal{N}(\\mu_j,\\sigma_j^2),\n",
    "\\quad i = 1,\\dots,n_j,\n",
    "$$\n",
    "\n",
    "with priors\n",
    "\n",
    "$$\n",
    "\\mu_j \\sim \\text{some prior}, \\qquad \\sigma_j \\sim \\text{some prior},\n",
    "\\quad j = 1,\\dots,J.\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "\n",
    "- Very flexible (separate parameters for each group).\n",
    "- For $J$ states and two parameters per state, we have roughly $2J$ parameters (e.g. $46 \\times 2$ in the slides).\n",
    "- Allows state-specific inference but\n",
    "\n",
    "  - can **overfit** groups with few observations,\n",
    "  - gives **no** clear estimate of the overall mean (country-level),\n",
    "  - cannot predict for **new groups** (states with no data), since there is no shared structure.\n",
    "\n",
    "No pooling ignores that groups are part of a **larger population**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c967a4",
   "metadata": {},
   "source": [
    "### Partial pooling and hierarchical models\n",
    "\n",
    "Partial pooling sits between complete pooling and no pooling.\n",
    "\n",
    "Key idea:\n",
    "\n",
    "- Each group has its own parameter (e.g. state mean $\\mu_j$),\n",
    "- These parameters are themselves assumed to come from a **population distribution** with its own hyperparameters.\n",
    "\n",
    "Cancer example: a **hierarchical normal model** for state means.\n",
    "\n",
    "Model:\n",
    "\n",
    "- County-level data (observations within states):\n",
    "\n",
    "  $$\n",
    "  y_{ij} \\mid \\mu_j,\\sigma_y\n",
    "  \\sim \\mathcal{N}(\\mu_j,\\sigma_y^2),\n",
    "  \\quad i = 1,\\dots,n_j,\\; j = 1,\\dots,J.\n",
    "  $$\n",
    "\n",
    "- State-level means:\n",
    "\n",
    "  $$\n",
    "  \\mu_j \\mid \\mu,\\sigma_\\mu\n",
    "  \\sim \\mathcal{N}(\\mu,\\sigma_\\mu^2),\n",
    "  \\quad j = 1,\\dots,J.\n",
    "  $$\n",
    "\n",
    "- Hyperpriors (country-level):\n",
    "\n",
    "  $$\n",
    "  \\mu \\sim \\text{some prior}, \\qquad \\sigma_\\mu \\sim \\text{some prior}, \\qquad \\sigma_y \\sim \\text{some prior}.\n",
    "  $$\n",
    "\n",
    "This is a **hierarchical model** (also called **multilevel** model).\n",
    "\n",
    "Information flow:\n",
    "\n",
    "- County-level data inform their state-specific means $\\mu_j$.\n",
    "- All state means jointly inform the hyperparameters $(\\mu,\\sigma_\\mu)$.\n",
    "- Hyperparameters \"feed back\" to group means, especially for groups with few data.\n",
    "\n",
    "This leads to the phenomenon of **shrinkage**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f73009",
   "metadata": {},
   "source": [
    "## Shrinkage in hierarchical models\n",
    "\n",
    "In a hierarchical normal model, posterior estimates of group means $\\mu_j$ are **shrunk** toward the overall mean $\\mu$.\n",
    "\n",
    "For a simple case with known $\\sigma_y$ and $\\sigma_\\mu$, and $n_j$ observations in group $j$, the posterior mean of\n",
    "$\\mu_j$ has the form\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_j^{\\text{post}}\n",
    "= w_j \\,\\bar{y}_j + (1 - w_j)\\,\\mu,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\bar{y}_j$ is the sample mean of group $j$,\n",
    "- $\\mu$ is the global mean (hyperparameter),\n",
    "- $w_j \\in (0,1)$ is a **weight** given by\n",
    "\n",
    "  $$\n",
    "  w_j\n",
    "  = \\frac{n_j / \\sigma_y^2}{n_j / \\sigma_y^2 + 1 / \\sigma_\\mu^2}\n",
    "  = \\frac{n_j}{n_j + \\sigma_y^2 / \\sigma_\\mu^2}.\n",
    "  $$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- If $n_j$ is **large** (many observations) or $\\sigma_y^2$ is small, then $w_j \\approx 1$ and\n",
    "  $\\hat{\\mu}_j^{\\text{post}} \\approx \\bar{y}_j$.\n",
    "\n",
    "  The group mean relies mostly on its own data.\n",
    "\n",
    "- If $n_j$ is **small** or $\\sigma_\\mu^2$ is small (strong hyperprior), then $w_j$ is smaller and\n",
    "  $\\hat{\\mu}_j^{\\text{post}}$ is closer to the global mean $\\mu$.\n",
    "\n",
    "  The group mean is strongly **shrunk** toward the global mean.\n",
    "\n",
    "Shrinkage is stronger when:\n",
    "\n",
    "- There are **few** observations in a group,\n",
    "- The group mean is **far** from the global mean,\n",
    "- The hyperprior variance $\\sigma_\\mu^2$ is **small** (more belief in a tight global distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a35b8b",
   "metadata": {},
   "source": [
    "## Between-group vs within-group variability\n",
    "\n",
    "In hierarchical models, it is useful to distinguish:\n",
    "\n",
    "- **Within-group variability**: how much observations vary around the group mean,\n",
    "- **Between-group variability**: how much group means vary around the global mean.\n",
    "\n",
    "In the hierarchical normal model:\n",
    "\n",
    "- Within-group variance (county-level noise):\n",
    "  $$\n",
    "  \\operatorname{Var}(Y_{ij} \\mid \\mu_j) = \\sigma_y^2.\n",
    "  $$\n",
    "\n",
    "- Between-group variance (variability of state means):\n",
    "  $$\n",
    "  \\operatorname{Var}(\\mu_j) = \\sigma_\\mu^2.\n",
    "  $$\n",
    "\n",
    "Using the **law of total variance**, total variance of $Y_{ij}$ (marginally over groups) can be decomposed as\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(Y_{ij})\n",
    "= \\mathbb{E}[\\operatorname{Var}(Y_{ij} \\mid \\mu_j)]\n",
    "  + \\operatorname{Var}(\\mathbb{E}[Y_{ij} \\mid \\mu_j])\n",
    "= \\sigma_y^2 + \\sigma_\\mu^2.\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- $\\sigma_y^2$ is the typical variability **within** states (across counties).\n",
    "- $\\sigma_\\mu^2$ is the variability **between** state means.\n",
    "- The relative sizes of these variances indicate how much of the total variability is due to differences **between**\n",
    "  states versus differences **within** states.\n",
    "\n",
    "The slides illustrate this decomposition visually using histograms and density plots from *Bayes Rules!*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cfcaad",
   "metadata": {},
   "source": [
    "## Predictions for new groups in hierarchical models\n",
    "\n",
    "One advantage of hierarchical models is the ability to make predictions for **new groups** (e.g. a state with no data).\n",
    "\n",
    "For a **new state** $j^\\ast$ with no observed data:\n",
    "\n",
    "- The prior for its mean is the **population distribution**:\n",
    "  $$\n",
    "  \\mu_{j^\\ast} \\mid \\mu,\\sigma_\\mu \\sim \\mathcal{N}(\\mu,\\sigma_\\mu^2).\n",
    "  $$\n",
    "\n",
    "- For a **new county** in this new state, the predictive distribution is\n",
    "  $$\n",
    "  Y_{\\text{new}} \\mid \\mu,\\sigma_\\mu,\\sigma_y\n",
    "  \\sim \\mathcal{N}(\\mu,\\; \\sigma_\\mu^2 + \\sigma_y^2).\n",
    "  $$\n",
    "\n",
    "The variance is **larger** because we are uncertain both about:\n",
    "\n",
    "- The state-level mean $\\mu_{j^\\ast}$ (epistemic uncertainty at group level),\n",
    "- The county-level noise $\\sigma_y^2$ (aleatoric uncertainty within groups).\n",
    "\n",
    "This explains why predictions for states like “Kansas” (not in the dataset) have visibly **wider uncertainty bands** in\n",
    "the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52082fa7",
   "metadata": {},
   "source": [
    "## Hierarchical linear regression\n",
    "\n",
    "The same hierarchical ideas extend naturally to **regression**.\n",
    "\n",
    "Motivating example from the slides: **pulmonary fibrosis** progression.\n",
    "\n",
    "- Repeated lung volume measurements $y_{ij}$ for patient $j$ at time $x_{ij}$.\n",
    "- We expect approximately linear decline per patient, but\n",
    "  - each patient has their own baseline lung volume,\n",
    "  - each patient has their own progression rate (slope).\n",
    "\n",
    "We therefore build a **random intercept and slope** model:\n",
    "\n",
    "- Observation model:\n",
    "  $$\n",
    "  y_{ij} \\mid \\beta_{0j},\\beta_{1j},\\sigma_y,x_{ij}\n",
    "  \\sim \\mathcal{N}\\big(\\beta_{0j} + \\beta_{1j} x_{ij},\\; \\sigma_y^2\\big),\n",
    "  $$\n",
    "  for $i = 1,\\dots,n_j$, $j = 1,\\dots,J$.\n",
    "\n",
    "- Patient-level parameters:\n",
    "  $$\n",
    "  \\beta_{0j} \\mid \\beta_0,\\sigma_0\n",
    "  \\sim \\mathcal{N}(\\beta_0,\\sigma_0^2),\n",
    "  \\qquad\n",
    "  \\beta_{1j} \\mid \\beta_1,\\sigma_1\n",
    "  \\sim \\mathcal{N}(\\beta_1,\\sigma_1^2).\n",
    "  $$\n",
    "\n",
    "- Hyperpriors:\n",
    "  $$\n",
    "  \\beta_0,\\beta_1,\\sigma_0,\\sigma_1,\\sigma_y\n",
    "  \\sim \\text{priors on appropriate supports}.\n",
    "  $$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\beta_0$ and $\\beta_1$ describe the **global disease level**:\n",
    "  - typical baseline lung volume,\n",
    "  - typical decline rate.\n",
    "- $\\beta_{0j}$ and $\\beta_{1j}$ describe **patient-level deviations** around these global averages.\n",
    "\n",
    "This is a **hierarchical linear regression** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9d9a5",
   "metadata": {},
   "source": [
    "### Shrinkage in random intercept and slope models\n",
    "\n",
    "As in simpler hierarchical models, the random intercept and slope model exhibits **shrinkage**:\n",
    "\n",
    "- Intercepts $\\beta_{0j}$ are shrunk toward the global intercept $\\beta_0$.\n",
    "- Slopes $\\beta_{1j}$ are shrunk toward the global slope $\\beta_1$.\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "- Patients with many measurements and clear trends have patient-specific estimates dominated by their own data.\n",
    "- Patients with few measurements or noisy data have intercepts and slopes that are pulled more strongly toward\n",
    "  the global means.\n",
    "\n",
    "This gives:\n",
    "\n",
    "- **More bias** for poorly observed patients (we borrow strength from the population),\n",
    "- But **less variance** across patient-specific estimates compared to fitting separate regression lines per patient.\n",
    "\n",
    "The slides illustrate this with a subsample of patients: the more uncertain the individual slope, the more it is shrunk\n",
    "toward the global mean slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0238e",
   "metadata": {},
   "source": [
    "### Hierarchical regression with Bambi\n",
    "\n",
    "Bambi provides a convenient interface for fitting random intercept and slope models.\n",
    "\n",
    "Conceptually, a Bambi model like\n",
    "\n",
    "```python\n",
    "bmb.Model(\"FVC ~ weeks + (weeks | patient_id)\", data=data)\n",
    "```\n",
    "\n",
    "implements the hierarchical structure:\n",
    "\n",
    "- Fixed (global) effects: **overall intercept and slope** (disease level),\n",
    "- Random (group-specific) effects: **patient-specific intercepts and slopes**.\n",
    "\n",
    "Under the hood, Bambi builds a PyMC model with priors on:\n",
    "\n",
    "- Global coefficients,\n",
    "- Group-level standard deviations (for intercepts and slopes),\n",
    "- Residual standard deviation $\\sigma_y$ of the observations.\n",
    "\n",
    "Fitting such a model can be **numerically challenging**:\n",
    "\n",
    "- More parameters and more complex posterior geometries,\n",
    "- Potential issues like divergences or low effective sample size,\n",
    "- Often need more tuning samples or higher target acceptance rates.\n",
    "\n",
    "This is why the slides emphasize careful diagnostics when using such models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4ac4a",
   "metadata": {},
   "source": [
    "## Adding group-level predictors\n",
    "\n",
    "Hierarchical models can include **group-level predictors** to explain variation in intercepts and slopes.\n",
    "\n",
    "Example (pulmonary fibrosis):\n",
    "\n",
    "- Patient-level covariates: age, sex, smoking status.\n",
    "- These can be used to explain differences in initial lung volume (intercept) and progression rate (slope).\n",
    "\n",
    "One way to write this is:\n",
    "\n",
    "- Intercept model:\n",
    "  $$\n",
    "  \\beta_{0j}\n",
    "  = \\gamma_{00} + \\gamma_{01} \\,\\text{age}_j + \\gamma_{02} \\,\\text{male}_j\n",
    "    + \\gamma_{03} \\,\\text{smoker}_j + u_{0j},\n",
    "  $$\n",
    "- Slope model:\n",
    "  $$\n",
    "  \\beta_{1j}\n",
    "  = \\gamma_{10} + \\gamma_{11} \\,\\text{age}_j + \\gamma_{12} \\,\\text{male}_j\n",
    "    + \\gamma_{13} \\,\\text{smoker}_j + u_{1j},\n",
    "  $$\n",
    "\n",
    "with random effects $u_{0j}$ and $u_{1j}$ having their own prior distributions.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- $\\gamma$’s describe how group-level covariates (e.g. age, sex, smoking) affect **baseline** and **trend**.\n",
    "- The $u$’s capture remaining unexplained patient-level variation.\n",
    "\n",
    "The slides show that some predictors (e.g. sex, age) may affect intercepts strongly, but not slopes, and warn that\n",
    "these relationships can be distorted by **collider effects**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dab16",
   "metadata": {},
   "source": [
    "## Collider effect and caution in causal interpretation\n",
    "\n",
    "The slides end with an important caveat: hierarchical models fitted to observational data can show **spurious\n",
    "associations** due to the **collider effect**.\n",
    "\n",
    "Example causal diagram:\n",
    "\n",
    "$$\n",
    "\\text{Smoking} \\longrightarrow \\text{Pulmonary Fibrosis} \\longleftarrow \\text{Genetic predisposition}.\n",
    "$$\n",
    "\n",
    "Here, pulmonary fibrosis is a **collider**: it has two incoming arrows.\n",
    "\n",
    "If we **condition** on having pulmonary fibrosis (i.e. restrict the dataset to patients with the disease), then:\n",
    "\n",
    "- People who do **not** smoke but still have pulmonary fibrosis are more likely to have a strong genetic predisposition.\n",
    "- Among patients with the disease, “not smoking” may appear to be associated with “worse genetics”.\n",
    "\n",
    "This can create the illusion that smoking is **protective**, even though it is not.\n",
    "\n",
    "Key message:\n",
    "\n",
    "- Hierarchical models help share information and manage uncertainty, but\n",
    "- **Causal interpretation requires explicit causal reasoning** and careful attention to colliders, confounders, and\n",
    "  selection bias.\n",
    "\n",
    "This motivates the topic of **causal modelling / Bayesian networks**, which is introduced in the following weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724dc9c9",
   "metadata": {},
   "source": [
    "## Back to the Week 1 motivation\n",
    "\n",
    "The final slides briefly revisit the motivation for Bayesian methods:\n",
    "\n",
    "Situations where the **Bayesian view** is particularly useful:\n",
    "\n",
    "- Quantifying **uncertainty** is central to the problem.\n",
    "- Only **limited data** are available.\n",
    "- **Prior knowledge** needs to be formally incorporated.\n",
    "- The model has a **graphical / network structure** (as in hierarchical models).\n",
    "\n",
    "Situations where the **frequentist view** may be perfectly adequate:\n",
    "\n",
    "- Abundant data and simple models.\n",
    "- Prior information is weak, controversial, or not crucial.\n",
    "- Computational simplicity and speed outweigh the benefits of full posterior inference.\n",
    "\n",
    "In the words (quoted in the slides) of Richard McElreath:\n",
    "\n",
    "> You don't have to use a chainsaw to cut the birthday cake.\n",
    "\n",
    "Bayesian hierarchical models are powerful tools—but they should be used where their additional complexity and richness\n",
    "actually help answer the scientific questions at hand."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
