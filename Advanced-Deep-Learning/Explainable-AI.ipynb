{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f069ac",
   "metadata": {},
   "source": "# Explainable AI"
  },
  {
   "cell_type": "markdown",
   "id": "30598122",
   "metadata": {},
   "source": [
    "## Why explainability matters\n",
    "\n",
    "- Modern deep models often function as **black boxes**, prioritizing predictive performance over transparency.\n",
    "- Transparency is crucial where models **impact society** (health, credit, justice, safety).\n",
    "- A typical ML pipeline maps inputs (image/text/tabular) through a function $f_\\theta(\\cdot)$ to an output $\\hat{y}$; errors backpropagate to optimize $\\theta$. Understanding **what** features and **why** they influence $\\hat{y}$ motivates XAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34959a",
   "metadata": {},
   "source": [
    "## Two core challenges in Explainable AI (XAI)\n",
    "\n",
    "- **Missingness problem.** How should we represent a feature being “absent”? Choice of baseline or masking strongly affects explanations.\n",
    "- **Correlation problem.** Real features are **dependent**. Methods that vary one feature while holding others fixed may produce unrealistic inputs and misleading attributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de180e67",
   "metadata": {},
   "source": [
    "## Nomenclature and taxonomy\n",
    "\n",
    "- **Intrinsic vs post hoc.** Intrinsic (glass box) models are interpretable by design; post hoc methods explain already-trained black-box models.\n",
    "- **Local vs global.** Local methods explain a specific prediction; global methods summarize model behavior across the dataset.\n",
    "- **Surrogates.** A simple, interpretable model $g$ approximates a complex $f$ (locally or globally) to obtain human-readable explanations.\n",
    "- **Counterfactuals.** “What minimal change to $x$ would flip the decision?” Provides actionable insight for an instance.\n",
    "- **Data modalities.** Images, text, and tabular data call for different masking/baseline strategies and surrogates.\n",
    "- **Attributions** A heatmap that shows which pixes in an image contributed the most to a models decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72a8d3",
   "metadata": {},
   "source": [
    "## Glass-box models\n",
    "\n",
    "### Linear models\n",
    "\n",
    "**Linear regression.** With standardized features, coefficients indicate relative importance.\n",
    "- Prediction: $\\hat{y}=\\beta_0+\\sum_{j=1}^p \\beta_j x_j$.\n",
    "- Fit by least squares (MSE):\n",
    "$$\n",
    "\\min_{\\beta_0,\\ldots,\\beta_p}\\ \\sum_{i=1}^n \\Bigl(y^{(i)}-\\beta_0-\\sum_{j=1}^p\\beta_j x^{(i)}_j\\Bigr)^2.\n",
    "$$\n",
    "\n",
    "**Logistic regression.** Probabilistic classification with sigmoid/softmax link.\n",
    "- Binary: $P(y=1\\mid x)=\\sigma(z)$, where $z=\\beta_0+\\sum_j \\beta_j x_j$, $\\ \\sigma(z)=\\tfrac{1}{1+e^{-z}}$.\n",
    "- Fit by (negative) log-likelihood:\n",
    "$$\n",
    "\\min_{\\beta}\\ -\\sum_{i=1}^n \\Bigl[y^{(i)}\\log\\sigma(z^{(i)})+(1-y^{(i)})\\log(1-\\sigma(z^{(i)}))\\Bigr].\n",
    "$$\n",
    "\n",
    "**Interpretation caveats.**\n",
    "- Standardization helps compare $|\\beta_j|$ across features.\n",
    "- Signs: $\\beta_j\\!\\!>\\!0$ means increasing $x_j$ increases $\\hat{y}$ (or the log-odds).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6081c",
   "metadata": {},
   "source": [
    "### Decision trees (and forests)\n",
    "\n",
    "**Decision trees.** Interpretable by **rules/paths** from root to leaf. For classification, each leaf stores a class probability; the prediction comes from the leaf reached by the instance.\n",
    "\n",
    "**Purity via entropy** (classification):\n",
    "$$\n",
    "H(D)= -\\sum_{c=1}^k p(C_c)\\log_2 p(C_c).\n",
    "$$\n",
    "\n",
    "**Information gain** for attribute $A$:\n",
    "$$\n",
    "\\mathrm{IG}(D,A)= H(D) - \\sum_{t\\in\\text{splits}} \\frac{|D_t|}{|D|} H(D_t).\n",
    "$$\n",
    "\n",
    "**Growing trees.** Greedy splits that maximize information gain until stopping criteria (max depth, min leaf size, purity). Watch for **overfitting**; forests (bagging, random feature subsets) improve generalization and support tasks like outlier detection (e.g., Isolation Forest).\n",
    "\n",
    "**Explainability.**\n",
    "- Global: feature importances, prototypical rules.\n",
    "- Local: path-level rules for a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec46839",
   "metadata": {},
   "source": [
    "## Perturbation-based explanations: occlusion\n",
    "\n",
    "**Idea.** Measure output change when parts of the input are masked.\n",
    "- Sliding window occlusion trades off **window size** and **stride**; results can vary notably.\n",
    "- **Adaptive occlusion** seeks the smallest region that preserves the prediction:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{m}) = \\bigl|\\hat{y}(\\mathbf{x})-\\hat{y}(\\mathbf{x}\\odot\\mathbf{m})\\bigr| + \\alpha \\sum_{i}|m_i|,\n",
    "$$\n",
    "with mask $\\mathbf{m}\\in[0,1]^d$ and elementwise product $\\odot$. Smaller masks with similar scores are preferred.\n",
    "\n",
    "**Randomized occlusion.** Sample many random masks, score, normalize, and aggregate into a heatmap.\n",
    "\n",
    "**Caveats.**\n",
    "- Can resemble segmentation rather than *decision* evidence.\n",
    "- Computationally expensive (many forward passes).\n",
    "- Sensitive to the choice of baseline (missingness). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75785c",
   "metadata": {},
   "source": [
    "## Class Activation Mapping (CAM and Grad-CAM)\n",
    "\n",
    "**CAM (for specific CNN architectures).**\n",
    "- Requires a **global average pooling** just before a linear classifier (softmax).\n",
    "- For class $c$, the map is the **weighted sum** of final conv feature maps:\n",
    "$$\n",
    "M_c=\\mathrm{ReLU}\\!\\left(\\sum_{k} w^{c}_{k}\\,A^{k}\\right),\n",
    "$$\n",
    "where $A^k$ are feature maps and $w_k^c$ are classifier weights. ReLU highlights positively contributing regions.\n",
    "\n",
    "**Grad-CAM (architecture-agnostic for CNNs).**\n",
    "- Uses gradients to obtain weights $\\alpha_k^c$ for the last conv layer’s feature maps:\n",
    "$$\n",
    "M_c=\\mathrm{ReLU}\\!\\left(\\sum_{k} \\alpha^{c}_{k}\\,A^{k}\\right),\\qquad\n",
    "\\alpha^{c}_{k}\\propto \\text{global average of }\\frac{\\partial \\hat{y}_c}{\\partial A^{k}}.\n",
    "$$\n",
    "- Frees architectural constraints, but resolution depends on the final conv layer; heatmap intensities are not comparable across images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd46f2",
   "metadata": {},
   "source": [
    "## Surrogate explanations: LIME (local, model-agnostic)\n",
    "\n",
    "**Goal.** Approximate a black-box $f$ near an instance $x$ by a simple surrogate model $g$ in an interpretable feature space $z'$ (e.g., superpixels, word indicators).\n",
    "- Sample perturbations around $x$ in $z'$-space, map back to original space $z$, and evaluate $f(z)$.\n",
    "- Fit $g$ with locality weights $\\pi_x(z)$:\n",
    "$$\n",
    "\\min_{g\\in\\mathcal{G}}\\ \\sum_{i}\\ \\pi_x(z_i)\\,\\bigl(f(z_i)-g(z'_i)\\bigr)^2\\ +\\ \\Omega(g),\n",
    "$$\n",
    "where $\\Omega(g)$ penalizes complexity (sparsity).\n",
    "\n",
    "**Design choices.**\n",
    "- Interpretable binary representation $z'$ (e.g., superpixel on/off).\n",
    "- Locality kernel $\\pi_x$ (e.g., exponential kernel in interpretable space).\n",
    "- Fidelity vs simplicity trade-off via $\\Omega(g)$.\n",
    "\n",
    "**Limitations.**\n",
    "- Faithfulness depends on sampling scheme, kernel, and representation of **missingness**.\n",
    "- Correlated features and unrealistic perturbations can mislead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46ea26",
   "metadata": {},
   "source": [
    "## Plot-based global summaries: PDP and ICE\n",
    "\n",
    "**Partial dependence (PDP).** For feature $i$,\n",
    "$$\n",
    "\\mathrm{PD}_i(x_i) = \\mathbb{E}_{x_{-i}}\\bigl[f(x_i,x_{-i})\\bigr].\n",
    "$$\n",
    "Averages out other features to show the **marginal** effect of $x_i$.\n",
    "\n",
    "**Individual conditional expectation (ICE).**\n",
    "- For a fixed instance $x^{(j)}$, trace $f(x_i, x_{-i}^{(j)})$ as $x_i$ varies.\n",
    "- Reveals **heterogeneity** that PDP might hide.\n",
    "\n",
    "**Caveats.**\n",
    "- If features are correlated, varying $x_i$ alone may create unrealistic inputs (correlation problem).\n",
    "- Aggregation across the dataset can hide important local behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a077ced",
   "metadata": {},
   "source": [
    "## Good practice and common pitfalls\n",
    "\n",
    "- Always consider the **missingness** and **correlation** problems when designing explanations.\n",
    "- Prefer **data-aware** perturbations and realistic baselines/masks.\n",
    "- Validate explanations qualitatively and, where possible, **quantitatively** (e.g., deletion/insertion tests, sanity checks).\n",
    "- Combine complementary views: intrinsic models, local surrogates, perturbation maps, and global plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b9570",
   "metadata": {},
   "source": [
    "# Explainable AI II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1fce7",
   "metadata": {},
   "source": [
    "## Gradient-based attribution: vanilla gradients and Gradient × Input\n",
    "\n",
    "**Setup.** Let a trained model produce a scalar score $f(x)$ for input $x \\in \\mathbb{R}^d$ (e.g., the logit for a chosen class).\n",
    "\n",
    "- **Vanilla gradients (saliency):** use the input gradient $\\nabla_x f(x)$ as an importance signal for each feature $i$:\n",
    "  $$S_i(x) \\;=\\; \\frac{\\partial f(x)}{\\partial x_i}.$$\n",
    "  This measures local sensitivity of $f$ to infinitesimal changes in $x_i$.\n",
    "\n",
    "- **Gradient × Input:** scale sensitivity by the feature value to indicate direction and magnitude:\n",
    "  $$M_i(x) \\;=\\; x_i \\cdot \\frac{\\partial f(x)}{\\partial x_i}.$$\n",
    "  This heuristic often sharpens attribution by weighting gradients with input intensity.\n",
    "\n",
    "**Known issues.** Raw input-gradients can be *noisy* and reflect high-frequency, local properties of piecewise-linear networks. They may also suffer from **saturation** (e.g., ReLU plateaus or sigmoid saturation), where important features can have near-zero gradients even though the prediction relies on them. This motivates path-based methods that probe sensitivity away from the saturated neighborhood around $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5218156",
   "metadata": {},
   "source": [
    "## Integrated Gradients (IG)\n",
    "\n",
    "**Goal.** Attribute the prediction at $x$ relative to a *baseline* (reference) $x'$ by aggregating gradients **along a path** from $x'$ to $x$. The standard straight-line path gives attributions\n",
    "$$\n",
    "\\phi_i^{IG}\\!\\left(f, x, x'\\right)\n",
    "\\;=\\;\n",
    "(x_i - x_i') \\int_{\\alpha = 0}^{1}\n",
    "\\frac{\\partial f\\!\\left(x' + \\alpha (x - x')\\right)}{\\partial x_i}\\, d\\alpha.\n",
    "$$\n",
    "\n",
    "**Intuition.** Starting from $x'$ (representing feature *missingness* or neutral input), gradually morph into $x$; accumulate each feature’s gradient contribution along the way. This alleviates local saturation around $x$ and captures how changes along the entire path affect $f$.\n",
    "\n",
    "**Discrete approximation.** Using $m$ steps with points $x^{(k)} = x' + \\tfrac{k}{m}(x - x')$:\n",
    "$$\n",
    "\\phi_i^{IG} \\approx (x_i - x_i') \\cdot \\frac{1}{m}\\sum_{k=1}^{m}\n",
    "\\frac{\\partial f\\!\\left(x^{(k)}\\right)}{\\partial x_i}.\n",
    "$$\n",
    "\n",
    "**Baseline choice matters.** Common baselines include a *black/white* image, *blurred* input, *uniform* random, or *Gaussian* noise references. Each choice encodes a notion of “feature missingness” and can bias the attribution. In practice, one should justify the baseline and, when possible, use multiple references or data-driven baselines.\n",
    "\n",
    ":::{prf:axiom} Completeness\n",
    "Completeness is a desirable property because it states that the importance scores for each feature break down the output of the network: each importance score represents that feature’s individual contribution to the network output, and added when together, we recover the output value itself.\n",
    "\n",
    "$$\n",
    "\\sum_{i} \\phi_i^{IG} (f, x, x') = f(x) - f(x')\n",
    "$$\n",
    "\n",
    "**The completeness axiom also provides a way to measure convergence:**\n",
    "\n",
    "In practice, we can’t compute the exact value of the integral. Instead, we use a discrete sum approximation with kk linearly-spaced points between 0 and 1 for some value of kk. If we only chose 1 point to approximate the integral, that feels like too few. Is 10 enough? 100? Intuitively 1,000 may seem like enough, but can we be certain?\n",
    "\n",
    "We can use the completeness axiom as a sanity check on convergence: run integrated gradients with kk points, measure $| \\sum_{i} \\phi_{i}^{\\mathrm{IG}} (f, x, x') - (f(x) - f(x')) |$, and if the difference is large, re-run with a larger kk.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae3833",
   "metadata": {},
   "source": [
    "## Shapley values for feature attribution\n",
    "\n",
    "**Cooperative game view.** Features are “players” that form coalitions; the model score $f$ is the “value” of a coalition. The **Shapley value** for feature $i$ is the average *marginal contribution* of $i$ over all subsets $S$ that exclude $i$:\n",
    "\n",
    ":::{prf:definition} Shapley value\n",
    "Let $f_\\theta$ be our model, $N$ be the set of features and $S$ be a subset of $N$. The Shapley value of feature $i$ is defined as\n",
    "\n",
    "$$\n",
    "\\phi_i(x) =\n",
    "\\sum_{S \\subseteq N \\setminus \\{i\\}}\n",
    "\\frac{|S|! \\times (|N|-|S|-1)!}{|N|!}\n",
    "\\Big( f_\\theta(S \\cup \\{i\\}) - f_\\theta(S) \\Big).\n",
    "$$\n",
    "\n",
    "$(|N| - |S| - 1)$ = Number of features not in $S$ excluding $i$\n",
    ":::\n",
    "\n",
    "**Combinatorics:** Exact computation is intractable for high-dimensional inputs due to $2^{|N|}$ subsets.\n",
    "\n",
    "**Monte Carlo** approximations sample permutations or coalitions and estimate marginal effects. For tabular data, a common strategy replaces “missing” features using values from a *background dataset* to keep inputs realistic (value with/without and average the difference).\n",
    "\n",
    "**Relation to EG:** EG can be viewed as a Shapley-inspired expectation for continuous inputs (e.g., images), where the background distribution substitutes for discrete coalition masking.\n",
    "\n",
    "### Axioms for Shapley values\n",
    "\n",
    "Shapley is characterized by the following axioms:\n",
    "\n",
    ":::{prf:axiom} Efficiency (or Additivity)\n",
    "The sum of all Shapley values equals the total prediction of the model when all features are included.\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\phi_i(v) = f(N)\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{prf:axiom} Symmetry\n",
    "If two features contribute equally to all possible coalitions, their Shapley values should be equal:\n",
    "\n",
    "- If $ f(S \\cup {i}) = f(S \\cup {j}) $ then $ \\phi_i(f) = \\phi_j(f) $.\n",
    ":::\n",
    "\n",
    ":::{prf:axiom} Dummy (or Null Player)\n",
    "If a feature adds no marginal value to any coalition, its Shapley value should be zero:\n",
    "\n",
    "- If $ f(S \\cup {i}) = f(S) $ then $ \\phi_i(f) = 0 $.\n",
    ":::\n",
    "\n",
    ":::{prf:axiom} Linearity\n",
    "The Shapley value of a sum of two games should be the sum of the Shapley values of the individual games. This allows for decomposition of complex games into simpler ones:\n",
    "$$\n",
    "\\phi_i(f + g) = \\phi_i(f) + \\phi_i(g)\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Expected Gradients (EG)\n",
    "\n",
    ":::{prf:definition} Expected Gradients (EG)\n",
    "To reduce dependence on a single baseline, average Integrated Gradients over a distribution of baselines and random path positions:\n",
    "\n",
    "$$\n",
    "\\phi_i^{EG}(f,x)\n",
    "\\approx\n",
    "\\mathbb{E}_{x' \\sim \\mathcal{D},\\;\\alpha \\sim \\mathrm{U}(0,1)}\n",
    "\\left[\\, (x_i - x_{i}') \\;\n",
    "\\frac{\\partial f\\!\\left(x' + \\alpha(x - x')\\right)}{\\partial x_i}\\, \\right]\n",
    "$$\n",
    "where $\\mathcal{D}$ is a background distribution (e.g., samples from the dataset).\n",
    ":::\n",
    "\n",
    "**Practical recipe:**\n",
    "\n",
    "1. Sample baselines $x' \\sim \\mathcal{D}$.\n",
    "2. For each $x'$, sample $\\alpha \\in [0,1]$ and evaluate the IG integrand along the straight-line path from $x'$ to $x$.\n",
    "3. Average the contributions to obtain $\\mathrm{EG}(x)$.\n",
    "\n",
    "This links path-integral attributions to **data-aware** notions of missingness and connects to Shapley-style expectations in image space."
   ],
   "id": "6e72d0de5c138e8e"
  },
  {
   "cell_type": "markdown",
   "id": "afb7cc52",
   "metadata": {},
   "source": [
    "## Counterfactual explanations (multi-objective view)\n",
    "\n",
    "**Definition.** Given an instance $x$ with an undesirable prediction $f(x)$, a **counterfactual** is a nearby $x'$ that achieves a desired outcome (e.g., flip a class) while satisfying constraints (plausibility/feasibility).\n",
    "\n",
    "**Typical objectives.**\n",
    "- **Proximity:** make $x'$ close to $x$ (small $\\|x'-x\\|$).\n",
    "- **Sparsity:** change as few features as possible.\n",
    "- **Plausibility/feasibility:** $x'$ should be realistic and respect constraints (immutability, valid ranges).\n",
    "- **Validity:** $f(x')$ attains the target outcome.\n",
    "\n",
    "**Optimization.** Many methods frame counterfactual search as a **multi-objective** problem and seek a diverse set of Pareto-optimal solutions. Selection emphasizes both performance (target achievement) and diversity among viable changes.\n",
    "\n",
    "**Strengths & limitations.**\n",
    "- Clear, actionable “what-to-change” guidance; can work with black-box models (just need $f(\\cdot)$).\n",
    "- Hyper-local: insightful for an instance, but less suited for global model understanding without aggregation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec83528",
   "metadata": {},
   "source": [
    "## Practical notes and recap\n",
    "\n",
    "- **Gradients** are fast but can be noisy and saturate.  \n",
    "- **Integrated Gradients** mitigate saturation by aggregating along a path from a **baseline**; results depend on the baseline choice.  \n",
    "- **Expected Gradients** average IG over baselines and path positions using a **background dataset**, reducing baseline bias.  \n",
    "- **Shapley values** provide principled attributions with fairness axioms; use Monte Carlo approximations with realistic background data.  \n",
    "- **Counterfactuals** offer actionable, instance-level recourses via multi-objective optimization (proximity, sparsity, plausibility, validity).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
