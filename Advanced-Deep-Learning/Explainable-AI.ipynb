{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f069ac",
   "metadata": {},
   "source": "# Explainable AI"
  },
  {
   "cell_type": "markdown",
   "id": "30598122",
   "metadata": {},
   "source": [
    "## Why explainability matters\n",
    "\n",
    "- Modern deep models often function as **black boxes**, prioritizing predictive performance over transparency.\n",
    "- Transparency is crucial where models **impact society** (health, credit, justice, safety).\n",
    "- A typical ML pipeline maps inputs (image/text/tabular) through a function $f_\\theta(\\cdot)$ to an output $\\hat{y}$; errors backpropagate to optimize $\\theta$. Understanding **what** features and **why** they influence $\\hat{y}$ motivates XAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34959a",
   "metadata": {},
   "source": [
    "## Two core challenges in Explainable AI (XAI)\n",
    "\n",
    "- **Missingness problem.** How should we represent a feature being “absent”? Choice of baseline or masking strongly affects explanations.\n",
    "- **Correlation problem.** Real features are **dependent**. Methods that vary one feature while holding others fixed may produce unrealistic inputs and misleading attributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de180e67",
   "metadata": {},
   "source": [
    "## Nomenclature and taxonomy\n",
    "\n",
    "- **Intrinsic vs post hoc.** Intrinsic (glass box) models are interpretable by design; post hoc methods explain already-trained black-box models.\n",
    "- **Local vs global.** Local methods explain a specific prediction; global methods summarize model behavior across the dataset.\n",
    "- **Surrogates.** A simple, interpretable model $g$ approximates a complex $f$ (locally or globally) to obtain human-readable explanations.\n",
    "- **Counterfactuals.** “What minimal change to $x$ would flip the decision?” Provides actionable insight for an instance.\n",
    "- **Data modalities.** Images, text, and tabular data call for different masking/baseline strategies and surrogates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72a8d3",
   "metadata": {},
   "source": [
    "## Glass-box models\n",
    "\n",
    "### Linear models\n",
    "\n",
    "**Linear regression.** With standardized features, coefficients indicate relative importance.\n",
    "- Prediction: $\\hat{y}=\\beta_0+\\sum_{j=1}^p \\beta_j x_j$.\n",
    "- Fit by least squares (MSE):\n",
    "$$\n",
    "\\min_{\\beta_0,\\ldots,\\beta_p}\\ \\sum_{i=1}^n \\Bigl(y^{(i)}-\\beta_0-\\sum_{j=1}^p\\beta_j x^{(i)}_j\\Bigr)^2.\n",
    "$$\n",
    "\n",
    "**Logistic regression.** Probabilistic classification with sigmoid/softmax link.\n",
    "- Binary: $P(y=1\\mid x)=\\sigma(z)$, where $z=\\beta_0+\\sum_j \\beta_j x_j$, $\\ \\sigma(z)=\\tfrac{1}{1+e^{-z}}$.\n",
    "- Fit by (negative) log-likelihood:\n",
    "$$\n",
    "\\min_{\\beta}\\ -\\sum_{i=1}^n \\Bigl[y^{(i)}\\log\\sigma(z^{(i)})+(1-y^{(i)})\\log(1-\\sigma(z^{(i)}))\\Bigr].\n",
    "$$\n",
    "\n",
    "**Interpretation caveats.**\n",
    "- Standardization helps compare $|\\beta_j|$ across features.\n",
    "- Signs: $\\beta_j\\!\\!>\\!0$ means increasing $x_j$ increases $\\hat{y}$ (or the log-odds).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6081c",
   "metadata": {},
   "source": [
    "### Decision trees (and forests)\n",
    "\n",
    "**Decision trees.** Interpretable by **rules/paths** from root to leaf. For classification, each leaf stores a class probability; the prediction comes from the leaf reached by the instance.\n",
    "\n",
    "**Purity via entropy** (classification):\n",
    "$$\n",
    "H(D)= -\\sum_{c=1}^k p(C_c)\\log_2 p(C_c).\n",
    "$$\n",
    "\n",
    "**Information gain** for attribute $A$:\n",
    "$$\n",
    "\\mathrm{IG}(D,A)= H(D) - \\sum_{t\\in\\text{splits}} \\frac{|D_t|}{|D|} H(D_t).\n",
    "$$\n",
    "\n",
    "**Growing trees.** Greedy splits that maximize information gain until stopping criteria (max depth, min leaf size, purity). Watch for **overfitting**; forests (bagging, random feature subsets) improve generalization and support tasks like outlier detection (e.g., Isolation Forest).\n",
    "\n",
    "**Explainability.**\n",
    "- Global: feature importances, prototypical rules.\n",
    "- Local: path-level rules for a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec46839",
   "metadata": {},
   "source": [
    "## Perturbation-based explanations: occlusion\n",
    "\n",
    "**Idea.** Measure output change when parts of the input are masked.\n",
    "- Sliding window occlusion trades off **window size** and **stride**; results can vary notably.\n",
    "- **Adaptive occlusion** seeks the smallest region that preserves the prediction:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{m}) = \\bigl|\\hat{y}(\\mathbf{x})-\\hat{y}(\\mathbf{x}\\odot\\mathbf{m})\\bigr| + \\alpha \\sum_{i}|m_i|,\n",
    "$$\n",
    "with mask $\\mathbf{m}\\in[0,1]^d$ and elementwise product $\\odot$. Smaller masks with similar scores are preferred.\n",
    "\n",
    "**Randomized occlusion.** Sample many random masks, score, normalize, and aggregate into a heatmap.\n",
    "\n",
    "**Caveats.**\n",
    "- Can resemble segmentation rather than *decision* evidence.\n",
    "- Computationally expensive (many forward passes).\n",
    "- Sensitive to the choice of baseline (missingness). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75785c",
   "metadata": {},
   "source": [
    "## Class Activation Mapping (CAM and Grad-CAM)\n",
    "\n",
    "**CAM (for specific CNN architectures).**\n",
    "- Requires a **global average pooling** just before a linear classifier (softmax).\n",
    "- For class $c$, the map is the **weighted sum** of final conv feature maps:\n",
    "$$\n",
    "M_c=\\mathrm{ReLU}\\!\\left(\\sum_{k} w^{c}_{k}\\,A^{k}\\right),\n",
    "$$\n",
    "where $A^k$ are feature maps and $w_k^c$ are classifier weights. ReLU highlights positively contributing regions.\n",
    "\n",
    "**Grad-CAM (architecture-agnostic for CNNs).**\n",
    "- Uses gradients to obtain weights $\\alpha_k^c$ for the last conv layer’s feature maps:\n",
    "$$\n",
    "M_c=\\mathrm{ReLU}\\!\\left(\\sum_{k} \\alpha^{c}_{k}\\,A^{k}\\right),\\qquad\n",
    "\\alpha^{c}_{k}\\propto \\text{global average of }\\frac{\\partial \\hat{y}_c}{\\partial A^{k}}.\n",
    "$$\n",
    "- Frees architectural constraints, but resolution depends on the final conv layer; heatmap intensities are not comparable across images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd46f2",
   "metadata": {},
   "source": [
    "## Surrogate explanations: LIME (local, model-agnostic)\n",
    "\n",
    "**Goal.** Approximate a black-box $f$ near an instance $x$ by a simple model $g$ in an interpretable feature space $z'$ (e.g., superpixels, word indicators).\n",
    "- Sample perturbations around $x$ in $z'$-space, map back to original space $z$, and evaluate $f(z)$.\n",
    "- Fit $g$ with locality weights $\\pi_x(z)$:\n",
    "$$\n",
    "\\min_{g\\in\\mathcal{G}}\\ \\sum_{i}\\ \\pi_x(z_i)\\,\\bigl(f(z_i)-g(z'_i)\\bigr)^2\\ +\\ \\Omega(g),\n",
    "$$\n",
    "where $\\Omega(g)$ penalizes complexity (sparsity).\n",
    "\n",
    "**Design choices.**\n",
    "- Interpretable binary representation $z'$ (e.g., superpixel on/off).\n",
    "- Locality kernel $\\pi_x$ (e.g., exponential kernel in interpretable space).\n",
    "- Fidelity vs simplicity trade-off via $\\Omega(g)$.\n",
    "\n",
    "**Limitations.**\n",
    "- Faithfulness depends on sampling scheme, kernel, and representation of **missingness**.\n",
    "- Correlated features and unrealistic perturbations can mislead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46ea26",
   "metadata": {},
   "source": [
    "## Plot-based global summaries: PDP and ICE\n",
    "\n",
    "**Partial dependence (PDP).** For feature $i$,\n",
    "$$\n",
    "\\mathrm{PD}_i(x_i) = \\mathbb{E}_{x_{-i}}\\bigl[f(x_i,x_{-i})\\bigr].\n",
    "$$\n",
    "Averages out other features to show the **marginal** effect of $x_i$.\n",
    "\n",
    "**Individual conditional expectation (ICE).**\n",
    "- For a fixed instance $x^{(j)}$, trace $f(x_i, x_{-i}^{(j)})$ as $x_i$ varies.\n",
    "- Reveals **heterogeneity** that PDP might hide.\n",
    "\n",
    "**Caveats.**\n",
    "- If features are correlated, varying $x_i$ alone may create unrealistic inputs (correlation problem).\n",
    "- Aggregation across the dataset can hide important local behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a077ced",
   "metadata": {},
   "source": [
    "## Good practice and common pitfalls\n",
    "\n",
    "- Always consider the **missingness** and **correlation** problems when designing explanations.\n",
    "- Prefer **data-aware** perturbations and realistic baselines/masks.\n",
    "- Validate explanations qualitatively and, where possible, **quantitatively** (e.g., deletion/insertion tests, sanity checks).\n",
    "- Combine complementary views: intrinsic models, local surrogates, perturbation maps, and global plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b9570",
   "metadata": {},
   "source": [
    "# Explainable AI II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1fce7",
   "metadata": {},
   "source": [
    "## Gradient-based attribution: vanilla gradients and Gradient × Input\n",
    "\n",
    "**Setup.** Let a trained model produce a scalar score $f(x)$ for input $x \\in \\mathbb{R}^d$ (e.g., the logit for a chosen class).\n",
    "\n",
    "- **Vanilla gradients (saliency):** use the input gradient $\\nabla_x f(x)$ as an importance signal for each feature $i$:\n",
    "  $$S_i(x) \\;=\\; \\frac{\\partial f(x)}{\\partial x_i}.$$\n",
    "  This measures local sensitivity of $f$ to infinitesimal changes in $x_i$.\n",
    "\n",
    "- **Gradient × Input:** scale sensitivity by the feature value to indicate direction and magnitude:\n",
    "  $$M_i(x) \\;=\\; x_i \\cdot \\frac{\\partial f(x)}{\\partial x_i}.$$\n",
    "  This heuristic often sharpens attribution by weighting gradients with input intensity.\n",
    "\n",
    "**Known issues.** Raw input-gradients can be *noisy* and reflect high-frequency, local properties of piecewise-linear networks. They may also suffer from **saturation** (e.g., ReLU plateaus or sigmoid saturation), where important features can have near-zero gradients even though the prediction relies on them. This motivates path-based methods that probe sensitivity away from the saturated neighborhood around $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5218156",
   "metadata": {},
   "source": [
    "## Integrated Gradients (IG)\n",
    "\n",
    "**Goal.** Attribute the prediction at $x$ relative to a *baseline* (reference) $x_0$ by aggregating gradients **along a path** from $x_0$ to $x$. The standard straight-line path gives attributions\n",
    "$$\n",
    "\\mathrm{IG}_i\\!\\left(f, x, x_0\\right)\n",
    "\\;=\\;\n",
    "(x_i - x_i^{0}) \\int_{0}^{1}\n",
    "\\frac{\\partial f\\!\\left(x_0 + \\alpha (x - x_0)\\right)}{\\partial x_i}\\, d\\alpha.\n",
    "$$\n",
    "\n",
    "**Intuition.** Starting from $x_0$ (representing feature *missingness* or neutral input), gradually morph into $x$; accumulate each feature’s gradient contribution along the way. This alleviates local saturation around $x$ and captures how changes along the entire path affect $f$.\n",
    "\n",
    "**Discrete approximation.** Using $m$ steps with points $x^{(k)} = x_0 + \\tfrac{k}{m}(x - x_0)$:\n",
    "$$\n",
    "\\mathrm{IG}_i \\approx (x_i - x_i^{0}) \\cdot \\frac{1}{m}\\sum_{k=1}^{m}\n",
    "\\frac{\\partial f\\!\\left(x^{(k)}\\right)}{\\partial x_i}.\n",
    "$$\n",
    "\n",
    "**Baseline choice matters.** Common baselines include a *black/white* image, *blurred* input, *uniform* random, or *Gaussian* noise references. Each choice encodes a notion of “feature missingness” and can bias the attribution. In practice, one should justify the baseline and, when possible, use multiple references or data-driven baselines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25854eea",
   "metadata": {},
   "source": [
    "## Expected Gradients (EG)\n",
    "\n",
    "**Motivation.** To reduce dependence on a single baseline, average Integrated Gradients over a distribution of baselines and random path positions. **Expected Gradients** compute\n",
    "$$\n",
    "\\mathrm{EG}_i(f,x)\n",
    "\\;=\\;\n",
    "\\mathbb{E}_{x_0 \\sim \\mathcal{D},\\;\\alpha \\sim \\mathrm{U}(0,1)}\n",
    "\\left[\\, (x_i - x_{0,i}) \\;\n",
    "\\frac{\\partial f\\!\\left(x_0 + \\alpha(x - x_0)\\right)}{\\partial x_i}\\, \\right],\n",
    "$$\n",
    "where $\\mathcal{D}$ is a background distribution (e.g., samples from the dataset).\n",
    "\n",
    "**Practical recipe.**\n",
    "1. Sample baselines $x_0 \\sim \\mathcal{D}$.\n",
    "2. For each $x_0$, sample $\\alpha \\in [0,1]$ and evaluate the IG integrand along the straight-line path from $x_0$ to $x$.\n",
    "3. Average the contributions to obtain $\\mathrm{EG}(x)$.\n",
    "\n",
    "This links path-integral attributions to **data-aware** notions of missingness and connects to Shapley-style expectations in image space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae3833",
   "metadata": {},
   "source": [
    "## Shapley values for feature attribution\n",
    "\n",
    "**Cooperative game view.** Features are “players” that form coalitions; the model score $f$ is the “value” of a coalition. The **Shapley value** for feature $i$ is the average *marginal contribution* of $i$ over all subsets $S$ that exclude $i$:\n",
    "$$\n",
    "\\phi_i(f,x) \\;=\\;\n",
    "\\sum_{S \\subseteq N \\setminus \\{i\\}}\n",
    "\\frac{|S|!\\, (|N|-|S|-1)!}{|N|!}\n",
    "\\Big( f(x_{S \\cup \\{i\\}}) - f(x_S) \\Big).\n",
    "$$\n",
    "Shapley is uniquely characterized by axioms (efficiency/additivity, symmetry, dummy, linearity).\n",
    "\n",
    "**Combinatorics.** Exact computation is intractable for high-dimensional inputs due to $2^{|N|}$ subsets. **Monte Carlo** approximations sample permutations or coalitions and estimate marginal effects. For tabular data, a common strategy replaces “missing” features using values from a *background dataset* to keep inputs realistic (value with/without and average the difference).\n",
    "\n",
    "**Relation to EG.** EG can be viewed as a Shapley-inspired expectation for continuous inputs (e.g., images), where the background distribution substitutes for discrete coalition masking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7cc52",
   "metadata": {},
   "source": [
    "## Counterfactual explanations (multi-objective view)\n",
    "\n",
    "**Definition.** Given an instance $x$ with an undesirable prediction $f(x)$, a **counterfactual** is a nearby $x'$ that achieves a desired outcome (e.g., flip a class) while satisfying constraints (plausibility/feasibility).\n",
    "\n",
    "**Typical objectives.**\n",
    "- **Proximity:** make $x'$ close to $x$ (small $\\|x'-x\\|$).\n",
    "- **Sparsity:** change as few features as possible.\n",
    "- **Plausibility/feasibility:** $x'$ should be realistic and respect constraints (immutability, valid ranges).\n",
    "- **Validity:** $f(x')$ attains the target outcome.\n",
    "\n",
    "**Optimization.** Many methods frame counterfactual search as a **multi-objective** problem and seek a diverse set of Pareto-optimal solutions. Selection emphasizes both performance (target achievement) and diversity among viable changes.\n",
    "\n",
    "**Strengths & limitations.**\n",
    "- Clear, actionable “what-to-change” guidance; can work with black-box models (just need $f(\\cdot)$).\n",
    "- Hyper-local: insightful for an instance, but less suited for global model understanding without aggregation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec83528",
   "metadata": {},
   "source": [
    "## Practical notes and recap\n",
    "\n",
    "- **Gradients** are fast but can be noisy and saturate.  \n",
    "- **Integrated Gradients** mitigate saturation by aggregating along a path from a **baseline**; results depend on the baseline choice.  \n",
    "- **Expected Gradients** average IG over baselines and path positions using a **background dataset**, reducing baseline bias.  \n",
    "- **Shapley values** provide principled attributions with fairness axioms; use Monte Carlo approximations with realistic background data.  \n",
    "- **Counterfactuals** offer actionable, instance-level recourses via multi-objective optimization (proximity, sparsity, plausibility, validity).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
