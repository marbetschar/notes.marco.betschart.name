{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f309c3e9",
   "metadata": {},
   "source": "# Basic Concepts"
  },
  {
   "cell_type": "markdown",
   "id": "2a8773d0",
   "metadata": {},
   "source": [
    "## Neural networks types and when to use them\n",
    "\n",
    "Deep learning offers several standard network types, each with typical application domains.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Architecture choice** depends on input structure:\n",
    "  - MLPs for vector features,\n",
    "  - CNNs for grid-like data,\n",
    "  - RNNs/Transformers for sequences,\n",
    "  - GNNs for general graphs.\n",
    "\n",
    "- **MLPs** are universal function approximators, but dense and parameter-heavy for structured inputs.\n",
    "\n",
    "- **CNNs** leverage convolution:\n",
    "  - Weight sharing, sparse connections, and receptive fields enable efficient representation learning on grids.\n",
    "\n",
    "- A rich set of **design patterns** make CNNs more effective and efficient:\n",
    "  - Downsampling: pooling, strided convs,\n",
    "  - Normalization: batch norm,\n",
    "  - Residual connections for deep networks,\n",
    "  - Upsampling: interpolation, unpooling, transposed convs,\n",
    "  - Bottlenecks and small kernels to reduce parameters,\n",
    "  - Multi-scale filters (Inception),\n",
    "  - Separable and depthwise separable convs,\n",
    "  - Multi-head architectures for multi-task learning,\n",
    "  - Sparsity and pruning to reduce cost.\n",
    "\n",
    "- **Classic architectures** (AlexNet, VGG, GoogLeNet, ResNet) instantiate these ideas in different ways and\n",
    "  form the foundation for many modern deep learning models.\n",
    "\n",
    "**Multi-Layer Perceptrons (MLPs / dense networks)**\n",
    "\n",
    "- Inputs and outputs are usually **fixed-dimensional vectors**.\n",
    "- Every neuron in one layer is connected to every neuron in the next.\n",
    "- Typical use cases:\n",
    "  - Tabular data (e.g. apartment features → rent),\n",
    "  - Final classification head after feature extraction,\n",
    "  - Generic feature transformation and dimensionality reduction.\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "- Exploit local structure and translation invariance.\n",
    "- Typical input types:\n",
    "  - 1D signals: audio, ECG, vibration signals, spectral data,\n",
    "  - 2D signals: images, game boards, card layouts,\n",
    "  - 3D data: volumetric medical scans, videos (2D + time), etc.\n",
    "- Standard choice for:\n",
    "  - Image classification, detection, segmentation,\n",
    "  - Many generative image models,\n",
    "  - Any data that live on a grid.\n",
    "\n",
    "**Recurrent Neural Networks (RNNs) and sequence models**\n",
    "\n",
    "- Process sequences step by step (text, time series, signals).\n",
    "- Historically used for:\n",
    "  - Language modeling (e.g. predicting the next word),\n",
    "  - Speech recognition,\n",
    "  - Sequential prediction tasks.\n",
    "\n",
    "(Modern sequence models are often **Transformers**, covered in later lectures.)\n",
    "\n",
    "**Graph Neural Networks (GNNs)**\n",
    "\n",
    "- Neural networks on **graph-structured** data.\n",
    "- CNNs can be viewed as a special case where the underlying graph is a regular grid.\n",
    "- Typical use cases:\n",
    "  - Molecules and materials,\n",
    "  - Networks and relational data.\n",
    "\n",
    "The choice of architecture is mainly driven by:\n",
    "\n",
    "- The **structure of the input** (vector, sequence, grid, graph),\n",
    "- The **symmetries and invariances** we want to exploit (e.g. translation invariance for images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e485d9",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptrons (MLPs)\n",
    "\n",
    "An MLP (multi-layer perceptron) is a stack of **fully connected layers**.\n",
    "\n",
    "For a single neuron with inputs $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "- Weights $w \\in \\mathbb{R}^n$,\n",
    "- Bias $b \\in \\mathbb{R}$,\n",
    "- Activation function $\\sigma$ (e.g. ReLU, sigmoid, tanh),\n",
    "\n",
    "the output is\n",
    "\n",
    "$$\n",
    "y = \\sigma\\!\\left(\\sum_{k=1}^n w_k x_k + b\\right)\n",
    "  = \\sigma(w^\\top x + b).\n",
    "$$\n",
    "\n",
    "A layer applies this operation to each neuron in parallel; stacking layers gives a deep network:\n",
    "\n",
    "$$\n",
    "h^{(1)} = \\sigma\\big(W^{(1)} x + b^{(1)}\\big), \\quad\n",
    "h^{(2)} = \\sigma\\big(W^{(2)} h^{(1)} + b^{(2)}\\big), \\quad \\dots\n",
    "$$\n",
    "\n",
    "### Universal approximation theorem\n",
    "\n",
    "The slides recall the **universal approximation theorem**:\n",
    "\n",
    "> A feed-forward network with a single hidden layer and a suitable non-linear activation function\n",
    "> can approximate any continuous function on a compact subset of $\\mathbb{R}^n$, given enough hidden units.\n",
    "\n",
    "Symbolically,\n",
    "\n",
    "$$\n",
    "f(x) \\approx \\sum_{i=1}^{N(\\varepsilon)} a_i\\, \\sigma(w_i \\cdot x + b_i),\n",
    "$$\n",
    "\n",
    "where $N(\\varepsilon)$ is large enough so that the approximation error is below $\\varepsilon$.\n",
    "\n",
    "This does **not** mean:\n",
    "\n",
    "- Any network is automatically good;\n",
    "- It says that, in principle, **capacity** is sufficient, but training and generalization are separate issues.\n",
    "\n",
    "### Typical uses of MLPs\n",
    "\n",
    "- Problems where inputs are already meaningful **features** (e.g. engineered descriptors, small vectors),\n",
    "- Final stages of larger networks:\n",
    "  - After CNN feature extraction (for classification),\n",
    "  - After attention/Transformer layers,\n",
    "- As generic **non-linear feature transforms** inside larger architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44b109",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are built from **convolutional layers**, which apply learnable filters across the input.\n",
    "\n",
    "### Convolution as an operation\n",
    "\n",
    "Continuous-time convolution of functions $f$ and $g$:\n",
    "\n",
    "$$\n",
    "(f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)\\,g(t - \\tau)\\,d\\tau.\n",
    "$$\n",
    "\n",
    "Discrete 1D convolution:\n",
    "\n",
    "$$\n",
    "(f * g)[n]\n",
    "= \\sum_{m=-M}^{M} f[n - m]\\, g[m].\n",
    "$$\n",
    "\n",
    "2D convolution for images (with kernel $\\omega$ and image $f$):\n",
    "\n",
    "$$\n",
    "g(x,y)\n",
    "= (\\omega * f)(x,y)\n",
    "= \\sum_{i=-a}^{a} \\sum_{j=-b}^{b}\n",
    "    \\omega(i,j)\\, f(x - i, y - j).\n",
    "$$\n",
    "\n",
    "In many deep learning libraries, the implemented operation is actually **cross-correlation** (no kernel flip):\n",
    "\n",
    "$$\n",
    "g(x,y)\n",
    "= \\sum_{i=-a}^{a} \\sum_{j=-b}^{b}\n",
    "    \\omega(i,j)\\, f(x + i, y + j).\n",
    "$$\n",
    "\n",
    "Since the kernel $\\omega$ is learned, the distinction does not matter in practice.\n",
    "\n",
    "### Key properties of CNNs\n",
    "\n",
    "- **Weight sharing**:\n",
    "  - The same filter is applied at all spatial positions.\n",
    "  - Greatly reduces the number of parameters.\n",
    "\n",
    "- **Sparse connectivity**:\n",
    "  - Each output location depends only on a **local neighborhood** in the input,\n",
    "  - Defined by the kernel size (e.g. $3 \\times 3$, $5 \\times 5$).\n",
    "\n",
    "- **Receptive field**:\n",
    "  - The region of the input that can influence a given activation.\n",
    "  - Increases with depth: deeper layers see larger and more abstract regions.\n",
    "\n",
    "### Multiple channels\n",
    "\n",
    "For multi-channel inputs (e.g. RGB images), each filter has a **separate kernel per input channel**; outputs are summed:\n",
    "\n",
    "- Input shape: $(\\text{channels}_\\text{in}, H, W)$,\n",
    "- Kernel shape: $(\\text{channels}_\\text{out}, \\text{channels}_\\text{in}, k_H, k_W)$,\n",
    "- Each output channel is a sum over input channels convolved with its corresponding kernels.\n",
    "\n",
    "### CNNs as representation learning\n",
    "\n",
    "CNNs automatically learn **feature detectors**:\n",
    "\n",
    "- Early layers: edges, colors, simple patterns,\n",
    "- Intermediate layers: textures, parts (eyes, wheels, etc.),\n",
    "- Deeper layers: object-level features.\n",
    "\n",
    "Pipeline view:\n",
    "\n",
    "> Input → Feature extraction (convolutional stack) → Feature classification (MLP head) → Output\n",
    "\n",
    "CNNs can also be seen as a special case of **graph neural networks**, where the underlying graph is the pixel grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497a44f",
   "metadata": {},
   "source": [
    "## Controlling spatial resolution: pooling and strided convolutions\n",
    "\n",
    "As data flows through a CNN, we often **reduce spatial size** while increasing the **number of channels**.\n",
    "\n",
    "### Why reduce spatial size?\n",
    "\n",
    "- To aggregate information from larger regions,\n",
    "- To reduce computational cost and memory usage,\n",
    "- To design encoder–decoder architectures (encoder compresses, decoder expands).\n",
    "\n",
    "### Pooling\n",
    "\n",
    "**Max pooling** and **average pooling** operate on local windows (e.g. $2 \\times 2$, $3 \\times 3$):\n",
    "\n",
    "- Max pooling:\n",
    "  - Takes the maximum in each window.\n",
    "  - Emphasizes strong activations and introduces some invariance to small translations.\n",
    "- Average pooling:\n",
    "  - Takes the average value.\n",
    "  - Smooths activations; less commonly used in modern deep CNNs than max pooling or strided convolutions.\n",
    "\n",
    "Pooling reduces resolution by the **stride** of the pooling window (e.g. stride 2 halves height and width).\n",
    "\n",
    "### Strided convolutions\n",
    "\n",
    "Instead of separate pooling layers, we can use **strided convolutions**:\n",
    "\n",
    "- The filter moves by more than one pixel at a time (stride > 1),\n",
    "- This simultaneously performs feature extraction and downsampling.\n",
    "\n",
    "Strided convolutions and pooling are both used to control spatial resolution in many architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d736170",
   "metadata": {},
   "source": [
    "## Normalization and batch normalization\n",
    "\n",
    "Deep networks can be hard to train because the distribution of activations in each layer changes during training.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "- Slower convergence,\n",
    "- Sensitivity to initialization,\n",
    "- More difficult tuning of learning rates.\n",
    "\n",
    "### Batch normalization\n",
    "\n",
    "Batch normalization (BatchNorm) normalizes activations within a mini-batch and then applies a learned affine transform.\n",
    "\n",
    "For each channel $c$:\n",
    "\n",
    "1. Compute mini-batch mean and variance:\n",
    "\n",
    "   $$\n",
    "   \\mu_c = \\frac{1}{m} \\sum_{b,x,y} I_{b,c,x,y}, \\qquad\n",
    "   \\sigma_c^2 = \\frac{1}{m} \\sum_{b,x,y} (I_{b,c,x,y} - \\mu_c)^2,\n",
    "   $$\n",
    "\n",
    "   where $I_{b,c,x,y}$ is the input activation, and $m$ is the number of elements in the batch for that channel.\n",
    "\n",
    "2. Normalize and scale/shift:\n",
    "\n",
    "   $$\n",
    "   O_{b,c,x,y}\n",
    "   = \\gamma_c \\frac{I_{b,c,x,y} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\varepsilon}}\n",
    "     + \\beta_c,\n",
    "   $$\n",
    "\n",
    "   with learnable parameters $\\gamma_c$ and $\\beta_c$.\n",
    "\n",
    "Effects:\n",
    "\n",
    "- Stabilizes and accelerates training,\n",
    "- Often allows **higher learning rates**,\n",
    "- Acts as a regularizer and can improve generalization.\n",
    "\n",
    "At inference time, running averages of $\\mu_c$ and $\\sigma_c^2$ are used instead of batch statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbb88e",
   "metadata": {},
   "source": [
    "## Residual connections\n",
    "\n",
    "When networks become very deep, plain stacks of layers are hard to train:\n",
    "\n",
    "- Gradients can vanish or explode,\n",
    "- Adding more layers can **hurt** performance, even if, in principle, a deeper network should be at least as good.\n",
    "\n",
    "### Residual blocks\n",
    "\n",
    "Residual networks (ResNets) introduce **skip connections**:\n",
    "\n",
    "Instead of learning a mapping $H(x)$ directly, the network learns a **residual function** $F(x)$ such that\n",
    "\n",
    "$$\n",
    "H(x) = F(x) + x.\n",
    "$$\n",
    "\n",
    "A residual block computes:\n",
    "\n",
    "1. $F(x)$ via a small stack of layers (e.g. conv → BN → ReLU → conv → BN),\n",
    "2. Adds the input: $y = F(x) + x$,\n",
    "3. Applies a non-linearity (often ReLU) after the addition.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- The network can easily learn the **identity mapping** (just set $F(x) \\approx 0$),\n",
    "- Gradients can propagate more directly through the skip connection,\n",
    "- Enables training of **very deep** networks (e.g. 50, 101, 152 layers and beyond).\n",
    "\n",
    "Variants:\n",
    "\n",
    "- Identity skip connections when input and output shapes match,\n",
    "- Projection (e.g. $1 \\times 1$ conv) in the skip path to match dimensions,\n",
    "- Bottleneck blocks (with $1 \\times 1$ convs) in deeper ResNets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035236",
   "metadata": {},
   "source": [
    "## Increasing spatial resolution: upsampling and transposed convolutions\n",
    "\n",
    "Encoder–decoder architectures (e.g. for segmentation, image generation, inpainting) must **increase** spatial resolution\n",
    "in later stages.\n",
    "\n",
    "### Upsampling\n",
    "\n",
    "Common upsampling methods:\n",
    "\n",
    "- **Nearest neighbor**:\n",
    "  - Each input pixel is replicated into a larger block (e.g. $2 \\times 2$),\n",
    "  - Simple and fast; can produce blocky outputs.\n",
    "\n",
    "- **Bed of nails**:\n",
    "  - Insert zeros between input pixels, then optionally apply a convolution,\n",
    "  - Separates the “increase resolution” step from “learned filtering”.\n",
    "\n",
    "- **Interpolation-based upsampling** (bilinear, bicubic):\n",
    "  - Smooth interpolations; often followed by a convolution layer.\n",
    "\n",
    "### Max unpooling\n",
    "\n",
    "If max pooling was used in the encoder:\n",
    "\n",
    "- The positions of the max elements are stored,\n",
    "- **Max unpooling** puts the pooled value back into its original location in a larger map, filling other entries with zeros,\n",
    "- This can help preserve spatial structure (e.g. edges or boundaries).\n",
    "\n",
    "### Transposed convolutions\n",
    "\n",
    "Transposed convolutions (sometimes called “deconvolutions”) implement **learned upsampling**:\n",
    "\n",
    "- Conceptually, they invert the shape change of a regular convolution with stride > 1,\n",
    "- Implementation:\n",
    "  - Spread each input value into a larger output region via the filter,\n",
    "  - Overlapping contributions are summed.\n",
    "\n",
    "Properties:\n",
    "\n",
    "- Learnable filters, similar to standard convolutions,\n",
    "- Can increase both **spatial resolution** and adjust **number of channels**,\n",
    "- Widely used in encoder–decoder CNNs and generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd31d87",
   "metadata": {},
   "source": [
    "## Bottleneck layers and channel reduction\n",
    "\n",
    "Deep CNNs often use many channels, which can make later layers very expensive:\n",
    "\n",
    "- A convolution with kernel size $k \\times k$, $C_\\text{in}$ input channels and $C_\\text{out}$ output channels\n",
    "  has $k^2 C_\\text{in} C_\\text{out}$ parameters.\n",
    "\n",
    "If $C_\\text{in}$ and $C_\\text{out}$ are both large, this is costly.\n",
    "\n",
    "### Bottleneck idea\n",
    "\n",
    "Use **$1 \\times 1$ convolutions** to reduce (or expand) the number of channels, forming a bottleneck:\n",
    "\n",
    "- Compress channels: $C_\\text{in} \\to C_\\text{mid}$ with a $1 \\times 1$ conv,\n",
    "- Apply a more expensive $k \\times k$ conv with fewer channels ($C_\\text{mid}$),\n",
    "- Optionally expand back: $C_\\text{mid} \\to C_\\text{out}$ with another $1 \\times 1$ conv.\n",
    "\n",
    "Example comparison from the slides:\n",
    "\n",
    "- Direct $3 \\times 3$ conv, $256 \\to 256$ channels:\n",
    "\n",
    "  $$\n",
    "  \\text{params} = (3 \\cdot 3 \\cdot 256 + 1) \\cdot 256 \\approx 590{,}000.\n",
    "  $$\n",
    "\n",
    "- Bottleneck: $256 \\to 64 \\to 64 \\to 256$ using $1 \\times 1$ and $3 \\times 3$ convs:\n",
    "\n",
    "  $$\n",
    "  (1 \\cdot 1 \\cdot 256 + 1) \\cdot 64\n",
    "  + (3 \\cdot 3 \\cdot 64 + 1) \\cdot 64\n",
    "  + (1 \\cdot 1 \\cdot 64 + 1) \\cdot 256\n",
    "  \\approx 70{,}000.\n",
    "  $$\n",
    "\n",
    "So the bottleneck block has **far fewer parameters** for a similar receptive field.\n",
    "\n",
    "Bottlenecks are a core component of:\n",
    "\n",
    "- Deeper ResNets (e.g. ResNet-50, ResNet-101),\n",
    "- Inception-style architectures (for dimensionality reduction before expensive convolutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a37c4",
   "metadata": {},
   "source": [
    "## Deeper networks with small kernels\n",
    "\n",
    "Another way to reduce parameters while keeping a large receptive field is to replace **large kernels** with stacks of\n",
    "**small kernels**.\n",
    "\n",
    "Example:\n",
    "\n",
    "- A single $11 \\times 11$ convolution has kernel size $11 \\cdot 11 = 121$ per input–output channel pair.\n",
    "- Three stacked $3 \\times 3$ convolutions:\n",
    "\n",
    "  - Each kernel is $3 \\cdot 3 = 9$ parameters,\n",
    "  - Three layers give a receptive field of $7 \\times 7$ or larger, depending on padding and stride,\n",
    "  - More layers = more non-linearities and representation power.\n",
    "\n",
    "More generally:\n",
    "\n",
    "- Several small kernels can approximate the effect of a large kernel at **lower cost**,\n",
    "- Deep stacks of $3 \\times 3$ convs (as in VGG) have become a common pattern.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Increased depth (more expressiveness),\n",
    "- Fewer parameters,\n",
    "- More regular structure, easier to tune and implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800cb44",
   "metadata": {},
   "source": [
    "## Multi-scale filters and Inception-style modules\n",
    "\n",
    "Classical image processing often uses filters at **multiple scales** to detect different types of features.\n",
    "\n",
    "In CNNs, using a single kernel size (e.g. $3 \\times 3$) might miss patterns that are best captured by larger or smaller\n",
    "receptive fields.\n",
    "\n",
    "### Inception modules\n",
    "\n",
    "Inception modules apply **multiple filter sizes in parallel** and then concatenate the results:\n",
    "\n",
    "- Branches might include:\n",
    "  - $1 \\times 1$ convolutions,\n",
    "  - $3 \\times 3$ convolutions,\n",
    "  - $5 \\times 5$ convolutions,\n",
    "  - Max pooling with projection.\n",
    "\n",
    "Outputs from all branches are concatenated along the channel dimension.\n",
    "\n",
    "To control the number of parameters, Inception uses **dimensionality reduction**:\n",
    "\n",
    "- $1 \\times 1$ convolutions before $3 \\times 3$ and $5 \\times 5$ to reduce the number of incoming channels,\n",
    "- This acts as a cheap bottleneck before expensive convolutions.\n",
    "\n",
    "Ideas behind Inception:\n",
    "\n",
    "- Approximate a **sparse** optimal structure with a dense but efficient module,\n",
    "- Process visual information at **multiple scales** simultaneously,\n",
    "- Use dimensionality reduction to keep computation manageable.\n",
    "\n",
    "GoogLeNet (Inception v1) and its successors (Inception v3, etc.) are built by stacking such modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7cc7a1",
   "metadata": {},
   "source": [
    "## Separable filters and depthwise separable convolutions\n",
    "\n",
    "Large 2D kernels (e.g. $7 \\times 7$) can be decomposed into more efficient forms.\n",
    "\n",
    "### Spatially separable filters\n",
    "\n",
    "A 2D kernel is **separable** if it can be written as an outer product of two 1D kernels.\n",
    "\n",
    "Example from the slides:\n",
    "\n",
    "- Smoothing filter:\n",
    "  $$\n",
    "  \\frac{1}{3}\n",
    "  \\begin{bmatrix}\n",
    "  1 \\\\ 1 \\\\ 1\n",
    "  \\end{bmatrix}\n",
    "  * \\frac{1}{3} [1 \\ 1 \\ 1]\n",
    "  = \\frac{1}{9}\n",
    "  \\begin{bmatrix}\n",
    "  1 & 1 & 1 \\\\\n",
    "  1 & 1 & 1 \\\\\n",
    "  1 & 1 & 1\n",
    "  \\end{bmatrix}.\n",
    "  $$\n",
    "\n",
    "- Edge filter:\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  1 \\\\ 2 \\\\ 1\n",
    "  \\end{bmatrix}\n",
    "  * [1 \\ 0 \\ -1]\n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0 & -1 \\\\\n",
    "  2 & 0 & -2 \\\\\n",
    "  1 & 0 & -1\n",
    "  \\end{bmatrix}.\n",
    "  $$\n",
    "\n",
    "Instead of a single $7 \\times 7$ filter (49 parameters), we can use a $7 \\times 1$ followed by a $1 \\times 7$ filter\n",
    "(14 parameters), achieving the same effect when the kernel is separable.\n",
    "\n",
    "### Depthwise separable convolutions\n",
    "\n",
    "Standard convs mix **spatial** and **cross-channel** correlations in one operation.\n",
    "\n",
    "Depthwise separable convolutions decouple these steps:\n",
    "\n",
    "1. **Depthwise convolution**:\n",
    "   - Convolve each input channel independently with its own spatial kernel (e.g. $3 \\times 3$),\n",
    "   - Produces the same number of channels as input.\n",
    "\n",
    "2. **Pointwise convolution**:\n",
    "   - Apply a $1 \\times 1$ convolution across channels to mix them,\n",
    "   - Changes the number of channels (e.g. from $C_\\text{in}$ to $C_\\text{out}$).\n",
    "\n",
    "This is similar in spirit to an “extreme” Inception module but applied per channel first, then mixing.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Significant reduction in computation and parameters,\n",
    "- Widely used in efficient models (e.g. MobileNet, Xception).\n",
    "\n",
    "Example PyTorch implementation from the slides:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size=3, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            nin, nin, kernel_size=kernel_size,\n",
    "            padding=padding, groups=nin, bias=bias\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(\n",
    "            nin, nout, kernel_size=1, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126938a",
   "metadata": {},
   "source": [
    "## Multi-head networks and shared feature representations\n",
    "\n",
    "Sometimes we want a network to solve **multiple tasks simultaneously** while sharing most of its computation.\n",
    "\n",
    "Examples from the slides:\n",
    "\n",
    "- AlphaZero:\n",
    "  - Shared CNN backbone → separate heads for **policy** (next move) and **value** (position evaluation).\n",
    "- Multi-head physics-informed networks (PINNs):\n",
    "  - Shared representation → different heads for different physical quantities or conditions.\n",
    "- Card game models (e.g. Jass):\n",
    "  - Shared representation → heads for policy, value, and card distribution.\n",
    "\n",
    "### General pattern\n",
    "\n",
    "- A shared **stem** (backbone) processes the input into a feature representation,\n",
    "- Multiple **heads** branch off, each with its own layers and loss function.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Parameter sharing:\n",
    "  - Reduced total number of parameters,\n",
    "  - Faster training and inference.\n",
    "- Multi-task learning:\n",
    "  - Tasks can **regularize** each other,\n",
    "  - Shared features can generalize better.\n",
    "\n",
    "Training:\n",
    "\n",
    "- Each head has its own loss $L_k$,\n",
    "- The total loss is often a weighted sum:\n",
    "  $$\n",
    "  L_\\text{total} = \\sum_k \\lambda_k L_k,\n",
    "  $$\n",
    "  with task-specific weights $\\lambda_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39bc71",
   "metadata": {},
   "source": [
    "## Sparsity and pruning in deep networks\n",
    "\n",
    "Modern networks can be very large, but in many cases only a fraction of the parameters are truly needed.\n",
    "\n",
    "### Why sparsity?\n",
    "\n",
    "- Reduce computational cost and memory footprint,\n",
    "- Enable deployment on constrained hardware,\n",
    "- Potentially improve interpretability.\n",
    "\n",
    "### Inducing sparsity\n",
    "\n",
    "Approaches include:\n",
    "\n",
    "- **$L_1$ regularization** on weights:\n",
    "  - Encourages many weights to be close to zero,\n",
    "  - Small-magnitude weights can then be **pruned** (set exactly to zero).\n",
    "- Structured pruning:\n",
    "  - Remove entire filters, channels, or blocks,\n",
    "  - Often easier to implement efficiently than unstructured sparsity.\n",
    "\n",
    "The slides highlight that:\n",
    "\n",
    "- There is often an **Occam’s hill**: test error vs sparsity first improves (as redundant parameters are removed),\n",
    "  then worsens once too much capacity is pruned.\n",
    "\n",
    "Key takeaway:\n",
    "\n",
    "> Carefully introduced sparsity can make networks cheaper and sometimes even more accurate, but excessive pruning degrades performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c5d34",
   "metadata": {},
   "source": [
    "## Classical CNN architectures and their design ideas\n",
    "\n",
    "Several influential CNN architectures illustrate the design patterns discussed above.\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "- Early success on ImageNet (2012).\n",
    "- Architecture:\n",
    "  - Large initial $11 \\times 11$ convolution with stride 4,\n",
    "  - Multiple conv and pooling layers,\n",
    "  - Two large fully connected layers (4096 units each),\n",
    "  - Output layer for 1000 ImageNet classes.\n",
    "- Key ideas at the time:\n",
    "  - Use of **ReLU** activations,\n",
    "  - Data augmentation,\n",
    "  - Dropout to reduce overfitting,\n",
    "  - GPU training.\n",
    "\n",
    "### VGG\n",
    "\n",
    "- Uses **only $3 \\times 3$ convolutions** throughout the network.\n",
    "- Stacks many such layers to increase depth:\n",
    "  - Simple, uniform architecture,\n",
    "  - Demonstrated that deeper networks with small kernels perform very well.\n",
    "- Heavy on parameters due to large fully connected layers at the end.\n",
    "\n",
    "### GoogLeNet / Inception\n",
    "\n",
    "- Introduced **Inception modules** with multiple kernel sizes ($1 \\times 1$, $3 \\times 3$, $5 \\times 5$) and pooling in parallel.\n",
    "- Uses $1 \\times 1$ convolutions for **dimensionality reduction**.\n",
    "- Overall architecture:\n",
    "  - Inception modules stacked with occasional pooling,\n",
    "  - Auxiliary classifiers (extra heads) during training to help gradients,\n",
    "  - Less reliance on large fully connected layers.\n",
    "\n",
    "### ResNet\n",
    "\n",
    "- Introduces **residual connections** (skip connections) to allow very deep networks.\n",
    "- Variants:\n",
    "  - Plain ResNets with basic blocks,\n",
    "  - Bottleneck ResNets using $1 \\times 1$ convs around $3 \\times 3$ convs.\n",
    "- Eliminates many intermediate pooling layers and relies on strides and residual blocks.\n",
    "- Scales to very deep networks (e.g. 152 or even 1000+ layers).\n",
    "\n",
    "These architectures combine:\n",
    "\n",
    "- Convolutional feature extraction,\n",
    "- Downsampling and upsampling strategies,\n",
    "- Normalization and residual connections,\n",
    "- Bottleneck and multi-scale modules,\n",
    "\n",
    "and they motivate many of the modern design choices in current CNN-based models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
