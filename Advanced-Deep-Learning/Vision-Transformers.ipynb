{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd713daa",
   "metadata": {},
   "source": "# Vision Transformers"
  },
  {
   "cell_type": "markdown",
   "id": "7f8365ee",
   "metadata": {},
   "source": [
    "## Vectorized self-attention in the encoder\n",
    "\n",
    "Self-attention can be written in a compact, vectorized form.\n",
    "\n",
    "Given a sequence of embeddings stacked in a matrix $X \\in \\mathbb{R}^{T \\times d}$ (each row is a token embedding):\n",
    "\n",
    "1. Compute **queries**, **keys**, and **values**:\n",
    "   $$\n",
    "   Q = X W^Q, \\quad\n",
    "   K = X W^K, \\quad\n",
    "   V = X W^V,\n",
    "   $$\n",
    "   where $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$.\n",
    "\n",
    "2. Compute attention scores:\n",
    "   $$\n",
    "   E = Q K^\\top \\in \\mathbb{R}^{T \\times T}.\n",
    "   $$\n",
    "\n",
    "3. Apply softmax row-wise:\n",
    "   $$\n",
    "   A = \\text{softmax}(E),\n",
    "   $$\n",
    "\n",
    "4. Compute the output:\n",
    "   $$\n",
    "   \\text{Output} = A V.\n",
    "   $$\n",
    "\n",
    "This is often written as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)\n",
    "= \\text{softmax}(QK^\\top)\\,V.\n",
    "$$\n",
    "\n",
    "In **scaled dot-product attention**, we divide by $\\sqrt{d_k}$:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)\n",
    "= \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V,\n",
    "$$\n",
    "\n",
    "which improves numerical stability when $d_k$ is large.\n",
    "\n",
    "Self-attention can be viewed as a learned, differentiable **key–value lookup** where each query selects a weighted\n",
    "combination of values based on similarity to keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd664c7",
   "metadata": {},
   "source": [
    "## ConvNets vs Transformers (conceptual comparison)\n",
    "\n",
    "The slides highlight high-level differences between convolutional networks and transformers.\n",
    "\n",
    "**Convolutional networks (CNNs)**\n",
    "\n",
    "- Operate on grid-structured inputs (e.g. images).\n",
    "- Use local filters and **weight sharing** across spatial positions.\n",
    "- Implicitly enforce **translation invariance**:\n",
    "  - Convolution kernels depend only on **relative position** within a local neighborhood.\n",
    "- Build large receptive fields by:\n",
    "  - Stacking many layers,\n",
    "  - Using pooling or strided convolutions to downsample.\n",
    "\n",
    "**Transformers**\n",
    "\n",
    "- Use self-attention to connect all positions:\n",
    "  - Any token can attend to any other in one step (global receptive field).\n",
    "- Do not have built-in translation invariance:\n",
    "  - Use **positional encodings** instead of relative positions in the kernel.\n",
    "- Are highly parallelizable across positions.\n",
    "\n",
    "For images, the question is:\n",
    "\n",
    "> Can we treat an image as a sequence and apply transformers directly, without convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be962d8",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT): main idea\n",
    "\n",
    "The core idea of the Vision Transformer is to **treat an image as a sequence of patches** and apply a standard transformer\n",
    "encoder for classification.\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "1. **Split image into patches**:\n",
    "   - Input image size: $H \\times W \\times C$.\n",
    "   - Choose patch size: $P \\times P$ (e.g. $16 \\times 16$).\n",
    "   - The image is reshaped into $N = \\frac{HW}{P^2}$ patches, each of size $P^2 C$.\n",
    "\n",
    "2. **Flatten patches**:\n",
    "   - Each patch is flattened into a vector $x_i \\in \\mathbb{R}^{P^2 C}$.\n",
    "\n",
    "3. **Linear projection**:\n",
    "   - Each patch vector is mapped to a $D$-dimensional embedding:\n",
    "     $$\n",
    "     z_i^0 = E_{\\text{patch}} x_i \\in \\mathbb{R}^D.\n",
    "     $$\n",
    "\n",
    "4. **Class token**:\n",
    "   - Prepend a **learnable** embedding $z_{\\text{class}}^0$ to the sequence.\n",
    "   - Its final representation after the transformer encoder is used as the **image representation** for classification.\n",
    "\n",
    "5. **Positional embeddings**:\n",
    "   - Add a learnable 1D positional embedding $p_i$ to each patch (and class) embedding:\n",
    "     $$\n",
    "     z_i^0 \\leftarrow z_i^0 + p_i.\n",
    "     $$\n",
    "\n",
    "6. **Transformer encoder**:\n",
    "   - Apply a standard transformer encoder (stack of multi-head self-attention + MLP blocks) to the sequence\n",
    "     $[z_{\\text{class}}^0, z_1^0,\\dots,z_N^0]$.\n",
    "\n",
    "7. **Classification head**:\n",
    "   - Take the final class token $z_{\\text{class}}^L$ from the top encoder layer.\n",
    "   - Feed it into an MLP classifier:\n",
    "     - Often a small MLP with one hidden layer during pretraining,\n",
    "     - Possibly a single linear layer for fine-tuning.\n",
    "\n",
    "In short:\n",
    "\n",
    "> ViT = **Patches → Linear embeddings + class token → Transformer encoder → MLP head**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03be25",
   "metadata": {},
   "source": [
    "## Vision Transformer architecture in more detail\n",
    "\n",
    "The slides summarize ViT with the following components:\n",
    "\n",
    "- **Patch + position embedding**:\n",
    "  - Image is divided into patches and each is linearly projected to dimension $D$.\n",
    "  - A learnable class token is prepended.\n",
    "  - Position embeddings are added to each token (including class token).\n",
    "\n",
    "- **Transformer encoder** (repeated $L$ times):\n",
    "  - LayerNorm → Multi-head self-attention → residual connection,\n",
    "  - LayerNorm → MLP (position-wise feed-forward) → residual connection.\n",
    "\n",
    "- **MLP head**:\n",
    "  - Takes the final representation of the class token,\n",
    "  - Outputs class logits (bird, ball, car, ...).\n",
    "\n",
    "Symbolically, for layer $\\ell$:\n",
    "\n",
    "1. Self-attention sublayer:\n",
    "   $$\n",
    "   \\tilde{Z}^{(\\ell)} = \\text{LayerNorm}\\big(Z^{(\\ell)}\\big),\n",
    "   \\qquad\n",
    "   Z^{(\\ell)}_{\\text{attn}} =\n",
    "   Z^{(\\ell)} + \\text{MultiHeadSelfAttn}(\\tilde{Z}^{(\\ell)}).\n",
    "   $$\n",
    "\n",
    "2. Feed-forward sublayer:\n",
    "   $$\n",
    "   \\hat{Z}^{(\\ell)} = \\text{LayerNorm}\\big(Z^{(\\ell)}_{\\text{attn}}\\big),\n",
    "   \\qquad\n",
    "   Z^{(\\ell+1)} =\n",
    "   Z^{(\\ell)}_{\\text{attn}} + \\text{FFN}(\\hat{Z}^{(\\ell)}).\n",
    "   $$\n",
    "\n",
    "The **depth** $L$, **hidden size** $D$, **MLP size**, and **number of heads** are varied across ViT model variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943aa2e",
   "metadata": {},
   "source": [
    "## ViT model variants and sizes\n",
    "\n",
    "The original ViT paper defines several standard configurations, similar to BERT:\n",
    "\n",
    "- **ViT-Base**:\n",
    "  - Layers: $12$\n",
    "  - Hidden size $D$: $768$\n",
    "  - MLP size: $3072$\n",
    "  - Attention heads: $12$\n",
    "  - Parameters: $\\approx 86$M\n",
    "\n",
    "- **ViT-Large**:\n",
    "  - Layers: $24$\n",
    "  - Hidden size $D$: $1024$\n",
    "  - MLP size: $4096$\n",
    "  - Attention heads: $16$\n",
    "  - Parameters: $\\approx 307$M\n",
    "\n",
    "- **ViT-Huge**:\n",
    "  - Layers: $32$\n",
    "  - Hidden size $D$: $1280$\n",
    "  - MLP size: $5120$\n",
    "  - Attention heads: $16$\n",
    "  - Parameters: $\\approx 632$M\n",
    "\n",
    "Notation like **ViT-L/16**:\n",
    "\n",
    "- “L” refers to the **Large** configuration,\n",
    "- “/16” refers to a patch size of $16 \\times 16$,\n",
    "- The sequence length is inversely proportional to the square of the patch size:\n",
    "  - Smaller patches → longer sequences → higher compute cost for attention.\n",
    "\n",
    "Key observation:\n",
    "\n",
    "- As in NLP, ViT performance tends to improve with **larger models** and **larger training datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fff69",
   "metadata": {},
   "source": [
    "## Pretraining and data requirements\n",
    "\n",
    "The slides discuss ViT performance on image classification benchmarks and its dependence on **pretraining data size**.\n",
    "\n",
    "Findings:\n",
    "\n",
    "- When trained on **mid-sized datasets** like ImageNet alone, ViT achieves **modest accuracies**, often below\n",
    "  strong CNN baselines.\n",
    "- When **pretrained on very large datasets** (e.g. JFT-300M, ImageNet-21k) and then fine-tuned,\n",
    "  ViT achieves **state-of-the-art** or competitive performance.\n",
    "\n",
    "Example summary:\n",
    "\n",
    "- ViT-L/16 and ViT-H/14 pretrained on JFT-300M outperform strong CNN baselines (e.g. “BiT” ResNets, EfficientNet-L2)\n",
    "  on a variety of datasets:\n",
    "  - ImageNet,\n",
    "  - CIFAR-10/100,\n",
    "  - Oxford Pets,\n",
    "  - Flowers,\n",
    "  - VTAB tasks.\n",
    "\n",
    "Data efficiency:\n",
    "\n",
    "- ViT has **fewer inductive biases** for vision than CNNs:\n",
    "  - It does not encode translation invariance or locality explicitly.\n",
    "- As a result, ViT behaves similarly to language transformers:\n",
    "  - Requires **very large pretraining datasets** to generalize well.\n",
    "  - Benefits strongly from transfer learning: pretrain on massive data, then fine-tune on specific tasks.\n",
    "\n",
    "Effect of dataset size (qualitative):\n",
    "\n",
    "- With small pretraining datasets (e.g. ImageNet-1k), larger ViT models can underperform smaller ones,\n",
    "  because they overfit and cannot fully exploit their capacity.\n",
    "- As pretraining data grows (ImageNet-21k, JFT-300M), larger models start to **dominate** and yield higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff84ea",
   "metadata": {},
   "source": [
    "## What does ViT learn?\n",
    "\n",
    "The slides show several visualizations from the ViT paper:\n",
    "\n",
    "### Patch embedding filters\n",
    "\n",
    "- The first linear projection that maps flattened patches to embeddings can be visualized.\n",
    "- Applying PCA to the learned $E_{\\text{patch}}$ filters and plotting them as images reveals:\n",
    "  - Many filters look like localized edge or color detectors,\n",
    "  - Similar to early layers in CNNs.\n",
    "\n",
    "This indicates that even without explicit convolution, ViT learns **patch-level patterns** reminiscent of CNN filters.\n",
    "\n",
    "### Attention distance\n",
    "\n",
    "- The **mean attention distance** of each head and layer can be measured (how far, in patch space, a token tends to attend).\n",
    "- Observations:\n",
    "  - Some attention heads in lower layers already attend to **distant patches**, providing a large receptive field early on.\n",
    "  - Others focus on nearby patches, capturing local structure.\n",
    "\n",
    "Analogy:\n",
    "\n",
    "- Attention distance is comparable to the **receptive field** in CNNs, but:\n",
    "  - Self-attention can access global context in a single layer,\n",
    "  - CNNs need many layers to build such large receptive fields.\n",
    "\n",
    "### Attention maps\n",
    "\n",
    "- By visualizing attention weights from the class token (or from certain heads), we see:\n",
    "  - The transformer focuses attention on **semantically relevant regions** of the image,\n",
    "  - E.g. the object of interest (dog, car, bird) rather than background.\n",
    "\n",
    "These visualizations support the idea that ViT learns meaningful global and local interactions through attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bd9a6",
   "metadata": {},
   "source": [
    "## Combining CNNs and attention: motivation (CoAtNet)\n",
    "\n",
    "Despite the strong performance of ViTs with massive pretraining, the slides note:\n",
    "\n",
    "- Transformers in vision often **lag behind** state-of-the-art CNNs on tasks with:\n",
    "  - Limited data,\n",
    "  - Strong inductive biases needed (e.g. local structure, translation invariance).\n",
    "- Transformers tend to have **larger model capacity**, but **weaker inductive bias**:\n",
    "  - They may overfit small datasets,\n",
    "  - Generalization can be worse compared to CNNs trained on the same data.\n",
    "\n",
    "Idea:\n",
    "\n",
    "> Combine the strengths of **convolutions** and **self-attention** in a single architecture.\n",
    "\n",
    "- Use convolution to capture **local patterns** and provide strong inductive bias.\n",
    "- Use attention to capture **global interactions** and long-range dependencies.\n",
    "\n",
    "CoAtNet is one such hybrid architecture explored in the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9eb844",
   "metadata": {},
   "source": [
    "## Convolution and self-attention: mathematical comparison\n",
    "\n",
    "The slides compare **depthwise convolution** and **self-attention** in a unified notation.\n",
    "\n",
    "Let $x_i$ denote the input feature at spatial position $i$.\n",
    "\n",
    "### Depthwise convolution\n",
    "\n",
    "With a local neighborhood $L(i)$ (e.g. a $3 \\times 3$ window), depthwise convolution computes:\n",
    "\n",
    "$$\n",
    "y_i = \\sum_{j \\in L(i)} w_{i-j} \\cdot x_j,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $w_{i-j}$ is a learned kernel weight depending only on the **relative position** $(i-j)$,\n",
    "- The kernel is **input-independent**,\n",
    "- The operation is **local** and translationally invariant.\n",
    "\n",
    "### Self-attention\n",
    "\n",
    "Let $G$ denote the set of all positions. Self-attention can be written as:\n",
    "\n",
    "$$\n",
    "y_i\n",
    "= \\sum_{j \\in G} A_{i,j} x_j,\n",
    "\\qquad\n",
    "A_{i,j}\n",
    "= \\frac{\\exp(x_i^\\top x_j)}{\\sum_{k \\in G} \\exp(x_i^\\top x_k)}.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- The attention weights $A_{i,j}$ depend on the **content** (features),\n",
    "- The operation is **global** (sum over all positions),\n",
    "- No inherent translation invariance (positions must be encoded separately).\n",
    "\n",
    "### Comparison\n",
    "\n",
    "- **Kernel**:\n",
    "  - Convolution: weights $w_{i-j}$ are fixed after training and **do not depend on input**.\n",
    "  - Attention: weights $A_{i,j}$ are **input-dependent** and can capture complex relations.\n",
    "- **Receptive field**:\n",
    "  - Convolution: local neighborhood $L(i)$ (small receptive field per layer).\n",
    "  - Attention: global set $G$ (global receptive field in one layer).\n",
    "- **Inductive bias**:\n",
    "  - Convolution: relies on relative positions; strong bias for local, translation-invariant features.\n",
    "  - Attention: relies on learned content similarity; more flexible but with weaker structural bias.\n",
    "\n",
    "This motivates architectures that **combine both** operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552e42e",
   "metadata": {},
   "source": [
    "## Relative self-attention in CoAtNet\n",
    "\n",
    "To combine convolutional and attention-like behaviors, CoAtNet uses **relative self-attention**.\n",
    "\n",
    "The idea:\n",
    "\n",
    "- Modify the attention scores by adding a **relative positional kernel** $w_{i-j}$:\n",
    "  $$\n",
    "  y_i^{\\text{pre}}\n",
    "  = \\sum_{j \\in G}\n",
    "      \\frac{\\exp(x_i^\\top x_j + w_{i-j})}\n",
    "           {\\sum_{k \\in G} \\exp(x_i^\\top x_k + w_{i-k})}\n",
    "      x_j.\n",
    "  $$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $x_i^\\top x_j$ is the content-based similarity (as in standard self-attention).\n",
    "- $w_{i-j}$ is a learnable weight depending on the **relative position** between $i$ and $j$.\n",
    "- The softmax is applied over all positions $j \\in G$.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- If $w_{i-j}$ is large for nearby positions and small for distant ones, attention is **biased toward local neighbors**,\n",
    "  mimicking convolutional behavior.\n",
    "- If $w_{i-j}$ is more uniform, attention can remain **global**.\n",
    "- The kernel remains **input-independent**, encoding structural biases,\n",
    "  while the dot-product term incorporates **input-dependent** interactions.\n",
    "\n",
    "This relative-attention formulation allows CoAtNet to:\n",
    "\n",
    "- Capture complex content-based dependencies,\n",
    "- Maintain useful inductive biases from convolutions (via relative positions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67babbfc",
   "metadata": {},
   "source": [
    "## CoAtNet vertical design: stages and downsampling\n",
    "\n",
    "Applying global self-attention at the **pixel level** is computationally prohibitive:\n",
    "\n",
    "- Complexity scales as $O(N^2)$ where $N$ is the number of tokens (pixels or patches).\n",
    "\n",
    "CoAtNet addresses this with a **stage-wise design** similar to CNNs:\n",
    "\n",
    "- Input: $224 \\times 224$ image.\n",
    "- **Stem (S0)**:\n",
    "  - Convolutional layers downsample to a coarser grid (e.g. $112 \\times 112$).\n",
    "- **Stages S1–S4**:\n",
    "  - At each stage, spatial resolution is further reduced (e.g. $56 \\times 56$, $28 \\times 28$, $14 \\times 14$, $7 \\times 7$),\n",
    "  - The number of channels is increased.\n",
    "\n",
    "Within stages:\n",
    "\n",
    "- Early stages (higher resolution) use **convolutional blocks**:\n",
    "  - Standard or depthwise convs,\n",
    "  - $1 \\times 1$ convs as bottlenecks,\n",
    "  - Residual connections.\n",
    "- Later stages (lower resolution) use **relative self-attention blocks** and feed-forward networks.\n",
    "\n",
    "The slides mention that good results (in terms of generalization, capacity, and transferability) were obtained with:\n",
    "\n",
    "- **Three convolutional blocks/stages**, followed by\n",
    "- **Two transformer blocks/stages**.\n",
    "\n",
    "Global pooling and a fully connected (FC) layer at the end produce classification logits.\n",
    "\n",
    "This vertical design:\n",
    "\n",
    "- Keeps early computations **efficient and local** via convolutions,\n",
    "- Uses **attention** when the sequence length is reduced enough to make it tractable,\n",
    "- Mimics the progressive downsampling seen in ResNets and other CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e0f5f",
   "metadata": {},
   "source": [
    "## CoAtNet results and trade-offs\n",
    "\n",
    "The slides show comparisons of:\n",
    "\n",
    "- Accuracy vs FLOPs,\n",
    "- Accuracy vs number of parameters,\n",
    "\n",
    "for CoAtNet and competing models.\n",
    "\n",
    "Qualitative conclusions:\n",
    "\n",
    "- CoAtNet achieves **strong accuracy** while maintaining:\n",
    "  - Competitive or reduced FLOPs compared to pure transformer or pure CNN variants,\n",
    "  - Good parameter efficiency.\n",
    "- By combining convolution and attention:\n",
    "  - It benefits from convolutional inductive biases on small/medium datasets,\n",
    "  - It leverages attention to capture global interactions and improve performance on challenging benchmarks.\n",
    "\n",
    "More broadly, hybrid architectures like CoAtNet illustrate that:\n",
    "\n",
    "> Neither pure CNNs nor pure transformers are optimal for all regimes; combining them can yield better accuracy–efficiency trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233d76b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Self-attention and transformer encoders, originally developed for sequences, can be applied to images by:\n",
    "  - Splitting images into patches,\n",
    "  - Embedding patches and adding positional information,\n",
    "  - Prepending a class token and using a transformer encoder.\n",
    "\n",
    "- Vision Transformers (ViT) show that:\n",
    "  - Pure transformer architectures can achieve state-of-the-art performance on image classification,\n",
    "  - But they require **large-scale pretraining** due to weaker inductive biases than CNNs.\n",
    "\n",
    "- ViT internal behavior:\n",
    "  - Patch embedding layers learn filters similar to early CNN layers,\n",
    "  - Some attention heads attend to distant patches even in lower layers,\n",
    "  - Attention maps focus on semantically important regions of the image.\n",
    "\n",
    "- Convolution vs attention:\n",
    "  - Convolution uses local, input-independent kernels and strong translation-invariance bias,\n",
    "  - Self-attention uses global, input-dependent weights but lacks structured biases,\n",
    "  - Relative self-attention bridges these by adding learnable relative position terms to attention scores.\n",
    "\n",
    "- CoAtNet and similar hybrids:\n",
    "  - Combine convolutional stages for local feature extraction and efficient downsampling,\n",
    "  - With transformer stages for global, content-based interactions,\n",
    "  - Achieve strong performance and favorable accuracy–efficiency trade-offs.\n",
    "\n",
    "These ideas provide a conceptual foundation for modern vision architectures that increasingly integrate both convolution\n",
    "and attention mechanisms."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
